{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nROkQT4rFjSO"
      },
      "outputs": [],
      "source": [
        "#Import The Solar Flare Data Set Files\n",
        "#change it according to your files location\n",
        "labelFile=\"Data\\\\labels_1540_4classes_icmla_21.pck\"\n",
        "inputsFile=\"Data\\\\mvts_1540_icmla_21.pck\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_v4--qEH4UG"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsMdgSmtJe6c"
      },
      "outputs": [],
      "source": [
        "def loadInputs(file_name):\n",
        "        with open(file_name, 'rb') as fp:\n",
        "            obj = pickle.load(fp)\n",
        "        return obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS9sMb7AJ4r2",
        "outputId": "a31e0c2e-8f97-444c-94ed-d117427c5df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainData.shape:  (1540, 33, 60)\n",
            "trainLebel.shape:  (1540,)\n",
            "Classes/labels :  [0 1 2 3]\n"
          ]
        }
      ],
      "source": [
        "#device = torch.device('cpu')\n",
        "#print(\"Processing in :\",device)\n",
        "\n",
        "Sampled_inputs=loadInputs(\"mvts_1540_icmla_21.pck\")\n",
        "Sampled_labels=loadInputs(\"labels_1540_4classes_icmla_21.pck\") \n",
        "trainData = Sampled_inputs\n",
        "trainLabel = Sampled_labels\n",
        "print(\"trainData.shape: \", trainData.shape)\n",
        "print(\"trainLebel.shape: \", trainLabel.shape)\n",
        "print(\"Classes/labels : \",np.unique(trainLabel))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj52QvgGKP9j",
        "outputId": "18268f98-2afa-47e5-df4f-4b1d78d28d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'numpy.ndarray'>\n",
            "trainData.shape:  (1540, 33, 60)\n",
            "<class 'numpy.ndarray'>\n",
            "trainLebel.shape:  (1540,)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "#standardization/z normalization of the univaraite time series\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "npArrays=[]\n",
        "for l in range(0, len(trainData)):\n",
        "  trainData_std = sc.fit_transform(trainData[l])\n",
        "  #trainData_std = trainData_std.astype(np.float64)\n",
        "  #print(type(trainData_std[0][0]))\n",
        "  npArrays.append(trainData_std)\n",
        "\n",
        "print(type(npArrays))\n",
        "arr = np.asarray(npArrays)\n",
        "print(type(arr))\n",
        "trainData=arr\n",
        "print(\"trainData.shape: \",trainData.shape)\n",
        "print(type(trainData))\n",
        "print(\"trainLebel.shape: \",trainLabel.shape)\n",
        "print(type(trainLabel))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXiji4kOKm5Z",
        "outputId": "55946ab6-9f15-450e-f683-a99b2a31b73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transposing trainData shape:  (1540, 60, 33)\n"
          ]
        }
      ],
      "source": [
        "#Transposing trainData to shape:(1540, 60, 33)\n",
        "trainDatatemp=np.empty([1540,60, 33])\n",
        "n=len(trainData)\n",
        "for l in range(0, n):\n",
        "  temp=trainData[l]\n",
        "  temp=temp.T\n",
        "  trainDatatemp[l,:,:]=temp\n",
        "  \n",
        "\n",
        "trainData=trainDatatemp\n",
        "print(\"Transposing trainData shape: \",trainData.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN9MfXJJKrsZ",
        "outputId": "8d1bd3c2-7dd6-4722-d104-39a36c712f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainData shape:  (1540, 60, 25)\n"
          ]
        }
      ],
      "source": [
        "#Taking the first 25 parameters which are based parameters:(1540, 60, 25)\n",
        "trainDatat1=np.empty([1540,60, 25])\n",
        "n=len(trainData)\n",
        "for l in range(0, n):\n",
        "  temp=trainData[l,:,0:25]\n",
        "  trainDatat1[l,:,:]=temp\n",
        "  \n",
        "\n",
        "trainData=trainDatat1\n",
        "print(\"trainData shape: \",trainData.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimensionality Reduction Train Step\n",
        "#Calculating the mean for each MVTS column (25 parameters which are based parameters)\n",
        "def Dimensionality_Reduction_Train(X_train):\n",
        " trainDatatmean=np.empty([862,25])\n",
        " n=len(X_train)\n",
        " for l in range(0, n):\n",
        "  temp=X_train[l,:,:]\n",
        "  #print(temp.shape)\n",
        "  #print(temp.shape[0])\n",
        "  #print(temp.shape[1])\n",
        "  x=temp.mean(axis=0)\n",
        "  #print(\"mean for mvts:\",l,\"is=\",x)\n",
        "  trainDatatmean[l]=x\n",
        "\n",
        " x_train=trainDatatmean\n",
        " print(\"trainDatatmean shape: \",trainDatatmean.shape)\n",
        " return x_train"
      ],
      "metadata": {
        "id": "wZedRG2JH5eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimensionality Reduction Step\n",
        "#Calculating the mean for each MVTS column (25 parameters which are based parameters)\n",
        "def Dimensionality_Reduction_Test(X_test):\n",
        " testDatatmean=np.empty([462,25])\n",
        " n=len(X_test)\n",
        " for l in range(0, n):\n",
        "  temp=X_test[l,:,:]\n",
        "  #print(temp.shape)\n",
        "  #print(temp.shape[0])\n",
        "  #print(temp.shape[1])\n",
        "  x=temp.mean(axis=0)\n",
        "  #print(\"mean for mvts:\",l,\"is=\",x)\n",
        "  testDatatmean[l]=x\n",
        "\n",
        " x_test=testDatatmean\n",
        " print(\"trainDatatmean shape: \",testDatatmean.shape)\n",
        " return x_test"
      ],
      "metadata": {
        "id": "hqXeJ1zw4vlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimensionality Reduction Step\n",
        "#Calculating the mean for each MVTS column (25 parameters which are based parameters)\n",
        "def Dimensionality_Reduction_Val(X_val):\n",
        " valDatatmean=np.empty([216,25])\n",
        " n=len(X_val)\n",
        " for l in range(0, n):\n",
        "  temp=X_val[l,:,:]\n",
        "  #print(temp.shape)\n",
        "  #print(temp.shape[0])\n",
        "  #print(temp.shape[1])\n",
        "  x=temp.mean(axis=0)\n",
        "  #print(\"mean for mvts:\",l,\"is=\",x)\n",
        "  valDatatmean[l]=x\n",
        "\n",
        " x_val=valDatatmean\n",
        " print(\"trainDatatmean shape: \",valDatatmean.shape)\n",
        " return x_val"
      ],
      "metadata": {
        "id": "-HRnJYxwVewD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K1=[2]\n",
        "K2=[2,20] \n",
        "K3=[2,16,20]\n",
        "K4=[2,16,17,20]\n",
        "K5=[2,7,16,17,20]\n",
        "K6=[2,6,7,16,17,20]\n",
        "K7=[2,6,7,12,16,17,20]\n",
        "K8=[2,6,7,9,12 ,16,17, 20]\n",
        "K9=[2,6,7, 9, 12,15,16,17, 20]\n",
        "K10=[2,6,7, 9, 12,13, 15,16,17, 20]\n",
        "K11=[2,6,7, 9,10,12,13, 15,16,17, 20]\n",
        "K12=[2,6,7, 9,10,12,13, 15,16,17, 20, 24]\n",
        "K13=[2,6,7,8,9,10,12,13, 15,16,17, 20, 24]\n",
        "K14=[2,6,7,8,9,10,12,13,14,15,16,17, 20, 24]\n",
        "K15=[2,6,7,8,9,10,12,13,14,15,16,17,18, 20, 24]\n",
        "K16=[2,6,7,8,9,10,11,12,13,14,15,16,17,18, 20, 24]\n",
        "K17=[2,3,6,7,8,9,10,11,12,13,14,15,16,17,18, 20, 24]\n",
        "K18=[2,3,6,7,8,9,10,11,12,13,14,15,16,17,18, 20,21, 24]\n",
        "K19=[2,3,6,7,8,9,10,11,12,13,14,15,16,17,18, 20,21,22, 24]\n",
        "K20=[2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22, 24]\n",
        "K21=[2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
        "K22=[2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
        "K23=[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
        "K24=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
        "K25=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
        " \n",
        "\n",
        "\n",
        " \n",
        " \n"
      ],
      "metadata": {
        "id": "T4r32NOfecZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TopK=[K1,K2,K3,K4,K5,K6,K7,K8,K9,K10,K11,K12,K13,K14,K15,K16,K17,K18,K19,K20,K21,K22,K23,K24,K25]"
      ],
      "metadata": {
        "id": "tFItWslaQOLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TopK[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLJC_6voRIWy",
        "outputId": "874daf9a-7879-4ca1-dade-1ce712ee3214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 20]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBClassifier\n",
        "def xgbclassif(Xtr,Xv,Xte,y_train, y_val,y_test):\n",
        " clf = xgb.XGBClassifier(objective=\"multi:softprob\")\n",
        " clf.fit(Xtr, y_train)\n",
        " yv_pred=clf.predict(Xv)\n",
        " print(\"validation data results:\")\n",
        " VScore=accuracy_score(y_val, yv_pred)\n",
        " print(\"Validation Accuracey\",VScore)\n",
        " print(confusion_matrix(y_val, yv_pred))\n",
        " print(classification_report(y_val, yv_pred))\n",
        " y_pred=clf.predict(Xte)\n",
        " print(\"test data results:\")\n",
        " TScore=accuracy_score(y_test, y_pred)\n",
        " print(\"Test Accuracey\",TScore)\n",
        " print(confusion_matrix(y_test, y_pred))\n",
        " print(classification_report(y_test, y_pred))\n",
        " return VScore, TScore"
      ],
      "metadata": {
        "id": "GA23ZzUuOAr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SVMClassifier\n",
        "def SVM1(Xtr,Xv,Xte,y_train, y_val,y_test):\n",
        " clf = svm.SVC()\n",
        " clf.fit(Xtr, y_train)\n",
        " yv_pred=clf.predict(Xv)\n",
        " print(\"validation data results:\")\n",
        " VScore=accuracy_score(y_val, yv_pred)\n",
        " print(\"Validation Accuracey\",VScore)\n",
        " print(confusion_matrix(y_val, yv_pred))\n",
        " print(classification_report(y_val, yv_pred))\n",
        " y_pred=clf.predict(Xte)\n",
        " print(\"test data results:\")\n",
        " TScore=accuracy_score(y_test, y_pred)\n",
        " print(\"Test Accuracey\",TScore)\n",
        " print(confusion_matrix(y_test, y_pred))\n",
        " print(classification_report(y_test, y_pred))\n",
        " return VScore, TScore"
      ],
      "metadata": {
        "id": "xpJ2fOKz6erC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNNClassifier\n",
        "def knn(Xtr,Xv,Xte,y_train, y_val,y_test):\n",
        " clf = KNeighborsClassifier(n_neighbors=5)\n",
        " clf.fit(Xtr, y_train)\n",
        " yv_pred=clf.predict(Xv)\n",
        " print(\"validation data results:\")\n",
        " VScore=accuracy_score(y_val, yv_pred)\n",
        " print(\"Validation Accuracey\",VScore)\n",
        " print(confusion_matrix(y_val, yv_pred))\n",
        " print(classification_report(y_val, yv_pred))\n",
        " y_pred=clf.predict(Xte)\n",
        " print(\"test data results:\")\n",
        " TScore=accuracy_score(y_test, y_pred)\n",
        " print(\"Test Accuracey\",TScore)\n",
        " print(confusion_matrix(y_test, y_pred))\n",
        " print(classification_report(y_test, y_pred))\n",
        " return VScore, TScore"
      ],
      "metadata": {
        "id": "zE6YTeEn9P0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RandomForestClassifier\n",
        "def RFtree(Xtr,Xv,Xte,y_train, y_val,y_test):\n",
        " clf = RandomForestClassifier()\n",
        " clf.fit(Xtr, y_train)\n",
        " yv_pred=clf.predict(Xv)\n",
        " print(\"validation data results:\")\n",
        " VScore=accuracy_score(y_val, yv_pred)\n",
        " print(\"Validation Accuracey\",VScore)\n",
        " print(confusion_matrix(y_val, yv_pred))\n",
        " print(classification_report(y_val, yv_pred))\n",
        " y_pred=clf.predict(Xte)\n",
        " print(\"test data results:\")\n",
        " TScore=accuracy_score(y_test, y_pred)\n",
        " print(\"Test Accuracey\",TScore)\n",
        " print(confusion_matrix(y_test, y_pred))\n",
        " print(classification_report(y_test, y_pred))\n",
        " return VScore, TScore"
      ],
      "metadata": {
        "id": "qy1HICQ-6B7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DecisionTreeClassifier\n",
        "def DTree(Xtr,Xv,Xte,y_train, y_val,y_test):\n",
        " clf = DecisionTreeClassifier()\n",
        " clf.fit(Xtr, y_train)\n",
        " yv_pred=clf.predict(Xv)\n",
        " print(\"validation data results:\")\n",
        " VScore=accuracy_score(y_val, yv_pred)\n",
        " print(\"Validation Accuracey\",VScore)\n",
        " print(confusion_matrix(y_val, yv_pred))\n",
        " print(classification_report(y_val, yv_pred))\n",
        " y_pred=clf.predict(Xte)\n",
        " print(\"test data results:\")\n",
        " TScore=accuracy_score(y_test, y_pred)\n",
        " print(\"Test Accuracey\",TScore)\n",
        " print(confusion_matrix(y_test, y_pred))\n",
        " print(classification_report(y_test, y_pred))\n",
        " return VScore, TScore"
      ],
      "metadata": {
        "id": "MDfwYxPDKzFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://towardsdatascience.com/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b\n"
      ],
      "metadata": {
        "id": "f1kLUtVE5nAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start(TopK):\n",
        "  validation_sizes=0.2\n",
        "  # Stratify the data\n",
        "  for r in range(0,5):\n",
        "    print(\"Random_state: \", r)\n",
        "    mvts_1540=trainData\n",
        "    labels_1540=trainLabel\n",
        "    X_train, X_test, y_train, y_test = train_test_split(mvts_1540, labels_1540, test_size=0.3, random_state=r, stratify=labels_1540)\n",
        "    print(\"X_train.shape y_train.shape y_test.shape \",X_train.shape, y_train.shape)\n",
        "    print(\"X_test.shape y_test.shape \",X_test.shape, y_test.shape)\n",
        "    validation_size = validation_sizes\n",
        "    print(\"\\n\\n\\n *************** Validation Size is: \", validation_sizes,\"of training size\")\n",
        "    print(\"after train validation split:\")\n",
        "    X_train, X_val,y_train, y_val= train_test_split(X_train,y_train, test_size=validation_size,random_state=r,stratify=y_train)\n",
        "    #check percentage of examples\n",
        "    print(\"X_train shape: \", X_train.shape)\n",
        "    print(\"y_train shape: \", y_train.shape)\n",
        "    print(\"X_test shape: \", X_test.shape)\n",
        "    print(\"y_test shape: \", y_test.shape)\n",
        "    print(\"X_val shape: \",X_val.shape) \n",
        "    print(\"y_val shape \",y_val.shape)\n",
        "\n",
        "    unique_y_train, counts_y_train = np.unique(y_train, return_counts=True)\n",
        "    y_train_stats = dict(zip(unique_y_train, counts_y_train))\n",
        "    print(\"y_train_counts\")\n",
        "    print(y_train_stats)\n",
        "    #270/(269+269+270+270) = 0.25\n",
        "    unique_y_val, counts_y_val = np.unique(y_val, return_counts=True)\n",
        "    y_val_stats = dict(zip(unique_y_val, counts_y_val))\n",
        "    print(\"y_validation_counts\")\n",
        "    print(y_val_stats)\n",
        "\n",
        "    unique_y_test, counts_y_test = np.unique(y_test, return_counts=True)\n",
        "    y_test_stats = dict(zip(unique_y_test, counts_y_test))\n",
        "    print(\"y_test_counts\")\n",
        "    print(y_test_stats)#116/(116+116+115+115) = 0.25\n",
        "    for j in range(0,25):\n",
        "            print(\"Results for Top K=\",j)\n",
        "            c=TopK[j]\n",
        "            print(c)\n",
        "            x_train=Dimensionality_Reduction_Train(X_train)\n",
        "            x_test=Dimensionality_Reduction_Test(X_test)\n",
        "            x_val=Dimensionality_Reduction_Val(X_val)\n",
        "            Xtr=x_train[:,c]\n",
        "            Xv=x_val[:,c]\n",
        "            Xte=x_test[:,c]\n",
        "            print(\"SVM classifier\")\n",
        "            Vsvmscore, Tsvmscore=SVM1(Xtr,Xv,Xte,y_train, y_val,y_test)\n",
        "            print(\"KNN classifier\")\n",
        "            Vknnscore, Tknnscore= knn(Xtr,Xv,Xte,y_train, y_val,y_test)\n",
        "            print(\"Desition Tree classifier\") \n",
        "            Vdtscore, Tdtscore=DTree(Xtr,Xv,Xte,y_train, y_val,y_test)\n",
        "            print(\"Random Forest classifier\") \n",
        "            Vrfscore, Trfscore=RFtree(Xtr,Xv,Xte,y_train, y_val,y_test)\n",
        "            print(\"XGBoost classifier\")\n",
        "            Vxgscore, Txgscore=xgbclassif(Xtr,Xv,Xte,y_train, y_val,y_test)\n",
        "            print(\"Validation and Test Accuracey for Top K=\",j)\n",
        "            print(\" Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are:\",\"[\",Vsvmscore,\",\",Vknnscore,\",\",Vdtscore,\",\",Vrfscore,\",\",Vxgscore,\"]\" )\n",
        "            print(\" Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are:\",\"[\",Tsvmscore,\",\",Tknnscore,\",\",Tdtscore,\",\",Trfscore,\",\",Txgscore,\"]\" )\n",
        "            ValidationSVMAcc.append(Vsvmscore)\n",
        "            TestSVMAcc.append(Tsvmscore)\n",
        "            ValidationKNNAcc.append(Vknnscore)\n",
        "            TestKNNAcc.append(Tknnscore)\n",
        "            ValidationDTAcc.append(Vdtscore)\n",
        "            TestDTAcc.append(Tdtscore)\n",
        "            ValidationRFAcc.append(Vrfscore)\n",
        "            TestRFAcc.append(Trfscore)\n",
        "            ValidationXGBoostAcc.append(Vxgscore)\n",
        "            TestXGBoostAcc.append(Txgscore)\n",
        "            \n"
      ],
      "metadata": {
        "id": "7S_iWOqzTUuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ValidationSVMAcc=[]\n",
        "TestSVMAcc=[]\n",
        "ValidationKNNAcc=[]\n",
        "TestKNNAcc=[]\n",
        "ValidationDTAcc=[]\n",
        "TestDTAcc=[]\n",
        "ValidationRFAcc=[]\n",
        "TestRFAcc=[]\n",
        "ValidationXGBoostAcc=[]\n",
        "TestXGBoostAcc=[]\n",
        "start(TopK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLZPzkz5XhO2",
        "outputId": "84b78dd5-57ff-49c2-9c91-cd40504d3a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6805555555555556\n",
            "[[49  2  3  0]\n",
            " [ 5 33 14  2]\n",
            " [ 5 12 29  8]\n",
            " [ 4  2 12 36]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84        54\n",
            "           1       0.67      0.61      0.64        54\n",
            "           2       0.50      0.54      0.52        54\n",
            "           3       0.78      0.67      0.72        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.68      0.68      0.68       216\n",
            "weighted avg       0.68      0.68      0.68       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6363636363636364\n",
            "[[105   8   3   0]\n",
            " [ 24  65  25   1]\n",
            " [ 18  21  58  19]\n",
            " [  3  14  32  66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.91      0.79       116\n",
            "           1       0.60      0.57      0.58       115\n",
            "           2       0.49      0.50      0.50       116\n",
            "           3       0.77      0.57      0.66       115\n",
            "\n",
            "    accuracy                           0.64       462\n",
            "   macro avg       0.64      0.64      0.63       462\n",
            "weighted avg       0.64      0.64      0.63       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 22\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.37962962962962965 , 0.5833333333333334 , 0.6064814814814815 , 0.625 , 0.6805555555555556 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.43073593073593075 , 0.5714285714285714 , 0.6190476190476191 , 0.6515151515151515 , 0.6363636363636364 ]\n",
            "Results for Top K= 23\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.37962962962962965\n",
            "[[29  8 17  0]\n",
            " [ 7  7 16 24]\n",
            " [ 9  0 20 25]\n",
            " [ 5  2 21 26]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.54      0.56        54\n",
            "           1       0.41      0.13      0.20        54\n",
            "           2       0.27      0.37      0.31        54\n",
            "           3       0.35      0.48      0.40        54\n",
            "\n",
            "    accuracy                           0.38       216\n",
            "   macro avg       0.40      0.38      0.37       216\n",
            "weighted avg       0.40      0.38      0.37       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.43073593073593075\n",
            "[[60 20 36  0]\n",
            " [27 22 32 34]\n",
            " [16  7 56 37]\n",
            " [ 6  3 45 61]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.52      0.53       116\n",
            "           1       0.42      0.19      0.26       115\n",
            "           2       0.33      0.48      0.39       116\n",
            "           3       0.46      0.53      0.49       115\n",
            "\n",
            "    accuracy                           0.43       462\n",
            "   macro avg       0.44      0.43      0.42       462\n",
            "weighted avg       0.44      0.43      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5833333333333334\n",
            "[[52  1  1  0]\n",
            " [ 7 25 17  5]\n",
            " [ 6 13 27  8]\n",
            " [ 5  9 18 22]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.96      0.84        54\n",
            "           1       0.52      0.46      0.49        54\n",
            "           2       0.43      0.50      0.46        54\n",
            "           3       0.63      0.41      0.49        54\n",
            "\n",
            "    accuracy                           0.58       216\n",
            "   macro avg       0.58      0.58      0.57       216\n",
            "weighted avg       0.58      0.58      0.57       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[113   1   2   0]\n",
            " [ 22  65  19   9]\n",
            " [ 21  35  42  18]\n",
            " [ 11  26  34  44]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.97      0.80       116\n",
            "           1       0.51      0.57      0.54       115\n",
            "           2       0.43      0.36      0.39       116\n",
            "           3       0.62      0.38      0.47       115\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6064814814814815\n",
            "[[48  3  2  1]\n",
            " [ 3 30 16  5]\n",
            " [ 6 15 21 12]\n",
            " [ 0  8 14 32]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86        54\n",
            "           1       0.54      0.56      0.55        54\n",
            "           2       0.40      0.39      0.39        54\n",
            "           3       0.64      0.59      0.62        54\n",
            "\n",
            "    accuracy                           0.61       216\n",
            "   macro avg       0.60      0.61      0.60       216\n",
            "weighted avg       0.60      0.61      0.60       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6341991341991342\n",
            "[[106   5   4   1]\n",
            " [  9  64  28  14]\n",
            " [  7  26  55  28]\n",
            " [  1   9  37  68]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.91      0.89       116\n",
            "           1       0.62      0.56      0.58       115\n",
            "           2       0.44      0.47      0.46       116\n",
            "           3       0.61      0.59      0.60       115\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.63      0.63      0.63       462\n",
            "weighted avg       0.63      0.63      0.63       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6064814814814815\n",
            "[[48  5  1  0]\n",
            " [ 4 30 18  2]\n",
            " [ 6 15 20 13]\n",
            " [ 5  4 12 33]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.89      0.82        54\n",
            "           1       0.56      0.56      0.56        54\n",
            "           2       0.39      0.37      0.38        54\n",
            "           3       0.69      0.61      0.65        54\n",
            "\n",
            "    accuracy                           0.61       216\n",
            "   macro avg       0.60      0.61      0.60       216\n",
            "weighted avg       0.60      0.61      0.60       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.645021645021645\n",
            "[[108   5   2   1]\n",
            " [ 14  69  24   8]\n",
            " [ 10  26  57  23]\n",
            " [  6  12  33  64]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85       116\n",
            "           1       0.62      0.60      0.61       115\n",
            "           2       0.49      0.49      0.49       116\n",
            "           3       0.67      0.56      0.61       115\n",
            "\n",
            "    accuracy                           0.65       462\n",
            "   macro avg       0.64      0.64      0.64       462\n",
            "weighted avg       0.64      0.65      0.64       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6805555555555556\n",
            "[[49  2  3  0]\n",
            " [ 5 33 14  2]\n",
            " [ 5 12 29  8]\n",
            " [ 4  2 12 36]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84        54\n",
            "           1       0.67      0.61      0.64        54\n",
            "           2       0.50      0.54      0.52        54\n",
            "           3       0.78      0.67      0.72        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.68      0.68      0.68       216\n",
            "weighted avg       0.68      0.68      0.68       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6363636363636364\n",
            "[[105   8   3   0]\n",
            " [ 24  65  25   1]\n",
            " [ 18  21  58  19]\n",
            " [  3  14  32  66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.91      0.79       116\n",
            "           1       0.60      0.57      0.58       115\n",
            "           2       0.49      0.50      0.50       116\n",
            "           3       0.77      0.57      0.66       115\n",
            "\n",
            "    accuracy                           0.64       462\n",
            "   macro avg       0.64      0.64      0.63       462\n",
            "weighted avg       0.64      0.64      0.63       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 23\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.37962962962962965 , 0.5833333333333334 , 0.6064814814814815 , 0.6064814814814815 , 0.6805555555555556 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.43073593073593075 , 0.5714285714285714 , 0.6341991341991342 , 0.645021645021645 , 0.6363636363636364 ]\n",
            "Results for Top K= 24\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.37962962962962965\n",
            "[[29  8 17  0]\n",
            " [ 7  7 16 24]\n",
            " [ 9  0 20 25]\n",
            " [ 5  2 21 26]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.54      0.56        54\n",
            "           1       0.41      0.13      0.20        54\n",
            "           2       0.27      0.37      0.31        54\n",
            "           3       0.35      0.48      0.40        54\n",
            "\n",
            "    accuracy                           0.38       216\n",
            "   macro avg       0.40      0.38      0.37       216\n",
            "weighted avg       0.40      0.38      0.37       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.43073593073593075\n",
            "[[60 20 36  0]\n",
            " [27 22 32 34]\n",
            " [16  7 56 37]\n",
            " [ 6  3 45 61]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.52      0.53       116\n",
            "           1       0.42      0.19      0.26       115\n",
            "           2       0.33      0.48      0.39       116\n",
            "           3       0.46      0.53      0.49       115\n",
            "\n",
            "    accuracy                           0.43       462\n",
            "   macro avg       0.44      0.43      0.42       462\n",
            "weighted avg       0.44      0.43      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5833333333333334\n",
            "[[52  1  1  0]\n",
            " [ 7 25 17  5]\n",
            " [ 6 13 27  8]\n",
            " [ 5  9 18 22]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.96      0.84        54\n",
            "           1       0.52      0.46      0.49        54\n",
            "           2       0.43      0.50      0.46        54\n",
            "           3       0.63      0.41      0.49        54\n",
            "\n",
            "    accuracy                           0.58       216\n",
            "   macro avg       0.58      0.58      0.57       216\n",
            "weighted avg       0.58      0.58      0.57       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[113   1   2   0]\n",
            " [ 21  66  19   9]\n",
            " [ 21  35  42  18]\n",
            " [ 11  26  34  44]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.97      0.80       116\n",
            "           1       0.52      0.57      0.54       115\n",
            "           2       0.43      0.36      0.39       116\n",
            "           3       0.62      0.38      0.47       115\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5925925925925926\n",
            "[[48  4  2  0]\n",
            " [ 4 29 16  5]\n",
            " [ 6 15 20 13]\n",
            " [ 0  8 15 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86        54\n",
            "           1       0.52      0.54      0.53        54\n",
            "           2       0.38      0.37      0.37        54\n",
            "           3       0.63      0.57      0.60        54\n",
            "\n",
            "    accuracy                           0.59       216\n",
            "   macro avg       0.59      0.59      0.59       216\n",
            "weighted avg       0.59      0.59      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6471861471861472\n",
            "[[107   6   3   0]\n",
            " [  8  67  27  13]\n",
            " [  6  25  56  29]\n",
            " [  1   9  36  69]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       116\n",
            "           1       0.63      0.58      0.60       115\n",
            "           2       0.46      0.48      0.47       116\n",
            "           3       0.62      0.60      0.61       115\n",
            "\n",
            "    accuracy                           0.65       462\n",
            "   macro avg       0.65      0.65      0.65       462\n",
            "weighted avg       0.65      0.65      0.65       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6203703703703703\n",
            "[[48  4  2  0]\n",
            " [ 4 29 19  2]\n",
            " [ 5 16 21 12]\n",
            " [ 5  2 11 36]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.89      0.83        54\n",
            "           1       0.57      0.54      0.55        54\n",
            "           2       0.40      0.39      0.39        54\n",
            "           3       0.72      0.67      0.69        54\n",
            "\n",
            "    accuracy                           0.62       216\n",
            "   macro avg       0.61      0.62      0.62       216\n",
            "weighted avg       0.61      0.62      0.62       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6623376623376623\n",
            "[[110   5   1   0]\n",
            " [ 15  73  22   5]\n",
            " [ 10  27  57  22]\n",
            " [  5   9  35  66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.95      0.86       116\n",
            "           1       0.64      0.63      0.64       115\n",
            "           2       0.50      0.49      0.49       116\n",
            "           3       0.71      0.57      0.63       115\n",
            "\n",
            "    accuracy                           0.66       462\n",
            "   macro avg       0.66      0.66      0.66       462\n",
            "weighted avg       0.66      0.66      0.66       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6805555555555556\n",
            "[[49  2  3  0]\n",
            " [ 5 33 14  2]\n",
            " [ 5 12 29  8]\n",
            " [ 4  2 12 36]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84        54\n",
            "           1       0.67      0.61      0.64        54\n",
            "           2       0.50      0.54      0.52        54\n",
            "           3       0.78      0.67      0.72        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.68      0.68      0.68       216\n",
            "weighted avg       0.68      0.68      0.68       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6363636363636364\n",
            "[[105   8   3   0]\n",
            " [ 24  65  25   1]\n",
            " [ 18  21  58  19]\n",
            " [  3  14  32  66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.91      0.79       116\n",
            "           1       0.60      0.57      0.58       115\n",
            "           2       0.49      0.50      0.50       116\n",
            "           3       0.77      0.57      0.66       115\n",
            "\n",
            "    accuracy                           0.64       462\n",
            "   macro avg       0.64      0.64      0.63       462\n",
            "weighted avg       0.64      0.64      0.63       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 24\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.37962962962962965 , 0.5833333333333334 , 0.5925925925925926 , 0.6203703703703703 , 0.6805555555555556 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.43073593073593075 , 0.5735930735930735 , 0.6471861471861472 , 0.6623376623376623 , 0.6363636363636364 ]\n",
            "Random_state:  4\n",
            "X_train.shape y_train.shape y_test.shape  (1078, 60, 25) (1078,)\n",
            "X_test.shape y_test.shape  (462, 60, 25) (462,)\n",
            "\n",
            "\n",
            "\n",
            " *************** Validation Size is:  0.2 of training size\n",
            "after train validation split:\n",
            "X_train shape:  (862, 60, 25)\n",
            "y_train shape:  (862,)\n",
            "X_test shape:  (462, 60, 25)\n",
            "y_test shape:  (462,)\n",
            "X_val shape:  (216, 60, 25)\n",
            "y_val shape  (216,)\n",
            "y_train_counts\n",
            "{0: 216, 1: 216, 2: 215, 3: 215}\n",
            "y_validation_counts\n",
            "{0: 54, 1: 54, 2: 54, 3: 54}\n",
            "y_test_counts\n",
            "{0: 115, 1: 115, 2: 116, 3: 116}\n",
            "Results for Top K= 0\n",
            "[2]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.49074074074074076\n",
            "[[42 12  0  0]\n",
            " [26  9 10  9]\n",
            " [ 9  9 21 15]\n",
            " [ 2  0 18 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.78      0.63        54\n",
            "           1       0.30      0.17      0.21        54\n",
            "           2       0.43      0.39      0.41        54\n",
            "           3       0.59      0.63      0.61        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.46      0.49      0.47       216\n",
            "weighted avg       0.46      0.49      0.47       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5021645021645021\n",
            "[[91 23  1  0]\n",
            " [42 37 22 14]\n",
            " [22 27 41 26]\n",
            " [13  8 32 63]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.79      0.64       115\n",
            "           1       0.39      0.32      0.35       115\n",
            "           2       0.43      0.35      0.39       116\n",
            "           3       0.61      0.54      0.58       116\n",
            "\n",
            "    accuracy                           0.50       462\n",
            "   macro avg       0.49      0.50      0.49       462\n",
            "weighted avg       0.49      0.50      0.49       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.49537037037037035\n",
            "[[44  9  1  0]\n",
            " [20 17 11  6]\n",
            " [10 13 21 10]\n",
            " [ 2 10 17 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.81      0.68        54\n",
            "           1       0.35      0.31      0.33        54\n",
            "           2       0.42      0.39      0.40        54\n",
            "           3       0.61      0.46      0.53        54\n",
            "\n",
            "    accuracy                           0.50       216\n",
            "   macro avg       0.49      0.50      0.48       216\n",
            "weighted avg       0.49      0.50      0.48       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[96 15  1  3]\n",
            " [50 29 33  3]\n",
            " [31 28 40 17]\n",
            " [13 23 38 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.83      0.63       115\n",
            "           1       0.31      0.25      0.28       115\n",
            "           2       0.36      0.34      0.35       116\n",
            "           3       0.65      0.36      0.46       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.45      0.45      0.43       462\n",
            "weighted avg       0.45      0.45      0.43       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4583333333333333\n",
            "[[32 15  4  3]\n",
            " [19 15 11  9]\n",
            " [ 6 10 23 15]\n",
            " [ 0  9 16 29]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.59      0.58        54\n",
            "           1       0.31      0.28      0.29        54\n",
            "           2       0.43      0.43      0.43        54\n",
            "           3       0.52      0.54      0.53        54\n",
            "\n",
            "    accuracy                           0.46       216\n",
            "   macro avg       0.45      0.46      0.46       216\n",
            "weighted avg       0.45      0.46      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.3939393939393939\n",
            "[[68 31  9  7]\n",
            " [33 28 36 18]\n",
            " [22 35 36 23]\n",
            " [10 18 38 50]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.59      0.55       115\n",
            "           1       0.25      0.24      0.25       115\n",
            "           2       0.30      0.31      0.31       116\n",
            "           3       0.51      0.43      0.47       116\n",
            "\n",
            "    accuracy                           0.39       462\n",
            "   macro avg       0.39      0.39      0.39       462\n",
            "weighted avg       0.39      0.39      0.39       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4583333333333333\n",
            "[[32 15  4  3]\n",
            " [19 15 11  9]\n",
            " [ 6 10 23 15]\n",
            " [ 0  9 16 29]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.59      0.58        54\n",
            "           1       0.31      0.28      0.29        54\n",
            "           2       0.43      0.43      0.43        54\n",
            "           3       0.52      0.54      0.53        54\n",
            "\n",
            "    accuracy                           0.46       216\n",
            "   macro avg       0.45      0.46      0.46       216\n",
            "weighted avg       0.45      0.46      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.3939393939393939\n",
            "[[68 31  9  7]\n",
            " [33 28 36 18]\n",
            " [22 35 36 23]\n",
            " [10 18 38 50]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.59      0.55       115\n",
            "           1       0.25      0.24      0.25       115\n",
            "           2       0.30      0.31      0.31       116\n",
            "           3       0.51      0.43      0.47       116\n",
            "\n",
            "    accuracy                           0.39       462\n",
            "   macro avg       0.39      0.39      0.39       462\n",
            "weighted avg       0.39      0.39      0.39       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.48148148148148145\n",
            "[[49  4  1  0]\n",
            " [27  7  9 11]\n",
            " [11  4 10 29]\n",
            " [ 2  2 12 38]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.91      0.69        54\n",
            "           1       0.41      0.13      0.20        54\n",
            "           2       0.31      0.19      0.23        54\n",
            "           3       0.49      0.70      0.58        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.44      0.48      0.42       216\n",
            "weighted avg       0.44      0.48      0.42       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.4805194805194805\n",
            "[[104   5   5   1]\n",
            " [ 52  21  21  21]\n",
            " [ 33  12  27  44]\n",
            " [ 13   7  26  70]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.90      0.66       115\n",
            "           1       0.47      0.18      0.26       115\n",
            "           2       0.34      0.23      0.28       116\n",
            "           3       0.51      0.60      0.56       116\n",
            "\n",
            "    accuracy                           0.48       462\n",
            "   macro avg       0.46      0.48      0.44       462\n",
            "weighted avg       0.46      0.48      0.44       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 0\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.49074074074074076 , 0.49537037037037035 , 0.4583333333333333 , 0.4583333333333333 , 0.48148148148148145 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.5021645021645021 , 0.44805194805194803 , 0.3939393939393939 , 0.3939393939393939 , 0.4805194805194805 ]\n",
            "Results for Top K= 1\n",
            "[2, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4444444444444444\n",
            "[[38  8  4  4]\n",
            " [16 13 13 12]\n",
            " [ 7  5 13 29]\n",
            " [ 1  4 17 32]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.70      0.66        54\n",
            "           1       0.43      0.24      0.31        54\n",
            "           2       0.28      0.24      0.26        54\n",
            "           3       0.42      0.59      0.49        54\n",
            "\n",
            "    accuracy                           0.44       216\n",
            "   macro avg       0.43      0.44      0.43       216\n",
            "weighted avg       0.43      0.44      0.43       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.4393939393939394\n",
            "[[82 21  5  7]\n",
            " [26 29 28 32]\n",
            " [17 15 30 54]\n",
            " [12  9 33 62]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.71      0.65       115\n",
            "           1       0.39      0.25      0.31       115\n",
            "           2       0.31      0.26      0.28       116\n",
            "           3       0.40      0.53      0.46       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.43      0.44      0.42       462\n",
            "weighted avg       0.43      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5787037037037037\n",
            "[[49  3  2  0]\n",
            " [14 26 11  3]\n",
            " [ 9 10 21 14]\n",
            " [ 3  3 19 29]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.91      0.76        54\n",
            "           1       0.62      0.48      0.54        54\n",
            "           2       0.40      0.39      0.39        54\n",
            "           3       0.63      0.54      0.58        54\n",
            "\n",
            "    accuracy                           0.58       216\n",
            "   macro avg       0.57      0.58      0.57       216\n",
            "weighted avg       0.57      0.58      0.57       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5281385281385281\n",
            "[[100  14   1   0]\n",
            " [ 37  48  23   7]\n",
            " [ 23  28  43  22]\n",
            " [ 10  18  35  53]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.87      0.70       115\n",
            "           1       0.44      0.42      0.43       115\n",
            "           2       0.42      0.37      0.39       116\n",
            "           3       0.65      0.46      0.54       116\n",
            "\n",
            "    accuracy                           0.53       462\n",
            "   macro avg       0.53      0.53      0.52       462\n",
            "weighted avg       0.53      0.53      0.52       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5324074074074074\n",
            "[[38  4 10  2]\n",
            " [10 28  9  7]\n",
            " [ 3 11 18 22]\n",
            " [ 1  4 18 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.70      0.72        54\n",
            "           1       0.60      0.52      0.55        54\n",
            "           2       0.33      0.33      0.33        54\n",
            "           3       0.50      0.57      0.53        54\n",
            "\n",
            "    accuracy                           0.53       216\n",
            "   macro avg       0.54      0.53      0.53       216\n",
            "weighted avg       0.54      0.53      0.53       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5194805194805194\n",
            "[[89 16  8  2]\n",
            " [14 51 34 16]\n",
            " [ 7 33 43 33]\n",
            " [ 7 17 35 57]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.77      0.77       115\n",
            "           1       0.44      0.44      0.44       115\n",
            "           2       0.36      0.37      0.36       116\n",
            "           3       0.53      0.49      0.51       116\n",
            "\n",
            "    accuracy                           0.52       462\n",
            "   macro avg       0.52      0.52      0.52       462\n",
            "weighted avg       0.52      0.52      0.52       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5879629629629629\n",
            "[[44  7  2  1]\n",
            " [ 7 31 10  6]\n",
            " [ 5  9 18 22]\n",
            " [ 1  4 15 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.81      0.79        54\n",
            "           1       0.61      0.57      0.59        54\n",
            "           2       0.40      0.33      0.36        54\n",
            "           3       0.54      0.63      0.58        54\n",
            "\n",
            "    accuracy                           0.59       216\n",
            "   macro avg       0.58      0.59      0.58       216\n",
            "weighted avg       0.58      0.59      0.58       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5606060606060606\n",
            "[[98 13  2  2]\n",
            " [25 50 30 10]\n",
            " [ 9 31 53 23]\n",
            " [ 8 15 35 58]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.85      0.77       115\n",
            "           1       0.46      0.43      0.45       115\n",
            "           2       0.44      0.46      0.45       116\n",
            "           3       0.62      0.50      0.56       116\n",
            "\n",
            "    accuracy                           0.56       462\n",
            "   macro avg       0.56      0.56      0.55       462\n",
            "weighted avg       0.56      0.56      0.55       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5509259259259259\n",
            "[[46  7  1  0]\n",
            " [18 25 10  1]\n",
            " [10 10 15 19]\n",
            " [ 2  4 15 33]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.85      0.71        54\n",
            "           1       0.54      0.46      0.50        54\n",
            "           2       0.37      0.28      0.32        54\n",
            "           3       0.62      0.61      0.62        54\n",
            "\n",
            "    accuracy                           0.55       216\n",
            "   macro avg       0.53      0.55      0.54       216\n",
            "weighted avg       0.53      0.55      0.54       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5411255411255411\n",
            "[[105   8   1   1]\n",
            " [ 41  48  22   4]\n",
            " [ 20  34  32  30]\n",
            " [ 14  13  24  65]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.91      0.71       115\n",
            "           1       0.47      0.42      0.44       115\n",
            "           2       0.41      0.28      0.33       116\n",
            "           3       0.65      0.56      0.60       116\n",
            "\n",
            "    accuracy                           0.54       462\n",
            "   macro avg       0.53      0.54      0.52       462\n",
            "weighted avg       0.53      0.54      0.52       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 1\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4444444444444444 , 0.5787037037037037 , 0.5324074074074074 , 0.5879629629629629 , 0.5509259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4393939393939394 , 0.5281385281385281 , 0.5194805194805194 , 0.5606060606060606 , 0.5411255411255411 ]\n",
            "Results for Top K= 2\n",
            "[2, 16, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[38  8  4  4]\n",
            " [15 12 14 13]\n",
            " [ 7  4 13 30]\n",
            " [ 1  2  9 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.70      0.66        54\n",
            "           1       0.46      0.22      0.30        54\n",
            "           2       0.33      0.24      0.28        54\n",
            "           3       0.47      0.78      0.59        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.47      0.49      0.46       216\n",
            "weighted avg       0.47      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.4588744588744589\n",
            "[[82 21  5  7]\n",
            " [27 27 29 32]\n",
            " [18 13 25 60]\n",
            " [12  3 23 78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.71      0.65       115\n",
            "           1       0.42      0.23      0.30       115\n",
            "           2       0.30      0.22      0.25       116\n",
            "           3       0.44      0.67      0.53       116\n",
            "\n",
            "    accuracy                           0.46       462\n",
            "   macro avg       0.44      0.46      0.43       462\n",
            "weighted avg       0.44      0.46      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6111111111111112\n",
            "[[49  2  3  0]\n",
            " [14 27  8  5]\n",
            " [ 8 10 20 16]\n",
            " [ 1  2 15 36]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.91      0.78        54\n",
            "           1       0.66      0.50      0.57        54\n",
            "           2       0.43      0.37      0.40        54\n",
            "           3       0.63      0.67      0.65        54\n",
            "\n",
            "    accuracy                           0.61       216\n",
            "   macro avg       0.60      0.61      0.60       216\n",
            "weighted avg       0.60      0.61      0.60       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5541125541125541\n",
            "[[101  13   1   0]\n",
            " [ 34  53  22   6]\n",
            " [ 26  27  40  23]\n",
            " [ 10  17  27  62]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.88      0.71       115\n",
            "           1       0.48      0.46      0.47       115\n",
            "           2       0.44      0.34      0.39       116\n",
            "           3       0.68      0.53      0.60       116\n",
            "\n",
            "    accuracy                           0.55       462\n",
            "   macro avg       0.55      0.55      0.54       462\n",
            "weighted avg       0.55      0.55      0.54       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6388888888888888\n",
            "[[48  1  3  2]\n",
            " [ 8 31 13  2]\n",
            " [ 3 14 24 13]\n",
            " [ 0  2 17 35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85        54\n",
            "           1       0.65      0.57      0.61        54\n",
            "           2       0.42      0.44      0.43        54\n",
            "           3       0.67      0.65      0.66        54\n",
            "\n",
            "    accuracy                           0.64       216\n",
            "   macro avg       0.64      0.64      0.64       216\n",
            "weighted avg       0.64      0.64      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6060606060606061\n",
            "[[97 10  6  2]\n",
            " [12 63 23 17]\n",
            " [13 30 52 21]\n",
            " [ 5 11 32 68]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.84      0.80       115\n",
            "           1       0.55      0.55      0.55       115\n",
            "           2       0.46      0.45      0.45       116\n",
            "           3       0.63      0.59      0.61       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.60      0.61      0.60       462\n",
            "weighted avg       0.60      0.61      0.60       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7453703703703703\n",
            "[[51  3  0  0]\n",
            " [ 6 39  8  1]\n",
            " [ 4 10 30 10]\n",
            " [ 0  2 11 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89        54\n",
            "           1       0.72      0.72      0.72        54\n",
            "           2       0.61      0.56      0.58        54\n",
            "           3       0.79      0.76      0.77        54\n",
            "\n",
            "    accuracy                           0.75       216\n",
            "   macro avg       0.74      0.75      0.74       216\n",
            "weighted avg       0.74      0.75      0.74       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6493506493506493\n",
            "[[106   8   1   0]\n",
            " [ 17  66  27   5]\n",
            " [ 13  27  51  25]\n",
            " [  3  14  22  77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.92      0.83       115\n",
            "           1       0.57      0.57      0.57       115\n",
            "           2       0.50      0.44      0.47       116\n",
            "           3       0.72      0.66      0.69       116\n",
            "\n",
            "    accuracy                           0.65       462\n",
            "   macro avg       0.64      0.65      0.64       462\n",
            "weighted avg       0.64      0.65      0.64       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6435185185185185\n",
            "[[44  8  2  0]\n",
            " [10 34  9  1]\n",
            " [ 8 11 18 17]\n",
            " [ 0  2  9 43]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.81      0.76        54\n",
            "           1       0.62      0.63      0.62        54\n",
            "           2       0.47      0.33      0.39        54\n",
            "           3       0.70      0.80      0.75        54\n",
            "\n",
            "    accuracy                           0.64       216\n",
            "   macro avg       0.63      0.64      0.63       216\n",
            "weighted avg       0.63      0.64      0.63       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6147186147186147\n",
            "[[106   8   1   0]\n",
            " [ 32  56  24   3]\n",
            " [ 22  25  41  28]\n",
            " [ 10  10  15  81]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.92      0.74       115\n",
            "           1       0.57      0.49      0.52       115\n",
            "           2       0.51      0.35      0.42       116\n",
            "           3       0.72      0.70      0.71       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.60      0.62      0.60       462\n",
            "weighted avg       0.60      0.61      0.60       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 2\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.6111111111111112 , 0.6388888888888888 , 0.7453703703703703 , 0.6435185185185185 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4588744588744589 , 0.5541125541125541 , 0.6060606060606061 , 0.6493506493506493 , 0.6147186147186147 ]\n",
            "Results for Top K= 3\n",
            "[2, 16, 17, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5370370370370371\n",
            "[[36  9  9  0]\n",
            " [10 19 16  9]\n",
            " [ 4  7 31 12]\n",
            " [ 1  5 18 30]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.67      0.69        54\n",
            "           1       0.47      0.35      0.40        54\n",
            "           2       0.42      0.57      0.48        54\n",
            "           3       0.59      0.56      0.57        54\n",
            "\n",
            "    accuracy                           0.54       216\n",
            "   macro avg       0.55      0.54      0.54       216\n",
            "weighted avg       0.55      0.54      0.54       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5194805194805194\n",
            "[[78 23 14  0]\n",
            " [20 42 36 17]\n",
            " [11 14 65 26]\n",
            " [ 7 17 37 55]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.68      0.68       115\n",
            "           1       0.44      0.37      0.40       115\n",
            "           2       0.43      0.56      0.49       116\n",
            "           3       0.56      0.47      0.51       116\n",
            "\n",
            "    accuracy                           0.52       462\n",
            "   macro avg       0.52      0.52      0.52       462\n",
            "weighted avg       0.52      0.52      0.52       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 29 10  5]\n",
            " [ 5 11 26 12]\n",
            " [ 0 16 15 23]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.54      0.52        54\n",
            "           2       0.49      0.48      0.49        54\n",
            "           3       0.57      0.43      0.49        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5584415584415584\n",
            "[[110   4   1   0]\n",
            " [ 21  68  19   7]\n",
            " [ 26  23  42  25]\n",
            " [ 10  28  40  38]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.96      0.78       115\n",
            "           1       0.55      0.59      0.57       115\n",
            "           2       0.41      0.36      0.39       116\n",
            "           3       0.54      0.33      0.41       116\n",
            "\n",
            "    accuracy                           0.56       462\n",
            "   macro avg       0.54      0.56      0.54       462\n",
            "weighted avg       0.54      0.56      0.54       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  1  2  1]\n",
            " [ 5 34 11  4]\n",
            " [ 4 11 25 14]\n",
            " [ 0  1 16 37]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.88        54\n",
            "           1       0.72      0.63      0.67        54\n",
            "           2       0.46      0.46      0.46        54\n",
            "           3       0.66      0.69      0.67        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6168831168831169\n",
            "[[101   5   8   1]\n",
            " [ 12  64  26  13]\n",
            " [ 14  30  52  20]\n",
            " [  3  11  34  68]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.88      0.82       115\n",
            "           1       0.58      0.56      0.57       115\n",
            "           2       0.43      0.45      0.44       116\n",
            "           3       0.67      0.59      0.62       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.62      0.61       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7314814814814815\n",
            "[[50  2  1  1]\n",
            " [ 4 38 11  1]\n",
            " [ 4 12 29  9]\n",
            " [ 0  2 11 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89        54\n",
            "           1       0.70      0.70      0.70        54\n",
            "           2       0.56      0.54      0.55        54\n",
            "           3       0.79      0.76      0.77        54\n",
            "\n",
            "    accuracy                           0.73       216\n",
            "   macro avg       0.73      0.73      0.73       216\n",
            "weighted avg       0.73      0.73      0.73       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.658008658008658\n",
            "[[107   7   1   0]\n",
            " [ 17  69  22   7]\n",
            " [ 17  26  48  25]\n",
            " [  4  10  22  80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.93      0.82       115\n",
            "           1       0.62      0.60      0.61       115\n",
            "           2       0.52      0.41      0.46       116\n",
            "           3       0.71      0.69      0.70       116\n",
            "\n",
            "    accuracy                           0.66       462\n",
            "   macro avg       0.65      0.66      0.65       462\n",
            "weighted avg       0.65      0.66      0.65       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6527777777777778\n",
            "[[47  6  1  0]\n",
            " [12 30 10  2]\n",
            " [ 7 10 20 17]\n",
            " [ 0  1  9 44]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.87      0.78        54\n",
            "           1       0.64      0.56      0.59        54\n",
            "           2       0.50      0.37      0.43        54\n",
            "           3       0.70      0.81      0.75        54\n",
            "\n",
            "    accuracy                           0.65       216\n",
            "   macro avg       0.64      0.65      0.64       216\n",
            "weighted avg       0.64      0.65      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[104   9   2   0]\n",
            " [ 23  62  27   3]\n",
            " [ 21  21  44  30]\n",
            " [  9  11  16  80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.90      0.76       115\n",
            "           1       0.60      0.54      0.57       115\n",
            "           2       0.49      0.38      0.43       116\n",
            "           3       0.71      0.69      0.70       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.62       462\n",
            "weighted avg       0.62      0.63      0.62       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 3\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.5370370370370371 , 0.5972222222222222 , 0.6759259259259259 , 0.7314814814814815 , 0.6527777777777778 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.5194805194805194 , 0.5584415584415584 , 0.6168831168831169 , 0.658008658008658 , 0.6277056277056277 ]\n",
            "Results for Top K= 4\n",
            "[2, 7, 16, 17, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[35  8 11  0]\n",
            " [14 11 27  2]\n",
            " [ 4  2 45  3]\n",
            " [ 1  3 36 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.65      0.65        54\n",
            "           1       0.46      0.20      0.28        54\n",
            "           2       0.38      0.83      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44372294372294374\n",
            "[[73 20 22  0]\n",
            " [25 23 61  6]\n",
            " [15  6 81 14]\n",
            " [ 7 11 70 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.38      0.20      0.26       115\n",
            "           2       0.35      0.70      0.46       116\n",
            "           3       0.58      0.24      0.34       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.48      0.44      0.42       462\n",
            "weighted avg       0.48      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[110   4   1   0]\n",
            " [ 20  70  17   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.61      0.59       115\n",
            "           2       0.44      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6527777777777778\n",
            "[[45  7  0  2]\n",
            " [ 5 36  9  4]\n",
            " [ 2 12 26 14]\n",
            " [ 0  3 17 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.83      0.85        54\n",
            "           1       0.62      0.67      0.64        54\n",
            "           2       0.50      0.48      0.49        54\n",
            "           3       0.63      0.63      0.63        54\n",
            "\n",
            "    accuracy                           0.65       216\n",
            "   macro avg       0.65      0.65      0.65       216\n",
            "weighted avg       0.65      0.65      0.65       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6168831168831169\n",
            "[[89 19  7  0]\n",
            " [ 9 74 18 14]\n",
            " [ 8 33 52 23]\n",
            " [ 2 15 29 70]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.80       115\n",
            "           1       0.52      0.64      0.58       115\n",
            "           2       0.49      0.45      0.47       116\n",
            "           3       0.65      0.60      0.63       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.62      0.62      0.62       462\n",
            "weighted avg       0.62      0.62      0.62       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7314814814814815\n",
            "[[51  2  0  1]\n",
            " [ 5 37 12  0]\n",
            " [ 3 12 30  9]\n",
            " [ 0  2 12 40]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.94      0.90        54\n",
            "           1       0.70      0.69      0.69        54\n",
            "           2       0.56      0.56      0.56        54\n",
            "           3       0.80      0.74      0.77        54\n",
            "\n",
            "    accuracy                           0.73       216\n",
            "   macro avg       0.73      0.73      0.73       216\n",
            "weighted avg       0.73      0.73      0.73       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6818181818181818\n",
            "[[111   4   0   0]\n",
            " [ 13  73  26   3]\n",
            " [ 14  24  54  24]\n",
            " [  4  12  23  77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.97      0.86       115\n",
            "           1       0.65      0.63      0.64       115\n",
            "           2       0.52      0.47      0.49       116\n",
            "           3       0.74      0.66      0.70       116\n",
            "\n",
            "    accuracy                           0.68       462\n",
            "   macro avg       0.67      0.68      0.67       462\n",
            "weighted avg       0.67      0.68      0.67       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6666666666666666\n",
            "[[46  6  2  0]\n",
            " [ 8 35  9  2]\n",
            " [ 8  9 22 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.85      0.79        54\n",
            "           1       0.69      0.65      0.67        54\n",
            "           2       0.49      0.41      0.44        54\n",
            "           3       0.71      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.67       216\n",
            "   macro avg       0.66      0.67      0.66       216\n",
            "weighted avg       0.66      0.67      0.66       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[107   8   0   0]\n",
            " [ 25  56  30   4]\n",
            " [ 18  20  49  29]\n",
            " [ 10   8  20  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.93      0.78       115\n",
            "           1       0.61      0.49      0.54       115\n",
            "           2       0.49      0.42      0.46       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.62       462\n",
            "weighted avg       0.62      0.63      0.62       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 4\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.6527777777777778 , 0.7314814814814815 , 0.6666666666666666 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44372294372294374 , 0.5735930735930735 , 0.6168831168831169 , 0.6818181818181818 , 0.6277056277056277 ]\n",
            "Results for Top K= 5\n",
            "[2, 6, 7, 16, 17, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[35  8 11  0]\n",
            " [14 11 27  2]\n",
            " [ 4  2 45  3]\n",
            " [ 1  3 36 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.65      0.65        54\n",
            "           1       0.46      0.20      0.28        54\n",
            "           2       0.38      0.83      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44155844155844154\n",
            "[[72 21 22  0]\n",
            " [25 23 61  6]\n",
            " [15  6 81 14]\n",
            " [ 7 11 70 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.38      0.20      0.26       115\n",
            "           2       0.35      0.70      0.46       116\n",
            "           3       0.58      0.24      0.34       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.48      0.44      0.42       462\n",
            "weighted avg       0.48      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[110   4   1   0]\n",
            " [ 20  70  17   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.61      0.59       115\n",
            "           2       0.44      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6342592592592593\n",
            "[[47  2  4  1]\n",
            " [ 6 34 12  2]\n",
            " [ 3 12 22 17]\n",
            " [ 0  4 16 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85        54\n",
            "           1       0.65      0.63      0.64        54\n",
            "           2       0.41      0.41      0.41        54\n",
            "           3       0.63      0.63      0.63        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.63      0.63      0.63       216\n",
            "weighted avg       0.63      0.63      0.63       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6017316017316018\n",
            "[[94 12  5  4]\n",
            " [10 71 24 10]\n",
            " [ 6 35 50 25]\n",
            " [ 2 17 34 63]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       115\n",
            "           1       0.53      0.62      0.57       115\n",
            "           2       0.44      0.43      0.44       116\n",
            "           3       0.62      0.54      0.58       116\n",
            "\n",
            "    accuracy                           0.60       462\n",
            "   macro avg       0.61      0.60      0.60       462\n",
            "weighted avg       0.61      0.60      0.60       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7453703703703703\n",
            "[[52  0  1  1]\n",
            " [ 6 37 10  1]\n",
            " [ 3  9 31 11]\n",
            " [ 0  2 11 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.96      0.90        54\n",
            "           1       0.77      0.69      0.73        54\n",
            "           2       0.58      0.57      0.58        54\n",
            "           3       0.76      0.76      0.76        54\n",
            "\n",
            "    accuracy                           0.75       216\n",
            "   macro avg       0.74      0.75      0.74       216\n",
            "weighted avg       0.74      0.75      0.74       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.670995670995671\n",
            "[[103   8   3   1]\n",
            " [ 13  75  25   2]\n",
            " [ 13  26  55  22]\n",
            " [  5  11  23  77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.90      0.83       115\n",
            "           1       0.62      0.65      0.64       115\n",
            "           2       0.52      0.47      0.50       116\n",
            "           3       0.75      0.66      0.71       116\n",
            "\n",
            "    accuracy                           0.67       462\n",
            "   macro avg       0.67      0.67      0.67       462\n",
            "weighted avg       0.67      0.67      0.67       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6666666666666666\n",
            "[[49  4  1  0]\n",
            " [ 9 33  9  3]\n",
            " [ 7 10 21 16]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.91      0.82        54\n",
            "           1       0.69      0.61      0.65        54\n",
            "           2       0.49      0.39      0.43        54\n",
            "           3       0.68      0.76      0.72        54\n",
            "\n",
            "    accuracy                           0.67       216\n",
            "   macro avg       0.65      0.67      0.66       216\n",
            "weighted avg       0.65      0.67      0.66       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6320346320346321\n",
            "[[110   5   0   0]\n",
            " [ 23  62  26   4]\n",
            " [ 23  24  41  28]\n",
            " [  8   6  23  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.64      0.54      0.58       115\n",
            "           2       0.46      0.35      0.40       116\n",
            "           3       0.71      0.68      0.70       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.62       462\n",
            "weighted avg       0.62      0.63      0.62       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 5\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.6342592592592593 , 0.7453703703703703 , 0.6666666666666666 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44155844155844154 , 0.5735930735930735 , 0.6017316017316018 , 0.670995670995671 , 0.6320346320346321 ]\n",
            "Results for Top K= 6\n",
            "[2, 6, 7, 12, 16, 17, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.47685185185185186\n",
            "[[33  8 13  0]\n",
            " [14 11 27  2]\n",
            " [ 4  2 45  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        54\n",
            "           1       0.46      0.20      0.28        54\n",
            "           2       0.37      0.83      0.51        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.55      0.48      0.45       216\n",
            "weighted avg       0.55      0.48      0.45       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44155844155844154\n",
            "[[72 21 22  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 7  8 73 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.40      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.56      0.24      0.34       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.48      0.44      0.42       462\n",
            "weighted avg       0.48      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[110   4   1   0]\n",
            " [ 20  70  17   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.61      0.59       115\n",
            "           2       0.44      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6342592592592593\n",
            "[[46  6  1  1]\n",
            " [ 5 35 12  2]\n",
            " [ 2 13 24 15]\n",
            " [ 0  4 18 32]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86        54\n",
            "           1       0.60      0.65      0.62        54\n",
            "           2       0.44      0.44      0.44        54\n",
            "           3       0.64      0.59      0.62        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.64      0.63      0.64       216\n",
            "weighted avg       0.64      0.63      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6147186147186147\n",
            "[[92 16  7  0]\n",
            " [ 8 73 24 10]\n",
            " [ 7 35 53 21]\n",
            " [ 2 15 33 66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82       115\n",
            "           1       0.53      0.63      0.57       115\n",
            "           2       0.45      0.46      0.45       116\n",
            "           3       0.68      0.57      0.62       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.63      0.62      0.62       462\n",
            "weighted avg       0.63      0.61      0.62       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.75\n",
            "[[52  0  1  1]\n",
            " [ 5 38 10  1]\n",
            " [ 3 11 30 10]\n",
            " [ 0  2 10 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91        54\n",
            "           1       0.75      0.70      0.72        54\n",
            "           2       0.59      0.56      0.57        54\n",
            "           3       0.78      0.78      0.78        54\n",
            "\n",
            "    accuracy                           0.75       216\n",
            "   macro avg       0.74      0.75      0.75       216\n",
            "weighted avg       0.74      0.75      0.75       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.645021645021645\n",
            "[[105   8   2   0]\n",
            " [ 13  71  25   6]\n",
            " [ 17  26  44  29]\n",
            " [  5   9  24  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.91      0.82       115\n",
            "           1       0.62      0.62      0.62       115\n",
            "           2       0.46      0.38      0.42       116\n",
            "           3       0.69      0.67      0.68       116\n",
            "\n",
            "    accuracy                           0.65       462\n",
            "   macro avg       0.63      0.65      0.64       462\n",
            "weighted avg       0.63      0.65      0.64       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[49  4  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 11 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.91      0.82        54\n",
            "           1       0.70      0.59      0.64        54\n",
            "           2       0.51      0.43      0.46        54\n",
            "           3       0.70      0.78      0.74        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6320346320346321\n",
            "[[110   5   0   0]\n",
            " [ 21  63  27   4]\n",
            " [ 23  25  41  27]\n",
            " [  9   8  21  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.62      0.55      0.58       115\n",
            "           2       0.46      0.35      0.40       116\n",
            "           3       0.72      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.62       462\n",
            "weighted avg       0.62      0.63      0.62       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 6\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.47685185185185186 , 0.5972222222222222 , 0.6342592592592593 , 0.75 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44155844155844154 , 0.5735930735930735 , 0.6147186147186147 , 0.645021645021645 , 0.6320346320346321 ]\n",
            "Results for Top K= 7\n",
            "[2, 6, 7, 9, 12, 16, 17, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.47685185185185186\n",
            "[[33  8 13  0]\n",
            " [14 11 27  2]\n",
            " [ 4  2 45  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        54\n",
            "           1       0.46      0.20      0.28        54\n",
            "           2       0.37      0.83      0.51        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.55      0.48      0.45       216\n",
            "weighted avg       0.55      0.48      0.45       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44155844155844154\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 7  8 73 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.40      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.56      0.24      0.34       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.48      0.44      0.42       462\n",
            "weighted avg       0.48      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[110   4   1   0]\n",
            " [ 20  70  17   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.61      0.59       115\n",
            "           2       0.44      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6342592592592593\n",
            "[[45  3  4  2]\n",
            " [ 2 38 10  4]\n",
            " [ 3 10 23 18]\n",
            " [ 0  4 19 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.83      0.87        54\n",
            "           1       0.69      0.70      0.70        54\n",
            "           2       0.41      0.43      0.42        54\n",
            "           3       0.56      0.57      0.57        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.64      0.63      0.64       216\n",
            "weighted avg       0.64      0.63      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5930735930735931\n",
            "[[92 15  8  0]\n",
            " [14 64 24 13]\n",
            " [11 30 52 23]\n",
            " [ 3 14 33 66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.80      0.78       115\n",
            "           1       0.52      0.56      0.54       115\n",
            "           2       0.44      0.45      0.45       116\n",
            "           3       0.65      0.57      0.61       116\n",
            "\n",
            "    accuracy                           0.59       462\n",
            "   macro avg       0.59      0.59      0.59       462\n",
            "weighted avg       0.59      0.59      0.59       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7175925925925926\n",
            "[[51  0  1  2]\n",
            " [ 6 37 10  1]\n",
            " [ 4 11 28 11]\n",
            " [ 0  2 13 39]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89        54\n",
            "           1       0.74      0.69      0.71        54\n",
            "           2       0.54      0.52      0.53        54\n",
            "           3       0.74      0.72      0.73        54\n",
            "\n",
            "    accuracy                           0.72       216\n",
            "   macro avg       0.71      0.72      0.71       216\n",
            "weighted avg       0.71      0.72      0.71       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6493506493506493\n",
            "[[102   8   4   1]\n",
            " [ 11  67  32   5]\n",
            " [ 14  22  52  28]\n",
            " [  6   6  25  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.89      0.82       115\n",
            "           1       0.65      0.58      0.61       115\n",
            "           2       0.46      0.45      0.45       116\n",
            "           3       0.70      0.68      0.69       116\n",
            "\n",
            "    accuracy                           0.65       462\n",
            "   macro avg       0.64      0.65      0.65       462\n",
            "weighted avg       0.64      0.65      0.65       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[49  4  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 11 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.91      0.82        54\n",
            "           1       0.70      0.59      0.64        54\n",
            "           2       0.51      0.43      0.46        54\n",
            "           3       0.70      0.78      0.74        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6320346320346321\n",
            "[[110   5   0   0]\n",
            " [ 21  63  27   4]\n",
            " [ 23  25  41  27]\n",
            " [  9   8  21  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.62      0.55      0.58       115\n",
            "           2       0.46      0.35      0.40       116\n",
            "           3       0.72      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.62       462\n",
            "weighted avg       0.62      0.63      0.62       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 7\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.47685185185185186 , 0.5972222222222222 , 0.6342592592592593 , 0.7175925925925926 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44155844155844154 , 0.5735930735930735 , 0.5930735930735931 , 0.6493506493506493 , 0.6320346320346321 ]\n",
            "Results for Top K= 8\n",
            "[2, 6, 7, 9, 12, 15, 16, 17, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.47685185185185186\n",
            "[[33  8 13  0]\n",
            " [14 11 27  2]\n",
            " [ 4  2 45  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        54\n",
            "           1       0.46      0.20      0.28        54\n",
            "           2       0.37      0.83      0.51        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.55      0.48      0.45       216\n",
            "weighted avg       0.55      0.48      0.45       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44155844155844154\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 7  8 73 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.40      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.56      0.24      0.34       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.48      0.44      0.42       462\n",
            "weighted avg       0.48      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[110   4   1   0]\n",
            " [ 20  70  17   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.61      0.59       115\n",
            "           2       0.44      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6388888888888888\n",
            "[[46  5  1  2]\n",
            " [ 2 38 11  3]\n",
            " [ 3 11 22 18]\n",
            " [ 0  4 18 32]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.85      0.88        54\n",
            "           1       0.66      0.70      0.68        54\n",
            "           2       0.42      0.41      0.42        54\n",
            "           3       0.58      0.59      0.59        54\n",
            "\n",
            "    accuracy                           0.64       216\n",
            "   macro avg       0.64      0.64      0.64       216\n",
            "weighted avg       0.64      0.64      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5930735930735931\n",
            "[[95 15  5  0]\n",
            " [12 70 24  9]\n",
            " [11 32 48 25]\n",
            " [ 3 18 34 61]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.83      0.81       115\n",
            "           1       0.52      0.61      0.56       115\n",
            "           2       0.43      0.41      0.42       116\n",
            "           3       0.64      0.53      0.58       116\n",
            "\n",
            "    accuracy                           0.59       462\n",
            "   macro avg       0.59      0.59      0.59       462\n",
            "weighted avg       0.59      0.59      0.59       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7407407407407407\n",
            "[[52  0  1  1]\n",
            " [ 5 38 10  1]\n",
            " [ 3 11 30 10]\n",
            " [ 0  2 12 40]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91        54\n",
            "           1       0.75      0.70      0.72        54\n",
            "           2       0.57      0.56      0.56        54\n",
            "           3       0.77      0.74      0.75        54\n",
            "\n",
            "    accuracy                           0.74       216\n",
            "   macro avg       0.74      0.74      0.74       216\n",
            "weighted avg       0.74      0.74      0.74       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6601731601731602\n",
            "[[104   8   2   1]\n",
            " [ 13  69  26   7]\n",
            " [ 13  23  52  28]\n",
            " [  5   6  25  80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.90      0.83       115\n",
            "           1       0.65      0.60      0.62       115\n",
            "           2       0.50      0.45      0.47       116\n",
            "           3       0.69      0.69      0.69       116\n",
            "\n",
            "    accuracy                           0.66       462\n",
            "   macro avg       0.65      0.66      0.65       462\n",
            "weighted avg       0.65      0.66      0.65       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6666666666666666\n",
            "[[49  4  1  0]\n",
            " [10 31 10  3]\n",
            " [ 6 10 22 16]\n",
            " [ 0  1 11 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.91      0.82        54\n",
            "           1       0.67      0.57      0.62        54\n",
            "           2       0.50      0.41      0.45        54\n",
            "           3       0.69      0.78      0.73        54\n",
            "\n",
            "    accuracy                           0.67       216\n",
            "   macro avg       0.65      0.67      0.66       216\n",
            "weighted avg       0.65      0.67      0.66       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 24  58  30   3]\n",
            " [ 22  23  43  28]\n",
            " [  9   7  21  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.62      0.50      0.56       115\n",
            "           2       0.46      0.37      0.41       116\n",
            "           3       0.72      0.68      0.70       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 8\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.47685185185185186 , 0.5972222222222222 , 0.6388888888888888 , 0.7407407407407407 , 0.6666666666666666 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44155844155844154 , 0.5735930735930735 , 0.5930735930735931 , 0.6601731601731602 , 0.6277056277056277 ]\n",
            "Results for Top K= 9\n",
            "[2, 6, 7, 9, 12, 13, 15, 16, 17, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.47685185185185186\n",
            "[[33  8 13  0]\n",
            " [14 11 27  2]\n",
            " [ 4  2 45  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        54\n",
            "           1       0.46      0.20      0.28        54\n",
            "           2       0.37      0.83      0.51        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.55      0.48      0.45       216\n",
            "weighted avg       0.55      0.48      0.45       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44155844155844154\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 7  8 73 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.40      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.56      0.24      0.34       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.48      0.44      0.42       462\n",
            "weighted avg       0.48      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[110   4   1   0]\n",
            " [ 20  70  17   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.61      0.59       115\n",
            "           2       0.44      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6435185185185185\n",
            "[[45  7  0  2]\n",
            " [ 3 35 12  4]\n",
            " [ 2 12 24 16]\n",
            " [ 0  4 15 35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.83      0.87        54\n",
            "           1       0.60      0.65      0.62        54\n",
            "           2       0.47      0.44      0.46        54\n",
            "           3       0.61      0.65      0.63        54\n",
            "\n",
            "    accuracy                           0.64       216\n",
            "   macro avg       0.65      0.64      0.64       216\n",
            "weighted avg       0.65      0.64      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6147186147186147\n",
            "[[93 16  5  1]\n",
            " [ 9 69 24 13]\n",
            " [ 5 32 56 23]\n",
            " [ 3 15 32 66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83       115\n",
            "           1       0.52      0.60      0.56       115\n",
            "           2       0.48      0.48      0.48       116\n",
            "           3       0.64      0.57      0.60       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.62      0.62      0.62       462\n",
            "weighted avg       0.62      0.61      0.62       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7361111111111112\n",
            "[[50  0  2  2]\n",
            " [ 6 37 10  1]\n",
            " [ 3 12 30  9]\n",
            " [ 0  2 10 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.88        54\n",
            "           1       0.73      0.69      0.70        54\n",
            "           2       0.58      0.56      0.57        54\n",
            "           3       0.78      0.78      0.78        54\n",
            "\n",
            "    accuracy                           0.74       216\n",
            "   macro avg       0.73      0.74      0.73       216\n",
            "weighted avg       0.73      0.74      0.73       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6471861471861472\n",
            "[[104   7   3   1]\n",
            " [ 13  68  28   6]\n",
            " [ 17  25  48  26]\n",
            " [  6   7  24  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.90      0.82       115\n",
            "           1       0.64      0.59      0.61       115\n",
            "           2       0.47      0.41      0.44       116\n",
            "           3       0.71      0.68      0.69       116\n",
            "\n",
            "    accuracy                           0.65       462\n",
            "   macro avg       0.64      0.65      0.64       462\n",
            "weighted avg       0.64      0.65      0.64       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7 10 23 14]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.70      0.59      0.64        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.71      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6233766233766234\n",
            "[[110   5   0   0]\n",
            " [ 23  57  30   5]\n",
            " [ 21  23  41  31]\n",
            " [  8   7  21  80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.62      0.50      0.55       115\n",
            "           2       0.45      0.35      0.39       116\n",
            "           3       0.69      0.69      0.69       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.62      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 9\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.47685185185185186 , 0.5972222222222222 , 0.6435185185185185 , 0.7361111111111112 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44155844155844154 , 0.5735930735930735 , 0.6147186147186147 , 0.6471861471861472 , 0.6233766233766234 ]\n",
            "Results for Top K= 10\n",
            "[2, 6, 7, 9, 10, 12, 13, 15, 16, 17, 20]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.47685185185185186\n",
            "[[33  8 13  0]\n",
            " [14 11 27  2]\n",
            " [ 4  2 45  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        54\n",
            "           1       0.46      0.20      0.28        54\n",
            "           2       0.37      0.83      0.51        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.55      0.48      0.45       216\n",
            "weighted avg       0.55      0.48      0.45       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44155844155844154\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 7  7 74 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.56      0.24      0.34       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.48      0.44      0.42       462\n",
            "weighted avg       0.48      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[110   4   1   0]\n",
            " [ 20  70  17   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.61      0.59       115\n",
            "           2       0.44      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6342592592592593\n",
            "[[46  6  0  2]\n",
            " [ 5 34 12  3]\n",
            " [ 2 11 23 18]\n",
            " [ 0  4 16 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86        54\n",
            "           1       0.62      0.63      0.62        54\n",
            "           2       0.45      0.43      0.44        54\n",
            "           3       0.60      0.63      0.61        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.63      0.63      0.63       216\n",
            "weighted avg       0.63      0.63      0.63       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6082251082251082\n",
            "[[93 16  6  0]\n",
            " [ 9 71 23 12]\n",
            " [ 8 32 50 26]\n",
            " [ 2 18 29 67]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.81      0.82       115\n",
            "           1       0.52      0.62      0.56       115\n",
            "           2       0.46      0.43      0.45       116\n",
            "           3       0.64      0.58      0.61       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.61      0.61      0.61       462\n",
            "weighted avg       0.61      0.61      0.61       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7268518518518519\n",
            "[[50  0  2  2]\n",
            " [ 6 37 10  1]\n",
            " [ 3 11 30 10]\n",
            " [ 0  2 12 40]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.88        54\n",
            "           1       0.74      0.69      0.71        54\n",
            "           2       0.56      0.56      0.56        54\n",
            "           3       0.75      0.74      0.75        54\n",
            "\n",
            "    accuracy                           0.73       216\n",
            "   macro avg       0.72      0.73      0.72       216\n",
            "weighted avg       0.72      0.73      0.72       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.645021645021645\n",
            "[[103   8   4   0]\n",
            " [ 15  68  26   6]\n",
            " [ 15  25  48  28]\n",
            " [  6   7  24  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.90      0.81       115\n",
            "           1       0.63      0.59      0.61       115\n",
            "           2       0.47      0.41      0.44       116\n",
            "           3       0.70      0.68      0.69       116\n",
            "\n",
            "    accuracy                           0.65       462\n",
            "   macro avg       0.64      0.65      0.64       462\n",
            "weighted avg       0.63      0.65      0.64       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7 10 23 14]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.70      0.59      0.64        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.71      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6233766233766234\n",
            "[[110   5   0   0]\n",
            " [ 23  57  30   5]\n",
            " [ 21  23  41  31]\n",
            " [  8   7  21  80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.62      0.50      0.55       115\n",
            "           2       0.45      0.35      0.39       116\n",
            "           3       0.69      0.69      0.69       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.62      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 10\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.47685185185185186 , 0.5972222222222222 , 0.6342592592592593 , 0.7268518518518519 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44155844155844154 , 0.5735930735930735 , 0.6082251082251082 , 0.645021645021645 , 0.6233766233766234 ]\n",
            "Results for Top K= 11\n",
            "[2, 6, 7, 9, 10, 12, 13, 15, 16, 17, 20, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.47685185185185186\n",
            "[[33  8 13  0]\n",
            " [14 11 27  2]\n",
            " [ 4  2 45  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        54\n",
            "           1       0.46      0.20      0.28        54\n",
            "           2       0.37      0.83      0.51        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.55      0.48      0.45       216\n",
            "weighted avg       0.55      0.48      0.45       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44372294372294374\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 74 29]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.57      0.25      0.35       116\n",
            "\n",
            "    accuracy                           0.44       462\n",
            "   macro avg       0.48      0.44      0.42       462\n",
            "weighted avg       0.48      0.44      0.42       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5735930735930735\n",
            "[[110   4   1   0]\n",
            " [ 20  70  17   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.61      0.59       115\n",
            "           2       0.44      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.56      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6388888888888888\n",
            "[[46  6  0  2]\n",
            " [ 5 35 11  3]\n",
            " [ 2 12 24 16]\n",
            " [ 0  4 17 33]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86        54\n",
            "           1       0.61      0.65      0.63        54\n",
            "           2       0.46      0.44      0.45        54\n",
            "           3       0.61      0.61      0.61        54\n",
            "\n",
            "    accuracy                           0.64       216\n",
            "   macro avg       0.64      0.64      0.64       216\n",
            "weighted avg       0.64      0.64      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6060606060606061\n",
            "[[94 18  3  0]\n",
            " [ 9 68 27 11]\n",
            " [ 7 32 50 27]\n",
            " [ 2 15 31 68]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       115\n",
            "           1       0.51      0.59      0.55       115\n",
            "           2       0.45      0.43      0.44       116\n",
            "           3       0.64      0.59      0.61       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.61      0.61      0.61       462\n",
            "weighted avg       0.61      0.61      0.61       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7453703703703703\n",
            "[[50  0  2  2]\n",
            " [ 7 37 10  0]\n",
            " [ 3 12 30  9]\n",
            " [ 1  2  7 44]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87        54\n",
            "           1       0.73      0.69      0.70        54\n",
            "           2       0.61      0.56      0.58        54\n",
            "           3       0.80      0.81      0.81        54\n",
            "\n",
            "    accuracy                           0.75       216\n",
            "   macro avg       0.74      0.75      0.74       216\n",
            "weighted avg       0.74      0.75      0.74       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6601731601731602\n",
            "[[105   8   2   0]\n",
            " [ 15  68  26   6]\n",
            " [ 15  22  53  26]\n",
            " [  7   7  23  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.91      0.82       115\n",
            "           1       0.65      0.59      0.62       115\n",
            "           2       0.51      0.46      0.48       116\n",
            "           3       0.71      0.68      0.70       116\n",
            "\n",
            "    accuracy                           0.66       462\n",
            "   macro avg       0.65      0.66      0.65       462\n",
            "weighted avg       0.65      0.66      0.65       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7 10 23 14]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.70      0.59      0.64        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.71      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6233766233766234\n",
            "[[110   5   0   0]\n",
            " [ 23  57  30   5]\n",
            " [ 21  23  41  31]\n",
            " [  8   7  21  80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.62      0.50      0.55       115\n",
            "           2       0.45      0.35      0.39       116\n",
            "           3       0.69      0.69      0.69       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.62      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 11\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.47685185185185186 , 0.5972222222222222 , 0.6388888888888888 , 0.7453703703703703 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44372294372294374 , 0.5735930735930735 , 0.6060606060606061 , 0.6601731601731602 , 0.6233766233766234 ]\n",
            "Results for Top K= 12\n",
            "[2, 6, 7, 8, 9, 10, 12, 13, 15, 16, 17, 20, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.48148148148148145\n",
            "[[33  8 13  0]\n",
            " [14 11 26  3]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.70      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.55      0.48      0.45       216\n",
            "weighted avg       0.55      0.48      0.45       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  69  18   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.60      0.58       115\n",
            "           2       0.43      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.55      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6435185185185185\n",
            "[[45  6  1  2]\n",
            " [ 2 37 12  3]\n",
            " [ 3 12 24 15]\n",
            " [ 0  4 17 33]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.83      0.87        54\n",
            "           1       0.63      0.69      0.65        54\n",
            "           2       0.44      0.44      0.44        54\n",
            "           3       0.62      0.61      0.62        54\n",
            "\n",
            "    accuracy                           0.64       216\n",
            "   macro avg       0.65      0.64      0.65       216\n",
            "weighted avg       0.65      0.64      0.65       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6125541125541125\n",
            "[[94 13  8  0]\n",
            " [14 66 25 10]\n",
            " [10 28 56 22]\n",
            " [ 3 17 29 67]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       115\n",
            "           1       0.53      0.57      0.55       115\n",
            "           2       0.47      0.48      0.48       116\n",
            "           3       0.68      0.58      0.62       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.62      0.61      0.61       462\n",
            "weighted avg       0.61      0.61      0.61       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7407407407407407\n",
            "[[50  1  1  2]\n",
            " [ 6 37 11  0]\n",
            " [ 3 12 30  9]\n",
            " [ 1  2  8 43]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88        54\n",
            "           1       0.71      0.69      0.70        54\n",
            "           2       0.60      0.56      0.58        54\n",
            "           3       0.80      0.80      0.80        54\n",
            "\n",
            "    accuracy                           0.74       216\n",
            "   macro avg       0.74      0.74      0.74       216\n",
            "weighted avg       0.74      0.74      0.74       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6428571428571429\n",
            "[[103   7   4   1]\n",
            " [ 15  64  24  12]\n",
            " [ 14  21  53  28]\n",
            " [  6   7  26  77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.90      0.81       115\n",
            "           1       0.65      0.56      0.60       115\n",
            "           2       0.50      0.46      0.48       116\n",
            "           3       0.65      0.66      0.66       116\n",
            "\n",
            "    accuracy                           0.64       462\n",
            "   macro avg       0.64      0.64      0.64       462\n",
            "weighted avg       0.63      0.64      0.64       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 12\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.48148148148148145 , 0.5972222222222222 , 0.6435185185185185 , 0.7407407407407407 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.6125541125541125 , 0.6428571428571429 , 0.6277056277056277 ]\n",
            "Results for Top K= 13\n",
            "[2, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 20, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.48148148148148145\n",
            "[[33  8 13  0]\n",
            " [14 11 26  3]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.70      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.48       216\n",
            "   macro avg       0.55      0.48      0.45       216\n",
            "weighted avg       0.55      0.48      0.45       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5692640692640693\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.57       115\n",
            "           2       0.43      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.55      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6342592592592593\n",
            "[[45  7  0  2]\n",
            " [ 5 33 13  3]\n",
            " [ 2 11 25 16]\n",
            " [ 0  4 16 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.83      0.85        54\n",
            "           1       0.60      0.61      0.61        54\n",
            "           2       0.46      0.46      0.46        54\n",
            "           3       0.62      0.63      0.62        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.64      0.63      0.64       216\n",
            "weighted avg       0.64      0.63      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6298701298701299\n",
            "[[93 15  7  0]\n",
            " [ 9 71 24 11]\n",
            " [ 6 31 60 19]\n",
            " [ 3 15 31 67]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.82       115\n",
            "           1       0.54      0.62      0.57       115\n",
            "           2       0.49      0.52      0.50       116\n",
            "           3       0.69      0.58      0.63       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.64      0.63      0.63       462\n",
            "weighted avg       0.64      0.63      0.63       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7175925925925926\n",
            "[[50  1  1  2]\n",
            " [ 7 37 10  0]\n",
            " [ 3 13 27 11]\n",
            " [ 1  2 10 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87        54\n",
            "           1       0.70      0.69      0.69        54\n",
            "           2       0.56      0.50      0.53        54\n",
            "           3       0.76      0.76      0.76        54\n",
            "\n",
            "    accuracy                           0.72       216\n",
            "   macro avg       0.71      0.72      0.71       216\n",
            "weighted avg       0.71      0.72      0.71       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6168831168831169\n",
            "[[100   9   4   2]\n",
            " [ 15  62  30   8]\n",
            " [ 18  20  49  29]\n",
            " [  7   8  27  74]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.87      0.78       115\n",
            "           1       0.63      0.54      0.58       115\n",
            "           2       0.45      0.42      0.43       116\n",
            "           3       0.65      0.64      0.65       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.62      0.61       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 13\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.48148148148148145 , 0.5972222222222222 , 0.6342592592592593 , 0.7175925925925926 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5692640692640693 , 0.6298701298701299 , 0.6168831168831169 , 0.6277056277056277 ]\n",
            "Results for Top K= 14\n",
            "[2, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[34  8 12  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5692640692640693\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.57       115\n",
            "           2       0.43      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.55      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6388888888888888\n",
            "[[47  2  3  2]\n",
            " [ 5 34 12  3]\n",
            " [ 3 12 23 16]\n",
            " [ 0  4 16 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86        54\n",
            "           1       0.65      0.63      0.64        54\n",
            "           2       0.43      0.43      0.43        54\n",
            "           3       0.62      0.63      0.62        54\n",
            "\n",
            "    accuracy                           0.64       216\n",
            "   macro avg       0.64      0.64      0.64       216\n",
            "weighted avg       0.64      0.64      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6233766233766234\n",
            "[[95 14  6  0]\n",
            " [ 8 73 24 10]\n",
            " [ 8 33 51 24]\n",
            " [ 2 17 28 69]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.83       115\n",
            "           1       0.53      0.63      0.58       115\n",
            "           2       0.47      0.44      0.45       116\n",
            "           3       0.67      0.59      0.63       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.63      0.62      0.62       462\n",
            "weighted avg       0.63      0.62      0.62       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7222222222222222\n",
            "[[48  1  3  2]\n",
            " [ 8 36 10  0]\n",
            " [ 3 10 31 10]\n",
            " [ 2  2  9 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.83        54\n",
            "           1       0.73      0.67      0.70        54\n",
            "           2       0.58      0.57      0.58        54\n",
            "           3       0.77      0.76      0.77        54\n",
            "\n",
            "    accuracy                           0.72       216\n",
            "   macro avg       0.72      0.72      0.72       216\n",
            "weighted avg       0.72      0.72      0.72       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6147186147186147\n",
            "[[100  10   4   1]\n",
            " [ 13  65  28   9]\n",
            " [ 16  22  45  33]\n",
            " [  6   7  29  74]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.87      0.80       115\n",
            "           1       0.62      0.57      0.59       115\n",
            "           2       0.42      0.39      0.41       116\n",
            "           3       0.63      0.64      0.64       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.61      0.61       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 14\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.6388888888888888 , 0.7222222222222222 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5692640692640693 , 0.6233766233766234 , 0.6147186147186147 , 0.6277056277056277 ]\n",
            "Results for Top K= 15\n",
            "[2, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[34  8 12  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5692640692640693\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.57       115\n",
            "           2       0.43      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.55      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6435185185185185\n",
            "[[46  6  0  2]\n",
            " [ 5 35 10  4]\n",
            " [ 2 12 24 16]\n",
            " [ 0  3 17 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86        54\n",
            "           1       0.62      0.65      0.64        54\n",
            "           2       0.47      0.44      0.46        54\n",
            "           3       0.61      0.63      0.62        54\n",
            "\n",
            "    accuracy                           0.64       216\n",
            "   macro avg       0.64      0.64      0.64       216\n",
            "weighted avg       0.64      0.64      0.64       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6082251082251082\n",
            "[[93 17  5  0]\n",
            " [ 8 70 23 14]\n",
            " [ 8 34 51 23]\n",
            " [ 3 13 33 67]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.81      0.82       115\n",
            "           1       0.52      0.61      0.56       115\n",
            "           2       0.46      0.44      0.45       116\n",
            "           3       0.64      0.58      0.61       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.61      0.61      0.61       462\n",
            "weighted avg       0.61      0.61      0.61       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7083333333333334\n",
            "[[48  2  2  2]\n",
            " [ 7 37 10  0]\n",
            " [ 3 13 28 10]\n",
            " [ 1  2 11 40]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85        54\n",
            "           1       0.69      0.69      0.69        54\n",
            "           2       0.55      0.52      0.53        54\n",
            "           3       0.77      0.74      0.75        54\n",
            "\n",
            "    accuracy                           0.71       216\n",
            "   macro avg       0.70      0.71      0.71       216\n",
            "weighted avg       0.70      0.71      0.71       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6406926406926406\n",
            "[[102   9   2   2]\n",
            " [ 14  67  26   8]\n",
            " [ 16  22  50  28]\n",
            " [  6   9  24  77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.89      0.81       115\n",
            "           1       0.63      0.58      0.60       115\n",
            "           2       0.49      0.43      0.46       116\n",
            "           3       0.67      0.66      0.67       116\n",
            "\n",
            "    accuracy                           0.64       462\n",
            "   macro avg       0.63      0.64      0.63       462\n",
            "weighted avg       0.63      0.64      0.63       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 15\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.6435185185185185 , 0.7083333333333334 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5692640692640693 , 0.6082251082251082 , 0.6406926406926406 , 0.6277056277056277 ]\n",
            "Results for Top K= 16\n",
            "[2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[34  8 12  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5692640692640693\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  22  43  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.57       115\n",
            "           2       0.43      0.37      0.40       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.55      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6574074074074074\n",
            "[[46  6  0  2]\n",
            " [ 4 34 12  4]\n",
            " [ 2 11 27 14]\n",
            " [ 0  3 16 35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.87        54\n",
            "           1       0.63      0.63      0.63        54\n",
            "           2       0.49      0.50      0.50        54\n",
            "           3       0.64      0.65      0.64        54\n",
            "\n",
            "    accuracy                           0.66       216\n",
            "   macro avg       0.66      0.66      0.66       216\n",
            "weighted avg       0.66      0.66      0.66       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6168831168831169\n",
            "[[92 16  6  1]\n",
            " [ 8 70 27 10]\n",
            " [ 8 31 56 21]\n",
            " [ 3 15 31 67]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81       115\n",
            "           1       0.53      0.61      0.57       115\n",
            "           2       0.47      0.48      0.47       116\n",
            "           3       0.68      0.58      0.62       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.63      0.62      0.62       462\n",
            "weighted avg       0.63      0.62      0.62       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7083333333333334\n",
            "[[49  2  2  1]\n",
            " [ 8 36 10  0]\n",
            " [ 4 11 28 11]\n",
            " [ 1  2 11 40]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.91      0.84        54\n",
            "           1       0.71      0.67      0.69        54\n",
            "           2       0.55      0.52      0.53        54\n",
            "           3       0.77      0.74      0.75        54\n",
            "\n",
            "    accuracy                           0.71       216\n",
            "   macro avg       0.70      0.71      0.70       216\n",
            "weighted avg       0.70      0.71      0.70       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6385281385281385\n",
            "[[103   9   3   0]\n",
            " [ 13  65  28   9]\n",
            " [ 18  19  51  28]\n",
            " [  6   7  27  76]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.90      0.81       115\n",
            "           1       0.65      0.57      0.60       115\n",
            "           2       0.47      0.44      0.45       116\n",
            "           3       0.67      0.66      0.66       116\n",
            "\n",
            "    accuracy                           0.64       462\n",
            "   macro avg       0.63      0.64      0.63       462\n",
            "weighted avg       0.63      0.64      0.63       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 16\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.6574074074074074 , 0.7083333333333334 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5692640692640693 , 0.6168831168831169 , 0.6385281385281385 , 0.6277056277056277 ]\n",
            "Results for Top K= 17\n",
            "[2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[34  8 12  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  21  44  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.58       115\n",
            "           2       0.44      0.38      0.41       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6620370370370371\n",
            "[[46  6  0  2]\n",
            " [ 5 35 11  3]\n",
            " [ 2 10 27 15]\n",
            " [ 0  4 15 35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86        54\n",
            "           1       0.64      0.65      0.64        54\n",
            "           2       0.51      0.50      0.50        54\n",
            "           3       0.64      0.65      0.64        54\n",
            "\n",
            "    accuracy                           0.66       216\n",
            "   macro avg       0.66      0.66      0.66       216\n",
            "weighted avg       0.66      0.66      0.66       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6125541125541125\n",
            "[[95 14  5  1]\n",
            " [ 9 68 28 10]\n",
            " [ 8 31 53 24]\n",
            " [ 2 17 30 67]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       115\n",
            "           1       0.52      0.59      0.56       115\n",
            "           2       0.46      0.46      0.46       116\n",
            "           3       0.66      0.58      0.61       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.62      0.61      0.61       462\n",
            "weighted avg       0.62      0.61      0.61       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7361111111111112\n",
            "[[51  0  1  2]\n",
            " [ 8 36 10  0]\n",
            " [ 3 10 30 11]\n",
            " [ 1  2  9 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87        54\n",
            "           1       0.75      0.67      0.71        54\n",
            "           2       0.60      0.56      0.58        54\n",
            "           3       0.76      0.78      0.77        54\n",
            "\n",
            "    accuracy                           0.74       216\n",
            "   macro avg       0.73      0.74      0.73       216\n",
            "weighted avg       0.73      0.74      0.73       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6298701298701299\n",
            "[[102   8   3   2]\n",
            " [ 15  65  25  10]\n",
            " [ 15  26  48  27]\n",
            " [  7   7  26  76]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.89      0.80       115\n",
            "           1       0.61      0.57      0.59       115\n",
            "           2       0.47      0.41      0.44       116\n",
            "           3       0.66      0.66      0.66       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.62       462\n",
            "weighted avg       0.62      0.63      0.62       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 17\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.6620370370370371 , 0.7361111111111112 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.6125541125541125 , 0.6298701298701299 , 0.6277056277056277 ]\n",
            "Results for Top K= 18\n",
            "[2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[34  8 12  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  21  44  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.58       115\n",
            "           2       0.44      0.38      0.41       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6342592592592593\n",
            "[[46  5  1  2]\n",
            " [ 2 38 11  3]\n",
            " [ 3 12 20 19]\n",
            " [ 0  4 17 33]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.85      0.88        54\n",
            "           1       0.64      0.70      0.67        54\n",
            "           2       0.41      0.37      0.39        54\n",
            "           3       0.58      0.61      0.59        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.63      0.63      0.63       216\n",
            "weighted avg       0.63      0.63      0.63       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6017316017316018\n",
            "[[95 13  7  0]\n",
            " [13 66 25 11]\n",
            " [11 30 51 24]\n",
            " [ 3 16 31 66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.83      0.80       115\n",
            "           1       0.53      0.57      0.55       115\n",
            "           2       0.45      0.44      0.44       116\n",
            "           3       0.65      0.57      0.61       116\n",
            "\n",
            "    accuracy                           0.60       462\n",
            "   macro avg       0.60      0.60      0.60       462\n",
            "weighted avg       0.60      0.60      0.60       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7268518518518519\n",
            "[[48  2  2  2]\n",
            " [ 8 36  9  1]\n",
            " [ 3 11 29 11]\n",
            " [ 1  2  7 44]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.84        54\n",
            "           1       0.71      0.67      0.69        54\n",
            "           2       0.62      0.54      0.57        54\n",
            "           3       0.76      0.81      0.79        54\n",
            "\n",
            "    accuracy                           0.73       216\n",
            "   macro avg       0.72      0.73      0.72       216\n",
            "weighted avg       0.72      0.73      0.72       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6255411255411255\n",
            "[[101  12   2   0]\n",
            " [ 15  62  31   7]\n",
            " [ 17  22  51  26]\n",
            " [  7   7  27  75]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.88      0.79       115\n",
            "           1       0.60      0.54      0.57       115\n",
            "           2       0.46      0.44      0.45       116\n",
            "           3       0.69      0.65      0.67       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.62       462\n",
            "weighted avg       0.62      0.63      0.62       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 18\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.6342592592592593 , 0.7268518518518519 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.6017316017316018 , 0.6255411255411255 , 0.6277056277056277 ]\n",
            "Results for Top K= 19\n",
            "[2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[34  8 12  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  21  44  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.58       115\n",
            "           2       0.44      0.38      0.41       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.625\n",
            "[[46  6  0  2]\n",
            " [ 6 33 11  4]\n",
            " [ 2 12 21 19]\n",
            " [ 0  4 15 35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85        54\n",
            "           1       0.60      0.61      0.61        54\n",
            "           2       0.45      0.39      0.42        54\n",
            "           3       0.58      0.65      0.61        54\n",
            "\n",
            "    accuracy                           0.62       216\n",
            "   macro avg       0.62      0.62      0.62       216\n",
            "weighted avg       0.62      0.62      0.62       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6082251082251082\n",
            "[[94 11  6  4]\n",
            " [ 9 69 23 14]\n",
            " [ 7 33 49 27]\n",
            " [ 2 18 27 69]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       115\n",
            "           1       0.53      0.60      0.56       115\n",
            "           2       0.47      0.42      0.44       116\n",
            "           3       0.61      0.59      0.60       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.61      0.61      0.61       462\n",
            "weighted avg       0.61      0.61      0.61       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7129629629629629\n",
            "[[49  1  2  2]\n",
            " [ 8 35 10  1]\n",
            " [ 3 11 28 12]\n",
            " [ 1  2  9 42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85        54\n",
            "           1       0.71      0.65      0.68        54\n",
            "           2       0.57      0.52      0.54        54\n",
            "           3       0.74      0.78      0.76        54\n",
            "\n",
            "    accuracy                           0.71       216\n",
            "   macro avg       0.71      0.71      0.71       216\n",
            "weighted avg       0.71      0.71      0.71       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6125541125541125\n",
            "[[101  11   2   1]\n",
            " [ 13  62  31   9]\n",
            " [ 16  21  49  30]\n",
            " [  7   7  31  71]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.88      0.80       115\n",
            "           1       0.61      0.54      0.57       115\n",
            "           2       0.43      0.42      0.43       116\n",
            "           3       0.64      0.61      0.63       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.61      0.61      0.61       462\n",
            "weighted avg       0.61      0.61      0.61       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 19\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.625 , 0.7129629629629629 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.6082251082251082 , 0.6125541125541125 , 0.6277056277056277 ]\n",
            "Results for Top K= 20\n",
            "[2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.49074074074074076\n",
            "[[35  8 11  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.65      0.65        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.53        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  21  44  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.58       115\n",
            "           2       0.44      0.38      0.41       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6342592592592593\n",
            "[[46  6  0  2]\n",
            " [ 5 34 12  3]\n",
            " [ 2 12 24 16]\n",
            " [ 0  4 17 33]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86        54\n",
            "           1       0.61      0.63      0.62        54\n",
            "           2       0.45      0.44      0.45        54\n",
            "           3       0.61      0.61      0.61        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.63      0.63      0.63       216\n",
            "weighted avg       0.63      0.63      0.63       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6125541125541125\n",
            "[[93 17  5  0]\n",
            " [ 9 70 27  9]\n",
            " [ 8 34 52 22]\n",
            " [ 2 15 31 68]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.81      0.82       115\n",
            "           1       0.51      0.61      0.56       115\n",
            "           2       0.45      0.45      0.45       116\n",
            "           3       0.69      0.59      0.63       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.62      0.61      0.61       462\n",
            "weighted avg       0.62      0.61      0.61       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6851851851851852\n",
            "[[46  3  3  2]\n",
            " [ 7 36 10  1]\n",
            " [ 4 12 27 11]\n",
            " [ 1  1 13 39]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82        54\n",
            "           1       0.69      0.67      0.68        54\n",
            "           2       0.51      0.50      0.50        54\n",
            "           3       0.74      0.72      0.73        54\n",
            "\n",
            "    accuracy                           0.69       216\n",
            "   macro avg       0.68      0.69      0.68       216\n",
            "weighted avg       0.68      0.69      0.68       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6168831168831169\n",
            "[[100   9   4   2]\n",
            " [ 15  62  30   8]\n",
            " [ 16  21  53  26]\n",
            " [  6   8  32  70]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.87      0.79       115\n",
            "           1       0.62      0.54      0.58       115\n",
            "           2       0.45      0.46      0.45       116\n",
            "           3       0.66      0.60      0.63       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.62      0.61       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 20\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.49074074074074076 , 0.5972222222222222 , 0.6342592592592593 , 0.6851851851851852 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.6125541125541125 , 0.6168831168831169 , 0.6277056277056277 ]\n",
            "Results for Top K= 21\n",
            "[2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.4861111111111111\n",
            "[[34  8 12  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.52        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  21  44  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.58       115\n",
            "           2       0.44      0.38      0.41       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6296296296296297\n",
            "[[47  5  0  2]\n",
            " [ 5 35 11  3]\n",
            " [ 3 11 22 18]\n",
            " [ 0  4 18 32]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86        54\n",
            "           1       0.64      0.65      0.64        54\n",
            "           2       0.43      0.41      0.42        54\n",
            "           3       0.58      0.59      0.59        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.63      0.63      0.63       216\n",
            "weighted avg       0.63      0.63      0.63       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6038961038961039\n",
            "[[95 15  5  0]\n",
            " [10 69 23 13]\n",
            " [ 8 32 52 24]\n",
            " [ 3 18 32 63]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82       115\n",
            "           1       0.51      0.60      0.55       115\n",
            "           2       0.46      0.45      0.46       116\n",
            "           3       0.63      0.54      0.58       116\n",
            "\n",
            "    accuracy                           0.60       462\n",
            "   macro avg       0.61      0.60      0.60       462\n",
            "weighted avg       0.61      0.60      0.60       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7083333333333334\n",
            "[[48  1  3  2]\n",
            " [ 8 35  9  2]\n",
            " [ 3 12 29 10]\n",
            " [ 2  2  9 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.83        54\n",
            "           1       0.70      0.65      0.67        54\n",
            "           2       0.58      0.54      0.56        54\n",
            "           3       0.75      0.76      0.75        54\n",
            "\n",
            "    accuracy                           0.71       216\n",
            "   macro avg       0.70      0.71      0.70       216\n",
            "weighted avg       0.70      0.71      0.70       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5995670995670995\n",
            "[[98 10  4  3]\n",
            " [12 63 30 10]\n",
            " [18 20 47 31]\n",
            " [ 7  8 32 69]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.85      0.78       115\n",
            "           1       0.62      0.55      0.58       115\n",
            "           2       0.42      0.41      0.41       116\n",
            "           3       0.61      0.59      0.60       116\n",
            "\n",
            "    accuracy                           0.60       462\n",
            "   macro avg       0.59      0.60      0.60       462\n",
            "weighted avg       0.59      0.60      0.59       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 21\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.4861111111111111 , 0.5972222222222222 , 0.6296296296296297 , 0.7083333333333334 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.6038961038961039 , 0.5995670995670995 , 0.6277056277056277 ]\n",
            "Results for Top K= 22\n",
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.49074074074074076\n",
            "[[35  8 11  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.65      0.65        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.53        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  21  44  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.58       115\n",
            "           2       0.44      0.38      0.41       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.625\n",
            "[[47  5  0  2]\n",
            " [ 5 33 13  3]\n",
            " [ 3 12 21 18]\n",
            " [ 0  5 15 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86        54\n",
            "           1       0.60      0.61      0.61        54\n",
            "           2       0.43      0.39      0.41        54\n",
            "           3       0.60      0.63      0.61        54\n",
            "\n",
            "    accuracy                           0.62       216\n",
            "   macro avg       0.62      0.62      0.62       216\n",
            "weighted avg       0.62      0.62      0.62       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5995670995670995\n",
            "[[95 15  5  0]\n",
            " [ 8 70 24 13]\n",
            " [ 8 36 46 26]\n",
            " [ 2 17 31 66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.83       115\n",
            "           1       0.51      0.61      0.55       115\n",
            "           2       0.43      0.40      0.41       116\n",
            "           3       0.63      0.57      0.60       116\n",
            "\n",
            "    accuracy                           0.60       462\n",
            "   macro avg       0.60      0.60      0.60       462\n",
            "weighted avg       0.60      0.60      0.60       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[49  2  2  1]\n",
            " [10 34  9  1]\n",
            " [ 4 12 26 12]\n",
            " [ 2  2 13 37]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.91      0.82        54\n",
            "           1       0.68      0.63      0.65        54\n",
            "           2       0.52      0.48      0.50        54\n",
            "           3       0.73      0.69      0.70        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6125541125541125\n",
            "[[99 11  3  2]\n",
            " [15 61 28 11]\n",
            " [19 20 48 29]\n",
            " [ 7  7 27 75]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.86      0.78       115\n",
            "           1       0.62      0.53      0.57       115\n",
            "           2       0.45      0.41      0.43       116\n",
            "           3       0.64      0.65      0.64       116\n",
            "\n",
            "    accuracy                           0.61       462\n",
            "   macro avg       0.60      0.61      0.61       462\n",
            "weighted avg       0.60      0.61      0.61       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6759259259259259\n",
            "[[50  3  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7  9 23 15]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83        54\n",
            "           1       0.71      0.59      0.65        54\n",
            "           2       0.50      0.43      0.46        54\n",
            "           3       0.69      0.76      0.73        54\n",
            "\n",
            "    accuracy                           0.68       216\n",
            "   macro avg       0.67      0.68      0.67       216\n",
            "weighted avg       0.67      0.68      0.67       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6277056277056277\n",
            "[[110   5   0   0]\n",
            " [ 23  58  30   4]\n",
            " [ 21  22  44  29]\n",
            " [  8   7  23  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.96      0.79       115\n",
            "           1       0.63      0.50      0.56       115\n",
            "           2       0.45      0.38      0.41       116\n",
            "           3       0.70      0.67      0.69       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.61       462\n",
            "weighted avg       0.62      0.63      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 22\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.49074074074074076 , 0.5972222222222222 , 0.625 , 0.6759259259259259 , 0.6759259259259259 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.5995670995670995 , 0.6125541125541125 , 0.6277056277056277 ]\n",
            "Results for Top K= 23\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.49074074074074076\n",
            "[[35  8 11  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.65      0.65        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.53        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.5972222222222222\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 12 25 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.51      0.52      0.51        54\n",
            "           2       0.48      0.46      0.47        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  21  44  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.58       115\n",
            "           2       0.44      0.38      0.41       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6527777777777778\n",
            "[[47  5  0  2]\n",
            " [ 4 36 10  4]\n",
            " [ 3 11 24 16]\n",
            " [ 0  4 16 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87        54\n",
            "           1       0.64      0.67      0.65        54\n",
            "           2       0.48      0.44      0.46        54\n",
            "           3       0.61      0.63      0.62        54\n",
            "\n",
            "    accuracy                           0.65       216\n",
            "   macro avg       0.65      0.65      0.65       216\n",
            "weighted avg       0.65      0.65      0.65       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6233766233766234\n",
            "[[97 12  6  0]\n",
            " [ 8 72 22 13]\n",
            " [ 7 32 52 25]\n",
            " [ 2 17 30 67]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       115\n",
            "           1       0.54      0.63      0.58       115\n",
            "           2       0.47      0.45      0.46       116\n",
            "           3       0.64      0.58      0.61       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.63      0.62      0.62       462\n",
            "weighted avg       0.63      0.62      0.62       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7037037037037037\n",
            "[[47  2  3  2]\n",
            " [ 9 34 10  1]\n",
            " [ 4 10 30 10]\n",
            " [ 2  2  9 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.87      0.81        54\n",
            "           1       0.71      0.63      0.67        54\n",
            "           2       0.58      0.56      0.57        54\n",
            "           3       0.76      0.76      0.76        54\n",
            "\n",
            "    accuracy                           0.70       216\n",
            "   macro avg       0.70      0.70      0.70       216\n",
            "weighted avg       0.70      0.70      0.70       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5952380952380952\n",
            "[[98 11  4  2]\n",
            " [16 58 30 11]\n",
            " [16 23 44 33]\n",
            " [ 6  8 27 75]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.85      0.78       115\n",
            "           1       0.58      0.50      0.54       115\n",
            "           2       0.42      0.38      0.40       116\n",
            "           3       0.62      0.65      0.63       116\n",
            "\n",
            "    accuracy                           0.60       462\n",
            "   macro avg       0.58      0.60      0.59       462\n",
            "weighted avg       0.58      0.60      0.59       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6620370370370371\n",
            "[[49  4  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7 10 21 16]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.91      0.82        54\n",
            "           1       0.68      0.59      0.63        54\n",
            "           2       0.48      0.39      0.43        54\n",
            "           3       0.68      0.76      0.72        54\n",
            "\n",
            "    accuracy                           0.66       216\n",
            "   macro avg       0.65      0.66      0.65       216\n",
            "weighted avg       0.65      0.66      0.65       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6212121212121212\n",
            "[[108   6   1   0]\n",
            " [ 23  59  28   5]\n",
            " [ 20  23  43  30]\n",
            " [ 10   7  22  77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.94      0.78       115\n",
            "           1       0.62      0.51      0.56       115\n",
            "           2       0.46      0.37      0.41       116\n",
            "           3       0.69      0.66      0.68       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.62      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 23\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.49074074074074076 , 0.5972222222222222 , 0.6527777777777778 , 0.7037037037037037 , 0.6620370370370371 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.6233766233766234 , 0.5952380952380952 , 0.6212121212121212 ]\n",
            "Results for Top K= 24\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "trainDatatmean shape:  (862, 25)\n",
            "trainDatatmean shape:  (462, 25)\n",
            "trainDatatmean shape:  (216, 25)\n",
            "SVM classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.49074074074074076\n",
            "[[35  8 11  0]\n",
            " [14 11 27  2]\n",
            " [ 4  1 46  3]\n",
            " [ 0  3 37 14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.65      0.65        54\n",
            "           1       0.48      0.20      0.29        54\n",
            "           2       0.38      0.85      0.53        54\n",
            "           3       0.74      0.26      0.38        54\n",
            "\n",
            "    accuracy                           0.49       216\n",
            "   macro avg       0.56      0.49      0.46       216\n",
            "weighted avg       0.56      0.49      0.46       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.44805194805194803\n",
            "[[72 20 23  0]\n",
            " [25 23 59  8]\n",
            " [15  6 81 14]\n",
            " [ 6  7 72 31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.63      0.62       115\n",
            "           1       0.41      0.20      0.27       115\n",
            "           2       0.34      0.70      0.46       116\n",
            "           3       0.58      0.27      0.37       116\n",
            "\n",
            "    accuracy                           0.45       462\n",
            "   macro avg       0.49      0.45      0.43       462\n",
            "weighted avg       0.49      0.45      0.43       462\n",
            "\n",
            "KNN classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6018518518518519\n",
            "[[51  1  2  0]\n",
            " [10 28 10  6]\n",
            " [ 5 11 26 12]\n",
            " [ 0 14 15 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        54\n",
            "           1       0.52      0.52      0.52        54\n",
            "           2       0.49      0.48      0.49        54\n",
            "           3       0.58      0.46      0.52        54\n",
            "\n",
            "    accuracy                           0.60       216\n",
            "   macro avg       0.59      0.60      0.59       216\n",
            "weighted avg       0.59      0.60      0.59       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.5714285714285714\n",
            "[[110   4   1   0]\n",
            " [ 20  68  19   8]\n",
            " [ 25  21  44  26]\n",
            " [  9  28  37  42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79       115\n",
            "           1       0.56      0.59      0.58       115\n",
            "           2       0.44      0.38      0.41       116\n",
            "           3       0.55      0.36      0.44       116\n",
            "\n",
            "    accuracy                           0.57       462\n",
            "   macro avg       0.56      0.57      0.55       462\n",
            "weighted avg       0.55      0.57      0.55       462\n",
            "\n",
            "Desition Tree classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6342592592592593\n",
            "[[46  6  0  2]\n",
            " [ 5 35 11  3]\n",
            " [ 2 13 22 17]\n",
            " [ 0  5 15 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86        54\n",
            "           1       0.59      0.65      0.62        54\n",
            "           2       0.46      0.41      0.43        54\n",
            "           3       0.61      0.63      0.62        54\n",
            "\n",
            "    accuracy                           0.63       216\n",
            "   macro avg       0.63      0.63      0.63       216\n",
            "weighted avg       0.63      0.63      0.63       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6190476190476191\n",
            "[[94 14  7  0]\n",
            " [ 9 72 25  9]\n",
            " [ 8 34 51 23]\n",
            " [ 2 17 28 69]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.82       115\n",
            "           1       0.53      0.63      0.57       115\n",
            "           2       0.46      0.44      0.45       116\n",
            "           3       0.68      0.59      0.64       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.63      0.62      0.62       462\n",
            "weighted avg       0.62      0.62      0.62       462\n",
            "\n",
            "Random Forest classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.7314814814814815\n",
            "[[50  1  2  1]\n",
            " [ 7 36 10  1]\n",
            " [ 3 11 29 11]\n",
            " [ 0  2  9 43]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88        54\n",
            "           1       0.72      0.67      0.69        54\n",
            "           2       0.58      0.54      0.56        54\n",
            "           3       0.77      0.80      0.78        54\n",
            "\n",
            "    accuracy                           0.73       216\n",
            "   macro avg       0.73      0.73      0.73       216\n",
            "weighted avg       0.73      0.73      0.73       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6298701298701299\n",
            "[[101  10   2   2]\n",
            " [ 13  65  27  10]\n",
            " [ 17  22  50  27]\n",
            " [  7   8  26  75]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.88      0.80       115\n",
            "           1       0.62      0.57      0.59       115\n",
            "           2       0.48      0.43      0.45       116\n",
            "           3       0.66      0.65      0.65       116\n",
            "\n",
            "    accuracy                           0.63       462\n",
            "   macro avg       0.62      0.63      0.62       462\n",
            "weighted avg       0.62      0.63      0.62       462\n",
            "\n",
            "XGBoost classifier\n",
            "validation data results:\n",
            "Validation Accuracey 0.6620370370370371\n",
            "[[49  4  1  0]\n",
            " [ 9 32 10  3]\n",
            " [ 7 10 21 16]\n",
            " [ 0  1 12 41]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.91      0.82        54\n",
            "           1       0.68      0.59      0.63        54\n",
            "           2       0.48      0.39      0.43        54\n",
            "           3       0.68      0.76      0.72        54\n",
            "\n",
            "    accuracy                           0.66       216\n",
            "   macro avg       0.65      0.66      0.65       216\n",
            "weighted avg       0.65      0.66      0.65       216\n",
            "\n",
            "test data results:\n",
            "Test Accuracey 0.6212121212121212\n",
            "[[108   6   1   0]\n",
            " [ 23  59  28   5]\n",
            " [ 20  23  43  30]\n",
            " [ 10   7  22  77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.94      0.78       115\n",
            "           1       0.62      0.51      0.56       115\n",
            "           2       0.46      0.37      0.41       116\n",
            "           3       0.69      0.66      0.68       116\n",
            "\n",
            "    accuracy                           0.62       462\n",
            "   macro avg       0.61      0.62      0.61       462\n",
            "weighted avg       0.61      0.62      0.61       462\n",
            "\n",
            "Validation and Test Accuracey for Top K= 24\n",
            " Validation Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.49074074074074076 , 0.6018518518518519 , 0.6342592592592593 , 0.7314814814814815 , 0.6620370370370371 ]\n",
            " Test Accuracey for SVM, KNN,Desition Tree, Random Forest, and XGBoost classifiers are: [ 0.44805194805194803 , 0.5714285714285714 , 0.6190476190476191 , 0.6298701298701299 , 0.6212121212121212 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"......SVM classifier......\")\n",
        "print(\"SVM Validation Accuracy\")\n",
        "print(ValidationSVMAcc)\n",
        "print(\"SVM Test Accuracy\")\n",
        "print(TestSVMAcc)\n",
        "print(\"......KNN classifier......\")\n",
        "print(\"KNN Validation Accuracy\")\n",
        "print(ValidationKNNAcc)\n",
        "print(\"KNN Validation Accuracy\")\n",
        "print(TestKNNAcc)\n",
        "print(\"......Decision Tree classifier......\")\n",
        "print(\"Decision Tree Validation Accuracy\")\n",
        "print(ValidationDTAcc)\n",
        "print(\"Decision Tree Validation Accuracy\")\n",
        "print(TestDTAcc)\n",
        "print(\"......Random Forest classifier......\")\n",
        "print(\"Random Forest Validation Accuracy\")\n",
        "print(ValidationRFAcc)\n",
        "print(\"Random Forest Test Accuracy\")\n",
        "print(TestRFAcc)\n",
        "print(\"......XGBoost classifier......\")\n",
        "print(\"XGBoost Validation Accuracy\")\n",
        "print(ValidationXGBoostAcc)\n",
        "print(\"XGBoost Test Accuracy\")\n",
        "print(TestXGBoostAcc)"
      ],
      "metadata": {
        "id": "D36izq15D_9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78352591-ed04-477a-9cbb-1d5d5e31b587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......SVM classifier......\n",
            "SVM Validation Accuracy\n",
            "[0.4537037037037037, 0.4212962962962963, 0.4351851851851852, 0.4212962962962963, 0.3888888888888889, 0.3888888888888889, 0.38425925925925924, 0.38425925925925924, 0.38425925925925924, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.49074074074074076, 0.49074074074074076, 0.4861111111111111, 0.5277777777777778, 0.4537037037037037, 0.4444444444444444, 0.4444444444444444, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.4351851851851852, 0.5, 0.44907407407407407, 0.4398148148148148, 0.49537037037037035, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4027777777777778, 0.4675925925925926, 0.4027777777777778, 0.4027777777777778, 0.49537037037037035, 0.38425925925925924, 0.375, 0.375, 0.37962962962962965, 0.3888888888888889, 0.38425925925925924, 0.38425925925925924, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.37962962962962965, 0.49074074074074076, 0.4444444444444444, 0.4861111111111111, 0.5370370370370371, 0.4861111111111111, 0.4861111111111111, 0.47685185185185186, 0.47685185185185186, 0.47685185185185186, 0.47685185185185186, 0.47685185185185186, 0.47685185185185186, 0.48148148148148145, 0.48148148148148145, 0.4861111111111111, 0.4861111111111111, 0.4861111111111111, 0.4861111111111111, 0.4861111111111111, 0.4861111111111111, 0.49074074074074076, 0.4861111111111111, 0.49074074074074076, 0.49074074074074076, 0.49074074074074076]\n",
            "SVM Test Accuracy\n",
            "[0.5064935064935064, 0.4523809523809524, 0.45454545454545453, 0.5, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.44372294372294374, 0.45454545454545453, 0.4588744588744589, 0.487012987012987, 0.4155844155844156, 0.41125541125541126, 0.4134199134199134, 0.4090909090909091, 0.4069264069264069, 0.4069264069264069, 0.4069264069264069, 0.4069264069264069, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4090909090909091, 0.4653679653679654, 0.4653679653679654, 0.45670995670995673, 0.5151515151515151, 0.44805194805194803, 0.44805194805194803, 0.44372294372294374, 0.44155844155844154, 0.44155844155844154, 0.44372294372294374, 0.44372294372294374, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.4393939393939394, 0.4393939393939394, 0.4393939393939394, 0.4393939393939394, 0.4393939393939394, 0.43722943722943725, 0.4696969696969697, 0.4134199134199134, 0.4134199134199134, 0.45021645021645024, 0.43073593073593075, 0.4264069264069264, 0.43722943722943725, 0.43722943722943725, 0.4393939393939394, 0.4393939393939394, 0.43722943722943725, 0.43722943722943725, 0.43506493506493504, 0.4329004329004329, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.43073593073593075, 0.5021645021645021, 0.4393939393939394, 0.4588744588744589, 0.5194805194805194, 0.44372294372294374, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44155844155844154, 0.44372294372294374, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803, 0.44805194805194803]\n",
            "......KNN classifier......\n",
            "KNN Validation Accuracy\n",
            "[0.375, 0.5324074074074074, 0.5740740740740741, 0.5833333333333334, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5879629629629629, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.42592592592592593, 0.5509259259259259, 0.5509259259259259, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6157407407407407, 0.6157407407407407, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6157407407407407, 0.6157407407407407, 0.6157407407407407, 0.6157407407407407, 0.6157407407407407, 0.6157407407407407, 0.44907407407407407, 0.5231481481481481, 0.5416666666666666, 0.5694444444444444, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5601851851851852, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4583333333333333, 0.46296296296296297, 0.5324074074074074, 0.5787037037037037, 0.5787037037037037, 0.5787037037037037, 0.5787037037037037, 0.5740740740740741, 0.5740740740740741, 0.5740740740740741, 0.5740740740740741, 0.5740740740740741, 0.5787037037037037, 0.5787037037037037, 0.5787037037037037, 0.5787037037037037, 0.5787037037037037, 0.5787037037037037, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.49537037037037035, 0.5787037037037037, 0.6111111111111112, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.5972222222222222, 0.6018518518518519]\n",
            "KNN Validation Accuracy\n",
            "[0.487012987012987, 0.5346320346320347, 0.551948051948052, 0.5844155844155844, 0.5844155844155844, 0.5844155844155844, 0.5844155844155844, 0.5822510822510822, 0.5822510822510822, 0.5822510822510822, 0.5822510822510822, 0.5822510822510822, 0.5822510822510822, 0.5822510822510822, 0.5822510822510822, 0.5822510822510822, 0.5844155844155844, 0.5865800865800865, 0.5865800865800865, 0.5865800865800865, 0.5865800865800865, 0.5887445887445888, 0.5887445887445888, 0.5887445887445888, 0.5887445887445888, 0.4329004329004329, 0.551948051948052, 0.5670995670995671, 0.6060606060606061, 0.6082251082251082, 0.6082251082251082, 0.6082251082251082, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6060606060606061, 0.6038961038961039, 0.6038961038961039, 0.6017316017316018, 0.6017316017316018, 0.42207792207792205, 0.577922077922078, 0.5995670995670995, 0.6190476190476191, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6168831168831169, 0.6168831168831169, 0.6147186147186147, 0.6168831168831169, 0.6168831168831169, 0.6168831168831169, 0.6168831168831169, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.6147186147186147, 0.4329004329004329, 0.5627705627705628, 0.6038961038961039, 0.5844155844155844, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5714285714285714, 0.5692640692640693, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5735930735930735, 0.44805194805194803, 0.5281385281385281, 0.5541125541125541, 0.5584415584415584, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5735930735930735, 0.5714285714285714, 0.5692640692640693, 0.5692640692640693, 0.5692640692640693, 0.5692640692640693, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714]\n",
            "......Decision Tree classifier......\n",
            "Decision Tree Validation Accuracy\n",
            "[0.37037037037037035, 0.5185185185185185, 0.6157407407407407, 0.5879629629629629, 0.6111111111111112, 0.6064814814814815, 0.6018518518518519, 0.6064814814814815, 0.6157407407407407, 0.6111111111111112, 0.5972222222222222, 0.6296296296296297, 0.6111111111111112, 0.5972222222222222, 0.625, 0.6064814814814815, 0.625, 0.625, 0.6157407407407407, 0.6157407407407407, 0.6157407407407407, 0.6203703703703703, 0.6064814814814815, 0.6064814814814815, 0.6157407407407407, 0.41203703703703703, 0.5601851851851852, 0.625, 0.6203703703703703, 0.6898148148148148, 0.6435185185185185, 0.6388888888888888, 0.6574074074074074, 0.6620370370370371, 0.6759259259259259, 0.6342592592592593, 0.6435185185185185, 0.6527777777777778, 0.6620370370370371, 0.6435185185185185, 0.6574074074074074, 0.6574074074074074, 0.6435185185185185, 0.6620370370370371, 0.6435185185185185, 0.6527777777777778, 0.6527777777777778, 0.6296296296296297, 0.6527777777777778, 0.6388888888888888, 0.44907407407407407, 0.5, 0.5740740740740741, 0.5879629629629629, 0.6111111111111112, 0.625, 0.6111111111111112, 0.6157407407407407, 0.6111111111111112, 0.6157407407407407, 0.6203703703703703, 0.6157407407407407, 0.6157407407407407, 0.6064814814814815, 0.6157407407407407, 0.6064814814814815, 0.6018518518518519, 0.6064814814814815, 0.6296296296296297, 0.6064814814814815, 0.6203703703703703, 0.6157407407407407, 0.6064814814814815, 0.6203703703703703, 0.6111111111111112, 0.4305555555555556, 0.5231481481481481, 0.5740740740740741, 0.6342592592592593, 0.6111111111111112, 0.5925925925925926, 0.6111111111111112, 0.6018518518518519, 0.6064814814814815, 0.5972222222222222, 0.6018518518518519, 0.5833333333333334, 0.5879629629629629, 0.6018518518518519, 0.5879629629629629, 0.5787037037037037, 0.6111111111111112, 0.5879629629629629, 0.5833333333333334, 0.6064814814814815, 0.6111111111111112, 0.6064814814814815, 0.6064814814814815, 0.6064814814814815, 0.5925925925925926, 0.4583333333333333, 0.5324074074074074, 0.6388888888888888, 0.6759259259259259, 0.6527777777777778, 0.6342592592592593, 0.6342592592592593, 0.6342592592592593, 0.6388888888888888, 0.6435185185185185, 0.6342592592592593, 0.6388888888888888, 0.6435185185185185, 0.6342592592592593, 0.6388888888888888, 0.6435185185185185, 0.6574074074074074, 0.6620370370370371, 0.6342592592592593, 0.625, 0.6342592592592593, 0.6296296296296297, 0.625, 0.6527777777777778, 0.6342592592592593]\n",
            "Decision Tree Validation Accuracy\n",
            "[0.4393939393939394, 0.538961038961039, 0.6212121212121212, 0.658008658008658, 0.6558441558441559, 0.6536796536796536, 0.6471861471861472, 0.6385281385281385, 0.6341991341991342, 0.6536796536796536, 0.6471861471861472, 0.6471861471861472, 0.6493506493506493, 0.6536796536796536, 0.6515151515151515, 0.6493506493506493, 0.6601731601731602, 0.6536796536796536, 0.658008658008658, 0.6428571428571429, 0.6536796536796536, 0.6471861471861472, 0.6277056277056277, 0.6363636363636364, 0.6536796536796536, 0.3917748917748918, 0.5367965367965368, 0.5952380952380952, 0.6320346320346321, 0.6255411255411255, 0.6017316017316018, 0.6038961038961039, 0.6190476190476191, 0.5974025974025974, 0.6168831168831169, 0.6147186147186147, 0.6082251082251082, 0.5887445887445888, 0.6038961038961039, 0.6125541125541125, 0.6147186147186147, 0.6168831168831169, 0.6233766233766234, 0.6147186147186147, 0.6103896103896104, 0.6103896103896104, 0.6103896103896104, 0.6017316017316018, 0.6060606060606061, 0.6103896103896104, 0.3874458874458874, 0.551948051948052, 0.6341991341991342, 0.6363636363636364, 0.6796536796536796, 0.6645021645021645, 0.645021645021645, 0.658008658008658, 0.6558441558441559, 0.6645021645021645, 0.6666666666666666, 0.658008658008658, 0.6515151515151515, 0.6428571428571429, 0.6601731601731602, 0.658008658008658, 0.6601731601731602, 0.6536796536796536, 0.6601731601731602, 0.658008658008658, 0.6688311688311688, 0.6471861471861472, 0.6515151515151515, 0.658008658008658, 0.658008658008658, 0.4025974025974026, 0.512987012987013, 0.6212121212121212, 0.6212121212121212, 0.6190476190476191, 0.6406926406926406, 0.6255411255411255, 0.6298701298701299, 0.6471861471861472, 0.6190476190476191, 0.6428571428571429, 0.6298701298701299, 0.6255411255411255, 0.6428571428571429, 0.6298701298701299, 0.6341991341991342, 0.645021645021645, 0.6298701298701299, 0.6341991341991342, 0.6363636363636364, 0.6385281385281385, 0.6233766233766234, 0.6190476190476191, 0.6341991341991342, 0.6471861471861472, 0.3939393939393939, 0.5194805194805194, 0.6060606060606061, 0.6168831168831169, 0.6168831168831169, 0.6017316017316018, 0.6147186147186147, 0.5930735930735931, 0.5930735930735931, 0.6147186147186147, 0.6082251082251082, 0.6060606060606061, 0.6125541125541125, 0.6298701298701299, 0.6233766233766234, 0.6082251082251082, 0.6168831168831169, 0.6125541125541125, 0.6017316017316018, 0.6082251082251082, 0.6125541125541125, 0.6038961038961039, 0.5995670995670995, 0.6233766233766234, 0.6190476190476191]\n",
            "......Random Forest classifier......\n",
            "Random Forest Validation Accuracy\n",
            "[0.37037037037037035, 0.5462962962962963, 0.6574074074074074, 0.6620370370370371, 0.6666666666666666, 0.6666666666666666, 0.6342592592592593, 0.6388888888888888, 0.6296296296296297, 0.6527777777777778, 0.625, 0.6296296296296297, 0.6111111111111112, 0.6527777777777778, 0.6111111111111112, 0.6296296296296297, 0.625, 0.6203703703703703, 0.6064814814814815, 0.6064814814814815, 0.6064814814814815, 0.5833333333333334, 0.5972222222222222, 0.5925925925925926, 0.5833333333333334, 0.41203703703703703, 0.625, 0.7129629629629629, 0.7407407407407407, 0.7407407407407407, 0.7314814814814815, 0.7222222222222222, 0.7083333333333334, 0.7175925925925926, 0.6944444444444444, 0.6851851851851852, 0.6944444444444444, 0.6666666666666666, 0.6712962962962963, 0.6666666666666666, 0.6759259259259259, 0.6851851851851852, 0.6574074074074074, 0.6851851851851852, 0.6759259259259259, 0.6990740740740741, 0.6481481481481481, 0.6527777777777778, 0.6388888888888888, 0.6620370370370371, 0.44907407407407407, 0.5787037037037037, 0.6296296296296297, 0.6620370370370371, 0.6435185185185185, 0.6481481481481481, 0.6851851851851852, 0.6944444444444444, 0.6712962962962963, 0.6620370370370371, 0.6574074074074074, 0.6620370370370371, 0.6620370370370371, 0.6481481481481481, 0.6481481481481481, 0.6435185185185185, 0.6481481481481481, 0.6759259259259259, 0.6527777777777778, 0.6527777777777778, 0.6527777777777778, 0.6620370370370371, 0.6527777777777778, 0.6388888888888888, 0.6481481481481481, 0.4305555555555556, 0.5462962962962963, 0.6712962962962963, 0.7037037037037037, 0.6805555555555556, 0.6712962962962963, 0.6898148148148148, 0.6851851851851852, 0.6851851851851852, 0.6666666666666666, 0.6712962962962963, 0.6481481481481481, 0.6620370370370371, 0.6203703703703703, 0.6435185185185185, 0.6388888888888888, 0.6388888888888888, 0.6435185185185185, 0.6296296296296297, 0.6481481481481481, 0.6342592592592593, 0.6435185185185185, 0.625, 0.6064814814814815, 0.6203703703703703, 0.4583333333333333, 0.5879629629629629, 0.7453703703703703, 0.7314814814814815, 0.7314814814814815, 0.7453703703703703, 0.75, 0.7175925925925926, 0.7407407407407407, 0.7361111111111112, 0.7268518518518519, 0.7453703703703703, 0.7407407407407407, 0.7175925925925926, 0.7222222222222222, 0.7083333333333334, 0.7083333333333334, 0.7361111111111112, 0.7268518518518519, 0.7129629629629629, 0.6851851851851852, 0.7083333333333334, 0.6759259259259259, 0.7037037037037037, 0.7314814814814815]\n",
            "Random Forest Test Accuracy\n",
            "[0.4393939393939394, 0.6060606060606061, 0.6818181818181818, 0.7056277056277056, 0.6904761904761905, 0.6948051948051948, 0.7034632034632035, 0.6904761904761905, 0.696969696969697, 0.696969696969697, 0.6883116883116883, 0.6991341991341992, 0.6861471861471862, 0.6666666666666666, 0.6774891774891775, 0.6818181818181818, 0.6818181818181818, 0.6948051948051948, 0.6818181818181818, 0.6753246753246753, 0.6623376623376623, 0.6623376623376623, 0.6493506493506493, 0.670995670995671, 0.6883116883116883, 0.3917748917748918, 0.5670995670995671, 0.6645021645021645, 0.6883116883116883, 0.683982683982684, 0.6753246753246753, 0.6774891774891775, 0.6796536796536796, 0.670995670995671, 0.6666666666666666, 0.658008658008658, 0.670995670995671, 0.6601731601731602, 0.6515151515151515, 0.6623376623376623, 0.6558441558441559, 0.658008658008658, 0.658008658008658, 0.6623376623376623, 0.6623376623376623, 0.6406926406926406, 0.6471861471861472, 0.6406926406926406, 0.6385281385281385, 0.6558441558441559, 0.3874458874458874, 0.5930735930735931, 0.7056277056277056, 0.7424242424242424, 0.7142857142857143, 0.7186147186147186, 0.7121212121212122, 0.7056277056277056, 0.7229437229437229, 0.70995670995671, 0.7077922077922078, 0.6926406926406926, 0.6904761904761905, 0.696969696969697, 0.696969696969697, 0.696969696969697, 0.683982683982684, 0.6818181818181818, 0.6904761904761905, 0.6796536796536796, 0.6883116883116883, 0.6688311688311688, 0.683982683982684, 0.6645021645021645, 0.683982683982684, 0.4025974025974026, 0.5757575757575758, 0.6666666666666666, 0.6991341991341992, 0.6753246753246753, 0.670995670995671, 0.6731601731601732, 0.670995670995671, 0.6926406926406926, 0.6774891774891775, 0.6861471861471862, 0.6558441558441559, 0.683982683982684, 0.6536796536796536, 0.6774891774891775, 0.6904761904761905, 0.6645021645021645, 0.6883116883116883, 0.6666666666666666, 0.6471861471861472, 0.6731601731601732, 0.670995670995671, 0.6515151515151515, 0.645021645021645, 0.6623376623376623, 0.3939393939393939, 0.5606060606060606, 0.6493506493506493, 0.658008658008658, 0.6818181818181818, 0.670995670995671, 0.645021645021645, 0.6493506493506493, 0.6601731601731602, 0.6471861471861472, 0.645021645021645, 0.6601731601731602, 0.6428571428571429, 0.6168831168831169, 0.6147186147186147, 0.6406926406926406, 0.6385281385281385, 0.6298701298701299, 0.6255411255411255, 0.6125541125541125, 0.6168831168831169, 0.5995670995670995, 0.6125541125541125, 0.5952380952380952, 0.6298701298701299]\n",
            "......XGBoost classifier......\n",
            "XGBoost Validation Accuracy\n",
            "[0.4444444444444444, 0.5555555555555556, 0.5879629629629629, 0.6111111111111112, 0.625, 0.6388888888888888, 0.625, 0.625, 0.6342592592592593, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.5, 0.5555555555555556, 0.6574074074074074, 0.6712962962962963, 0.6759259259259259, 0.6990740740740741, 0.7083333333333334, 0.7083333333333334, 0.6851851851851852, 0.6898148148148148, 0.6898148148148148, 0.6898148148148148, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.6944444444444444, 0.48148148148148145, 0.5370370370370371, 0.6296296296296297, 0.6620370370370371, 0.6574074074074074, 0.6620370370370371, 0.6574074074074074, 0.6574074074074074, 0.6481481481481481, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6620370370370371, 0.6620370370370371, 0.6620370370370371, 0.6620370370370371, 0.6574074074074074, 0.6574074074074074, 0.6574074074074074, 0.6574074074074074, 0.6574074074074074, 0.6574074074074074, 0.6574074074074074, 0.6574074074074074, 0.6574074074074074, 0.49074074074074076, 0.5231481481481481, 0.6064814814814815, 0.6527777777777778, 0.6759259259259259, 0.6620370370370371, 0.6620370370370371, 0.6620370370370371, 0.6620370370370371, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.6805555555555556, 0.48148148148148145, 0.5509259259259259, 0.6435185185185185, 0.6527777777777778, 0.6666666666666666, 0.6666666666666666, 0.6759259259259259, 0.6759259259259259, 0.6666666666666666, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6759259259259259, 0.6620370370370371, 0.6620370370370371]\n",
            "XGBoost Test Accuracy\n",
            "[0.4935064935064935, 0.5562770562770563, 0.6363636363636364, 0.6558441558441559, 0.6558441558441559, 0.670995670995671, 0.6666666666666666, 0.6666666666666666, 0.6645021645021645, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.670995670995671, 0.45670995670995673, 0.5735930735930735, 0.6493506493506493, 0.6277056277056277, 0.6428571428571429, 0.6493506493506493, 0.645021645021645, 0.645021645021645, 0.6363636363636364, 0.645021645021645, 0.645021645021645, 0.645021645021645, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.6341991341991342, 0.4935064935064935, 0.5670995670995671, 0.6493506493506493, 0.6623376623376623, 0.670995670995671, 0.6731601731601732, 0.6774891774891775, 0.6774891774891775, 0.6731601731601732, 0.6818181818181818, 0.6818181818181818, 0.6818181818181818, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.6601731601731602, 0.4805194805194805, 0.5541125541125541, 0.6190476190476191, 0.658008658008658, 0.6103896103896104, 0.6385281385281385, 0.6385281385281385, 0.6385281385281385, 0.6385281385281385, 0.6406926406926406, 0.6406926406926406, 0.6406926406926406, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.4805194805194805, 0.5411255411255411, 0.6147186147186147, 0.6277056277056277, 0.6277056277056277, 0.6320346320346321, 0.6320346320346321, 0.6320346320346321, 0.6277056277056277, 0.6233766233766234, 0.6233766233766234, 0.6233766233766234, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6277056277056277, 0.6212121212121212, 0.6212121212121212]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ValidationSVMAcc))"
      ],
      "metadata": {
        "id": "QjjHJZx2HFLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdc9b95-f785-48d2-b362-f029d90359ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Vdt0=ValidationDTAcc[0:25]\n",
        "Vdt1=ValidationDTAcc[25:50]\n",
        "Vdt2=ValidationDTAcc[50:75]\n",
        "Vdt3=ValidationDTAcc[75:100]\n",
        "Vdt4=ValidationDTAcc[100:125]\n",
        "Tdt0=TestDTAcc[0:25]\n",
        "Tdt1=TestDTAcc[25:50]\n",
        "Tdt2=TestDTAcc[50:75]\n",
        "Tdt3=TestDTAcc[75:100]\n",
        "Tdt4=TestDTAcc[100:125]"
      ],
      "metadata": {
        "id": "8CGdurwZIabV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vrf0=ValidationRFAcc[0:25]\n",
        "Vrf1=ValidationRFAcc[25:50]\n",
        "Vrf2=ValidationRFAcc[50:75]\n",
        "Vrf3=ValidationRFAcc[75:100]\n",
        "Vrf4=ValidationRFAcc[100:125]\n",
        "Trf0=TestRFAcc[0:25]\n",
        "Trf1=TestRFAcc[25:50]\n",
        "Trf2=TestRFAcc[50:75]\n",
        "Trf3=TestRFAcc[75:100]\n",
        "Trf4=TestRFAcc[100:125]"
      ],
      "metadata": {
        "id": "jDx8P248JESo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vxgboost0=ValidationXGBoostAcc[0:25]\n",
        "Vxgboost1=ValidationXGBoostAcc[25:50]\n",
        "Vxgboost2=ValidationXGBoostAcc[50:75]\n",
        "Vxgboost3=ValidationXGBoostAcc[75:100]\n",
        "Vxgboost4=ValidationXGBoostAcc[100:125]\n",
        "Txgboost0=TestXGBoostAcc[0:25]\n",
        "Txgboost1=TestXGBoostAcc[25:50]\n",
        "Txgboost2=TestXGBoostAcc[50:75]\n",
        "Txgboost3=TestXGBoostAcc[75:100]\n",
        "Txgboost4=TestXGBoostAccTrf4=TestRFAcc[100:125]"
      ],
      "metadata": {
        "id": "O9dIThBWJd9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vaidation and Test Accuracey Results For SVM Classifier:\")\n",
        "\n",
        "Vsvm0=ValidationSVMAcc[0:25]\n",
        "Vsvm1=ValidationSVMAcc[25:50]\n",
        "Vsvm2=ValidationSVMAcc[50:75]\n",
        "Vsvm3=ValidationSVMAcc[75:100]\n",
        "Vsvm4=ValidationSVMAcc[100:125]\n",
        "Tsvm0=TestSVMAcc[0:25]\n",
        "Tsvm1=TestSVMAcc[25:50]\n",
        "Tsvm2=TestSVMAcc[50:75]\n",
        "Tsvm3=TestSVMAcc[75:100]\n",
        "Tsvm4=TestSVMAcc[100:125]\n",
        "\n",
        "MET=[]\n",
        "MAT=[]\n",
        "SDT=[]\n",
        "VT=[]\n",
        "ET=[]\n",
        "MEV=[]\n",
        "MAV=[]\n",
        "SDV=[]\n",
        "VV=[]\n",
        "EV=[]\n",
        "A=[]\n",
        "B=[]\n",
        "# Original array\n",
        "for i in range(0,25):\n",
        "  a= np.array([Vsvm0[i],Vsvm1[i],Vsvm2[i],Vsvm3[i],Vsvm4[i]])\n",
        "  b= np.array([Tsvm0[i],Tsvm1[i],Tsvm2[i],Tsvm3[i],Tsvm4[i]])\n",
        "  A.append(a)\n",
        "  B.append(b)\n",
        "  print(A[i])\n",
        "  print(B[i])\n",
        "  mav= np.max(A[i])\n",
        "  MAV.append(mav)\n",
        "  mat= np.max(B[i])\n",
        "  MAT.append(mat)\n",
        "  print(\"\\nMax Validation Accuracy value of top k= \", i+1,\":\", mav)\n",
        "  print(\"\\nMax Test Accuracy value of top k= \", i+1,\":\", mat)\n",
        "\n",
        "  mev= np.mean(A[i])\n",
        "  MEV.append(mev)\n",
        "  print(\"\\nMean Validation Accuracy value of top k= \", i+1,\":\", mev)\n",
        "  met= np.mean(B[i])\n",
        "  MET.append(met)\n",
        "  print(\"\\nMean Test Accuracy value of top k= \", i+1,\":\", met)\n",
        "   \n",
        "  sdv= np.std(A[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Validation Accuracy value of top k= \", i+1,\":\", sdv)\n",
        "  SDV.append(sdv)\n",
        "\n",
        "\n",
        "  sdt= np.std(B[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Test Accuracy value of top k= \", i+1,\":\", sdt)\n",
        "  SDT.append(sdt)\n",
        "\n",
        "  ev=mev+sdv\n",
        "  print(\"\\n Mean + SD: of Validation Accuracy value of top k= \", i+1,\":\", mev, \"+-\", sdv)\n",
        "  print(ev)\n",
        "  EV.append(ev)\n",
        "\n",
        "  et=met+sdt\n",
        "  print(\"\\n Mean + SD: of Test Accuracy value of top k= \", i+1,\":\", met, \"+-\", sdt)\n",
        "  print(et)\n",
        "  ET.append(et)\n",
        "  \n",
        "  vv= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vv)\n",
        "  VV.append(vv)\n",
        "\n",
        "  vt= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vt)\n",
        "  VT.append(vt)\n",
        "\n",
        "X= np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
        "df=pd.DataFrame({'Top K Number': X+1, 'SVM Max Validation Accuracy': MAV, 'SVM Mean Validation Accuracy': MEV, 'SVM SD Validation Accuracy': SDV, 'SVM Mean+-SD Validation Accuracy': EV})\n",
        "print(tabulate(df,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df.to_excel('SVMValidationResults.xlsx')\n",
        "df1=pd.DataFrame({'Top K Number': X+1, 'SVM Max Test Accuracy': MAT, 'SVM Mean Test Accuracy': MET, 'SVM SD Test Accuracy': SDT, 'SVM Mean+-SD Test Accuracy': ET})\n",
        "print(tabulate(df1,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df1.to_excel('SVMTestResults.xlsx')\n"
      ],
      "metadata": {
        "id": "DMjTN8IfMAFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240c504f-1d24-43ec-bd63-5ee4ab866311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vaidation and Test Accuracey Results For SVM Classifier:\n",
            "[0.4537037  0.49074074 0.5        0.46759259 0.49074074]\n",
            "[0.50649351 0.44372294 0.46536797 0.46969697 0.5021645 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  1 : 0.5\n",
            "\n",
            "Max Test Accuracy value of top k=  1 : 0.5064935064935064\n",
            "\n",
            "Mean Validation Accuracy value of top k=  1 : 0.4805555555555555\n",
            "\n",
            "Mean Test Accuracy value of top k=  1 : 0.4774891774891775\n",
            "\n",
            "std of Validation Accuracy value of top k=  1 : 0.0171733675842513\n",
            "\n",
            "std of Test Accuracy value of top k=  1 : 0.023655543016994348\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  1 : 0.4805555555555555 +- 0.0171733675842513\n",
            "0.4977289231398068\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  1 : 0.4774891774891775 +- 0.023655543016994348\n",
            "0.5011447205061719\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "[0.4212963  0.49074074 0.44907407 0.40277778 0.44444444]\n",
            "[0.45238095 0.45454545 0.46536797 0.41341991 0.43939394]\n",
            "\n",
            "Max Validation Accuracy value of top k=  2 : 0.49074074074074076\n",
            "\n",
            "Max Test Accuracy value of top k=  2 : 0.4653679653679654\n",
            "\n",
            "Mean Validation Accuracy value of top k=  2 : 0.4416666666666666\n",
            "\n",
            "Mean Test Accuracy value of top k=  2 : 0.445021645021645\n",
            "\n",
            "std of Validation Accuracy value of top k=  2 : 0.02965855070008698\n",
            "\n",
            "std of Test Accuracy value of top k=  2 : 0.017827930980064074\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  2 : 0.4416666666666666 +- 0.02965855070008698\n",
            "0.4713252173667536\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  2 : 0.445021645021645 +- 0.017827930980064074\n",
            "0.46284957600170906\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "[0.43518519 0.48611111 0.43981481 0.40277778 0.48611111]\n",
            "[0.45454545 0.45887446 0.45670996 0.41341991 0.45887446]\n",
            "\n",
            "Max Validation Accuracy value of top k=  3 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  3 : 0.4588744588744589\n",
            "\n",
            "Mean Validation Accuracy value of top k=  3 : 0.45\n",
            "\n",
            "Mean Test Accuracy value of top k=  3 : 0.44848484848484854\n",
            "\n",
            "std of Validation Accuracy value of top k=  3 : 0.032128428838699014\n",
            "\n",
            "std of Test Accuracy value of top k=  3 : 0.01760580035295477\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  3 : 0.45 +- 0.032128428838699014\n",
            "0.482128428838699\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  3 : 0.44848484848484854 +- 0.01760580035295477\n",
            "0.4660906488378033\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "[0.4212963  0.52777778 0.49537037 0.49537037 0.53703704]\n",
            "[0.5        0.48701299 0.51515152 0.45021645 0.51948052]\n",
            "\n",
            "Max Validation Accuracy value of top k=  4 : 0.5370370370370371\n",
            "\n",
            "Max Test Accuracy value of top k=  4 : 0.5194805194805194\n",
            "\n",
            "Mean Validation Accuracy value of top k=  4 : 0.4953703703703704\n",
            "\n",
            "Mean Test Accuracy value of top k=  4 : 0.49437229437229435\n",
            "\n",
            "std of Validation Accuracy value of top k=  4 : 0.04067756043794223\n",
            "\n",
            "std of Test Accuracy value of top k=  4 : 0.024890833786086648\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  4 : 0.4953703703703704 +- 0.04067756043794223\n",
            "0.5360479308083126\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  4 : 0.49437229437229435 +- 0.024890833786086648\n",
            "0.519263128158381\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "[0.38888889 0.4537037  0.40277778 0.38425926 0.48611111]\n",
            "[0.44155844 0.41558442 0.44805195 0.43073593 0.44372294]\n",
            "\n",
            "Max Validation Accuracy value of top k=  5 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  5 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  5 : 0.4231481481481481\n",
            "\n",
            "Mean Test Accuracy value of top k=  5 : 0.435930735930736\n",
            "\n",
            "std of Validation Accuracy value of top k=  5 : 0.039997427900842916\n",
            "\n",
            "std of Test Accuracy value of top k=  5 : 0.011664236870396073\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  5 : 0.4231481481481481 +- 0.039997427900842916\n",
            "0.46314557604899104\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  5 : 0.435930735930736 +- 0.011664236870396073\n",
            "0.4475949728011321\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "[0.38888889 0.44444444 0.40277778 0.375      0.48611111]\n",
            "[0.44155844 0.41125541 0.44805195 0.42640693 0.44155844]\n",
            "\n",
            "Max Validation Accuracy value of top k=  6 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  6 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  6 : 0.41944444444444445\n",
            "\n",
            "Mean Test Accuracy value of top k=  6 : 0.4337662337662337\n",
            "\n",
            "std of Validation Accuracy value of top k=  6 : 0.04063538566202164\n",
            "\n",
            "std of Test Accuracy value of top k=  6 : 0.013314767531800595\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  6 : 0.41944444444444445 +- 0.04063538566202164\n",
            "0.4600798301064661\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  6 : 0.4337662337662337 +- 0.013314767531800595\n",
            "0.4470810012980343\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "[0.38425926 0.44444444 0.40277778 0.375      0.47685185]\n",
            "[0.44155844 0.41341991 0.44372294 0.43722944 0.44155844]\n",
            "\n",
            "Max Validation Accuracy value of top k=  7 : 0.47685185185185186\n",
            "\n",
            "Max Test Accuracy value of top k=  7 : 0.44372294372294374\n",
            "\n",
            "Mean Validation Accuracy value of top k=  7 : 0.41666666666666663\n",
            "\n",
            "Mean Test Accuracy value of top k=  7 : 0.4354978354978355\n",
            "\n",
            "std of Validation Accuracy value of top k=  7 : 0.03840081732097726\n",
            "\n",
            "std of Test Accuracy value of top k=  7 : 0.011238748905408807\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  7 : 0.41666666666666663 +- 0.03840081732097726\n",
            "0.4550674839876439\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  7 : 0.4354978354978355 +- 0.011238748905408807\n",
            "0.4467365844032443\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "[0.38425926 0.43518519 0.40740741 0.37962963 0.47685185]\n",
            "[0.44155844 0.40909091 0.44155844 0.43722944 0.44155844]\n",
            "\n",
            "Max Validation Accuracy value of top k=  8 : 0.47685185185185186\n",
            "\n",
            "Max Test Accuracy value of top k=  8 : 0.44155844155844154\n",
            "\n",
            "Mean Validation Accuracy value of top k=  8 : 0.41666666666666663\n",
            "\n",
            "Mean Test Accuracy value of top k=  8 : 0.4341991341991342\n",
            "\n",
            "std of Validation Accuracy value of top k=  8 : 0.03598029486615824\n",
            "\n",
            "std of Test Accuracy value of top k=  8 : 0.01266557475179894\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  8 : 0.41666666666666663 +- 0.03598029486615824\n",
            "0.45264696153282485\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  8 : 0.4341991341991342 +- 0.01266557475179894\n",
            "0.4468647089509331\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "[0.38425926 0.43518519 0.40740741 0.38888889 0.47685185]\n",
            "[0.44372294 0.40692641 0.44155844 0.43939394 0.44155844]\n",
            "\n",
            "Max Validation Accuracy value of top k=  9 : 0.47685185185185186\n",
            "\n",
            "Max Test Accuracy value of top k=  9 : 0.44372294372294374\n",
            "\n",
            "Mean Validation Accuracy value of top k=  9 : 0.4185185185185185\n",
            "\n",
            "Mean Test Accuracy value of top k=  9 : 0.4346320346320346\n",
            "\n",
            "std of Validation Accuracy value of top k=  9 : 0.034221701134400616\n",
            "\n",
            "std of Test Accuracy value of top k=  9 : 0.013920290208496566\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  9 : 0.4185185185185185 +- 0.034221701134400616\n",
            "0.4527402196529191\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  9 : 0.4346320346320346 +- 0.013920290208496566\n",
            "0.44855232484053115\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "[0.37962963 0.43518519 0.40740741 0.38425926 0.47685185]\n",
            "[0.44372294 0.40692641 0.44372294 0.43939394 0.44155844]\n",
            "\n",
            "Max Validation Accuracy value of top k=  10 : 0.47685185185185186\n",
            "\n",
            "Max Test Accuracy value of top k=  10 : 0.44372294372294374\n",
            "\n",
            "Mean Validation Accuracy value of top k=  10 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  10 : 0.4350649350649351\n",
            "\n",
            "std of Validation Accuracy value of top k=  10 : 0.03598029486615824\n",
            "\n",
            "std of Test Accuracy value of top k=  10 : 0.014160543059563754\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  10 : 0.4166666666666667 +- 0.03598029486615824\n",
            "0.4526469615328249\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  10 : 0.4350649350649351 +- 0.014160543059563754\n",
            "0.44922547812449887\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "[0.37962963 0.43518519 0.40740741 0.38425926 0.47685185]\n",
            "[0.44372294 0.40692641 0.44372294 0.43722944 0.44155844]\n",
            "\n",
            "Max Validation Accuracy value of top k=  11 : 0.47685185185185186\n",
            "\n",
            "Max Test Accuracy value of top k=  11 : 0.44372294372294374\n",
            "\n",
            "Mean Validation Accuracy value of top k=  11 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  11 : 0.4346320346320346\n",
            "\n",
            "std of Validation Accuracy value of top k=  11 : 0.03598029486615824\n",
            "\n",
            "std of Test Accuracy value of top k=  11 : 0.01405427106558263\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  11 : 0.4166666666666667 +- 0.03598029486615824\n",
            "0.4526469615328249\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  11 : 0.4346320346320346 +- 0.01405427106558263\n",
            "0.44868630569761725\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "[0.37962963 0.43518519 0.40740741 0.37962963 0.47685185]\n",
            "[0.44372294 0.40692641 0.44155844 0.43722944 0.44372294]\n",
            "\n",
            "Max Validation Accuracy value of top k=  12 : 0.47685185185185186\n",
            "\n",
            "Max Test Accuracy value of top k=  12 : 0.44372294372294374\n",
            "\n",
            "Mean Validation Accuracy value of top k=  12 : 0.41574074074074086\n",
            "\n",
            "Mean Test Accuracy value of top k=  12 : 0.4346320346320346\n",
            "\n",
            "std of Validation Accuracy value of top k=  12 : 0.036851386559504436\n",
            "\n",
            "std of Test Accuracy value of top k=  12 : 0.01405427106558263\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  12 : 0.41574074074074086 +- 0.036851386559504436\n",
            "0.4525921273002453\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  12 : 0.4346320346320346 +- 0.01405427106558263\n",
            "0.44868630569761725\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48148148]\n",
            "[0.44372294 0.40909091 0.44155844 0.43506494 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  13 : 0.48148148148148145\n",
            "\n",
            "Max Test Accuracy value of top k=  13 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  13 : 0.41574074074074074\n",
            "\n",
            "Mean Test Accuracy value of top k=  13 : 0.4354978354978355\n",
            "\n",
            "std of Validation Accuracy value of top k=  13 : 0.038667801884853865\n",
            "\n",
            "std of Test Accuracy value of top k=  13 : 0.01385281385281384\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  13 : 0.41574074074074074 +- 0.038667801884853865\n",
            "0.4544085426255946\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  13 : 0.4354978354978355 +- 0.01385281385281384\n",
            "0.44935064935064933\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48148148]\n",
            "[0.44372294 0.40909091 0.44155844 0.43290043 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  14 : 0.48148148148148145\n",
            "\n",
            "Max Test Accuracy value of top k=  14 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  14 : 0.41574074074074074\n",
            "\n",
            "Mean Test Accuracy value of top k=  14 : 0.43506493506493504\n",
            "\n",
            "std of Validation Accuracy value of top k=  14 : 0.038667801884853865\n",
            "\n",
            "std of Test Accuracy value of top k=  14 : 0.013893338992104935\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  14 : 0.41574074074074074 +- 0.038667801884853865\n",
            "0.4544085426255946\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  14 : 0.43506493506493504 +- 0.013893338992104935\n",
            "0.44895827405704\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48611111]\n",
            "[0.44372294 0.40909091 0.44155844 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  15 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  15 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  15 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  15 : 0.4346320346320346\n",
            "\n",
            "std of Validation Accuracy value of top k=  15 : 0.040253824294970646\n",
            "\n",
            "std of Test Accuracy value of top k=  15 : 0.013987441057492207\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  15 : 0.4166666666666667 +- 0.040253824294970646\n",
            "0.45692049096163734\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  15 : 0.4346320346320346 +- 0.013987441057492207\n",
            "0.4486194756895268\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48611111]\n",
            "[0.44372294 0.40909091 0.44155844 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  16 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  16 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  16 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  16 : 0.4346320346320346\n",
            "\n",
            "std of Validation Accuracy value of top k=  16 : 0.040253824294970646\n",
            "\n",
            "std of Test Accuracy value of top k=  16 : 0.013987441057492207\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  16 : 0.4166666666666667 +- 0.040253824294970646\n",
            "0.45692049096163734\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  16 : 0.4346320346320346 +- 0.013987441057492207\n",
            "0.4486194756895268\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48611111]\n",
            "[0.44372294 0.40909091 0.44155844 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  17 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  17 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  17 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  17 : 0.4346320346320346\n",
            "\n",
            "std of Validation Accuracy value of top k=  17 : 0.040253824294970646\n",
            "\n",
            "std of Test Accuracy value of top k=  17 : 0.013987441057492207\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  17 : 0.4166666666666667 +- 0.040253824294970646\n",
            "0.45692049096163734\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  17 : 0.4346320346320346 +- 0.013987441057492207\n",
            "0.4486194756895268\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48611111]\n",
            "[0.44372294 0.40909091 0.44155844 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  18 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  18 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  18 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  18 : 0.4346320346320346\n",
            "\n",
            "std of Validation Accuracy value of top k=  18 : 0.040253824294970646\n",
            "\n",
            "std of Test Accuracy value of top k=  18 : 0.013987441057492207\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  18 : 0.4166666666666667 +- 0.040253824294970646\n",
            "0.45692049096163734\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  18 : 0.4346320346320346 +- 0.013987441057492207\n",
            "0.4486194756895268\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48611111]\n",
            "[0.44372294 0.40909091 0.44155844 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  19 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  19 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  19 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  19 : 0.4346320346320346\n",
            "\n",
            "std of Validation Accuracy value of top k=  19 : 0.040253824294970646\n",
            "\n",
            "std of Test Accuracy value of top k=  19 : 0.013987441057492207\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  19 : 0.4166666666666667 +- 0.040253824294970646\n",
            "0.45692049096163734\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  19 : 0.4346320346320346 +- 0.013987441057492207\n",
            "0.4486194756895268\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48611111]\n",
            "[0.44372294 0.40909091 0.43939394 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  20 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  20 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  20 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  20 : 0.4341991341991342\n",
            "\n",
            "std of Validation Accuracy value of top k=  20 : 0.040253824294970646\n",
            "\n",
            "std of Test Accuracy value of top k=  20 : 0.013798595195246075\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  20 : 0.4166666666666667 +- 0.040253824294970646\n",
            "0.45692049096163734\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  20 : 0.4341991341991342 +- 0.013798595195246075\n",
            "0.44799772939438026\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.49074074]\n",
            "[0.44372294 0.40909091 0.43939394 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  21 : 0.49074074074074076\n",
            "\n",
            "Max Test Accuracy value of top k=  21 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  21 : 0.4175925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  21 : 0.4341991341991342\n",
            "\n",
            "std of Validation Accuracy value of top k=  21 : 0.04186168353873079\n",
            "\n",
            "std of Test Accuracy value of top k=  21 : 0.013798595195246075\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  21 : 0.4175925925925926 +- 0.04186168353873079\n",
            "0.4594542761313234\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  21 : 0.4341991341991342 +- 0.013798595195246075\n",
            "0.44799772939438026\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.48611111]\n",
            "[0.44372294 0.40909091 0.43939394 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  22 : 0.4861111111111111\n",
            "\n",
            "Max Test Accuracy value of top k=  22 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  22 : 0.4166666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  22 : 0.4341991341991342\n",
            "\n",
            "std of Validation Accuracy value of top k=  22 : 0.040253824294970646\n",
            "\n",
            "std of Test Accuracy value of top k=  22 : 0.013798595195246075\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  22 : 0.4166666666666667 +- 0.040253824294970646\n",
            "0.45692049096163734\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  22 : 0.4341991341991342 +- 0.013798595195246075\n",
            "0.44799772939438026\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.49074074]\n",
            "[0.44372294 0.40909091 0.43939394 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  23 : 0.49074074074074076\n",
            "\n",
            "Max Test Accuracy value of top k=  23 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  23 : 0.4175925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  23 : 0.4341991341991342\n",
            "\n",
            "std of Validation Accuracy value of top k=  23 : 0.04186168353873079\n",
            "\n",
            "std of Test Accuracy value of top k=  23 : 0.013798595195246075\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  23 : 0.4175925925925926 +- 0.04186168353873079\n",
            "0.4594542761313234\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  23 : 0.4341991341991342 +- 0.013798595195246075\n",
            "0.44799772939438026\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.49074074]\n",
            "[0.44372294 0.40909091 0.43939394 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  24 : 0.49074074074074076\n",
            "\n",
            "Max Test Accuracy value of top k=  24 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  24 : 0.4175925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  24 : 0.4341991341991342\n",
            "\n",
            "std of Validation Accuracy value of top k=  24 : 0.04186168353873079\n",
            "\n",
            "std of Test Accuracy value of top k=  24 : 0.013798595195246075\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  24 : 0.4175925925925926 +- 0.04186168353873079\n",
            "0.4594542761313234\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  24 : 0.4341991341991342 +- 0.013798595195246075\n",
            "0.44799772939438026\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "[0.37962963 0.43518519 0.40277778 0.37962963 0.49074074]\n",
            "[0.44372294 0.40909091 0.43722944 0.43073593 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  25 : 0.49074074074074076\n",
            "\n",
            "Max Test Accuracy value of top k=  25 : 0.44805194805194803\n",
            "\n",
            "Mean Validation Accuracy value of top k=  25 : 0.4175925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  25 : 0.4337662337662338\n",
            "\n",
            "std of Validation Accuracy value of top k=  25 : 0.04186168353873079\n",
            "\n",
            "std of Test Accuracy value of top k=  25 : 0.013662107219099124\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  25 : 0.4175925925925926 +- 0.04186168353873079\n",
            "0.4594542761313234\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  25 : 0.4337662337662338 +- 0.013662107219099124\n",
            "0.4474283409853329\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "╒════╤════════════════╤═══════════════════════════════╤════════════════════════════════╤══════════════════════════════╤════════════════════════════════════╕\n",
            "│    │   Top K Number │   SVM Max Validation Accuracy │   SVM Mean Validation Accuracy │   SVM SD Validation Accuracy │   SVM Mean+-SD Validation Accuracy │\n",
            "╞════╪════════════════╪═══════════════════════════════╪════════════════════════════════╪══════════════════════════════╪════════════════════════════════════╡\n",
            "│  0 │              1 │                      0.5      │                       0.480556 │                    0.0171734 │                           0.497729 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  1 │              2 │                      0.490741 │                       0.441667 │                    0.0296586 │                           0.471325 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  2 │              3 │                      0.486111 │                       0.45     │                    0.0321284 │                           0.482128 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  3 │              4 │                      0.537037 │                       0.49537  │                    0.0406776 │                           0.536048 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  4 │              5 │                      0.486111 │                       0.423148 │                    0.0399974 │                           0.463146 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  5 │              6 │                      0.486111 │                       0.419444 │                    0.0406354 │                           0.46008  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  6 │              7 │                      0.476852 │                       0.416667 │                    0.0384008 │                           0.455067 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  7 │              8 │                      0.476852 │                       0.416667 │                    0.0359803 │                           0.452647 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  8 │              9 │                      0.476852 │                       0.418519 │                    0.0342217 │                           0.45274  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  9 │             10 │                      0.476852 │                       0.416667 │                    0.0359803 │                           0.452647 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 10 │             11 │                      0.476852 │                       0.416667 │                    0.0359803 │                           0.452647 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 11 │             12 │                      0.476852 │                       0.415741 │                    0.0368514 │                           0.452592 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 12 │             13 │                      0.481481 │                       0.415741 │                    0.0386678 │                           0.454409 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 13 │             14 │                      0.481481 │                       0.415741 │                    0.0386678 │                           0.454409 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 14 │             15 │                      0.486111 │                       0.416667 │                    0.0402538 │                           0.45692  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 15 │             16 │                      0.486111 │                       0.416667 │                    0.0402538 │                           0.45692  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 16 │             17 │                      0.486111 │                       0.416667 │                    0.0402538 │                           0.45692  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 17 │             18 │                      0.486111 │                       0.416667 │                    0.0402538 │                           0.45692  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 18 │             19 │                      0.486111 │                       0.416667 │                    0.0402538 │                           0.45692  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 19 │             20 │                      0.486111 │                       0.416667 │                    0.0402538 │                           0.45692  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 20 │             21 │                      0.490741 │                       0.417593 │                    0.0418617 │                           0.459454 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 21 │             22 │                      0.486111 │                       0.416667 │                    0.0402538 │                           0.45692  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 22 │             23 │                      0.490741 │                       0.417593 │                    0.0418617 │                           0.459454 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 23 │             24 │                      0.490741 │                       0.417593 │                    0.0418617 │                           0.459454 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 24 │             25 │                      0.490741 │                       0.417593 │                    0.0418617 │                           0.459454 │\n",
            "╘════╧════════════════╧═══════════════════════════════╧════════════════════════════════╧══════════════════════════════╧════════════════════════════════════╛\n",
            "╒════╤════════════════╤═════════════════════════╤══════════════════════════╤════════════════════════╤══════════════════════════════╕\n",
            "│    │   Top K Number │   SVM Max Test Accuracy │   SVM Mean Test Accuracy │   SVM SD Test Accuracy │   SVM Mean+-SD Test Accuracy │\n",
            "╞════╪════════════════╪═════════════════════════╪══════════════════════════╪════════════════════════╪══════════════════════════════╡\n",
            "│  0 │              1 │                0.506494 │                 0.477489 │              0.0236555 │                     0.501145 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  1 │              2 │                0.465368 │                 0.445022 │              0.0178279 │                     0.46285  │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  2 │              3 │                0.458874 │                 0.448485 │              0.0176058 │                     0.466091 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  3 │              4 │                0.519481 │                 0.494372 │              0.0248908 │                     0.519263 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  4 │              5 │                0.448052 │                 0.435931 │              0.0116642 │                     0.447595 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  5 │              6 │                0.448052 │                 0.433766 │              0.0133148 │                     0.447081 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  6 │              7 │                0.443723 │                 0.435498 │              0.0112387 │                     0.446737 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  7 │              8 │                0.441558 │                 0.434199 │              0.0126656 │                     0.446865 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  8 │              9 │                0.443723 │                 0.434632 │              0.0139203 │                     0.448552 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  9 │             10 │                0.443723 │                 0.435065 │              0.0141605 │                     0.449225 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 10 │             11 │                0.443723 │                 0.434632 │              0.0140543 │                     0.448686 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 11 │             12 │                0.443723 │                 0.434632 │              0.0140543 │                     0.448686 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 12 │             13 │                0.448052 │                 0.435498 │              0.0138528 │                     0.449351 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 13 │             14 │                0.448052 │                 0.435065 │              0.0138933 │                     0.448958 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 14 │             15 │                0.448052 │                 0.434632 │              0.0139874 │                     0.448619 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 15 │             16 │                0.448052 │                 0.434632 │              0.0139874 │                     0.448619 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 16 │             17 │                0.448052 │                 0.434632 │              0.0139874 │                     0.448619 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 17 │             18 │                0.448052 │                 0.434632 │              0.0139874 │                     0.448619 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 18 │             19 │                0.448052 │                 0.434632 │              0.0139874 │                     0.448619 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 19 │             20 │                0.448052 │                 0.434199 │              0.0137986 │                     0.447998 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 20 │             21 │                0.448052 │                 0.434199 │              0.0137986 │                     0.447998 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 21 │             22 │                0.448052 │                 0.434199 │              0.0137986 │                     0.447998 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 22 │             23 │                0.448052 │                 0.434199 │              0.0137986 │                     0.447998 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 23 │             24 │                0.448052 │                 0.434199 │              0.0137986 │                     0.447998 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 24 │             25 │                0.448052 │                 0.433766 │              0.0136621 │                     0.447428 │\n",
            "╘════╧════════════════╧═════════════════════════╧══════════════════════════╧════════════════════════╧══════════════════════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vaidation and Test Accuracey Results For KNN Classifier:\")\n",
        "\n",
        "Vknn0=ValidationKNNAcc[0:25]\n",
        "Vknn1=ValidationKNNAcc[25:50]\n",
        "Vknn2=ValidationKNNAcc[50:75]\n",
        "Vknn3=ValidationKNNAcc[75:100]\n",
        "Vknn4=ValidationKNNAcc[100:125]\n",
        "Tknn0=TestKNNAcc[0:25]\n",
        "Tknn1=TestKNNAcc[25:50]\n",
        "Tknn2=TestKNNAcc[50:75]\n",
        "Tknn3=TestKNNAcc[75:100]\n",
        "Tknn4=TestKNNAcc[100:125]\n",
        "\n",
        "MET=[]\n",
        "MAT=[]\n",
        "SDT=[]\n",
        "VT=[]\n",
        "ET=[]\n",
        "MEV=[]\n",
        "MAV=[]\n",
        "SDV=[]\n",
        "VV=[]\n",
        "EV=[]\n",
        "A=[]\n",
        "B=[]\n",
        "# Original array\n",
        "for i in range(0,25):\n",
        "  a= np.array([Vknn0[i],Vknn1[i],Vknn2[i],Vknn3[i],Vknn4[i]])\n",
        "  b= np.array([Tknn0[i],Tknn1[i],Tknn2[i],Tknn3[i],Tknn4[i]])\n",
        "  A.append(a)\n",
        "  B.append(b)\n",
        "  print(A[i])\n",
        "  print(B[i])\n",
        "  mav= np.max(A[i])\n",
        "  MAV.append(mav)\n",
        "  mat= np.max(B[i])\n",
        "  MAT.append(mat)\n",
        "  print(\"\\nMax Validation Accuracy value of top k= \", i+1,\":\", mav)\n",
        "  print(\"\\nMax Test Accuracy value of top k= \", i+1,\":\", mat)\n",
        "\n",
        "  mev= np.mean(A[i])\n",
        "  MEV.append(mev)\n",
        "  print(\"\\nMean Validation Accuracy value of top k= \", i+1,\":\", mev)\n",
        "  met= np.mean(B[i])\n",
        "  MET.append(met)\n",
        "  print(\"\\nMean Test Accuracy value of top k= \", i+1,\":\", met)\n",
        "   \n",
        "  sdv= np.std(A[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Validation Accuracy value of top k= \", i+1,\":\", sdv)\n",
        "  SDV.append(sdv)\n",
        "\n",
        "\n",
        "  sdt= np.std(B[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Test Accuracy value of top k= \", i+1,\":\", sdt)\n",
        "  SDT.append(sdt)\n",
        "\n",
        "  ev=mev+sdv\n",
        "  print(\"\\n Mean + SD: of Validation Accuracy value of top k= \", i+1,\":\", mev, \"+-\", sdv)\n",
        "  print(ev)\n",
        "  EV.append(ev)\n",
        "\n",
        "  et=met+sdt\n",
        "  print(\"\\n Mean + SD: of Test Accuracy value of top k= \", i+1,\":\", met, \"+-\", sdt)\n",
        "  print(et)\n",
        "  ET.append(et)\n",
        "  \n",
        "  vv= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vv)\n",
        "  VV.append(vv)\n",
        "\n",
        "  vt= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vt)\n",
        "  VT.append(vt)\n",
        "\n",
        "X= np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
        "df=pd.DataFrame({'Top K Number': X+1, 'KNN Max Validation Accuracy': MAV, 'KNN Mean Validation Accuracy': MEV, 'KNN SD Validation Accuracy': SDV, 'KNN Mean+-SD Validation Accuracy': EV})\n",
        "print(tabulate(df,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df.to_excel('KNNValidationResults.xlsx')\n",
        "df1=pd.DataFrame({'Top K Number': X+1, 'KNN Max Test Accuracy': MAT, 'Knn Mean Test Accuracy': MET, 'KNN SD Test Accuracy': SDT, 'KNN Mean+-SD Test Accuracy': ET})\n",
        "print(tabulate(df1,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df1.to_excel('KNNTestResults.xlsx')"
      ],
      "metadata": {
        "id": "iWU1jKfQSDZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cecd28-acf9-4f6b-cfa3-7261529e7922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vaidation and Test Accuracey Results For KNN Classifier:\n",
            "[0.375      0.42592593 0.44907407 0.45833333 0.49537037]\n",
            "[0.48701299 0.43290043 0.42207792 0.43290043 0.44805195]\n",
            "\n",
            "Max Validation Accuracy value of top k=  1 : 0.49537037037037035\n",
            "\n",
            "Max Test Accuracy value of top k=  1 : 0.487012987012987\n",
            "\n",
            "Mean Validation Accuracy value of top k=  1 : 0.44074074074074077\n",
            "\n",
            "Mean Test Accuracy value of top k=  1 : 0.4445887445887446\n",
            "\n",
            "std of Validation Accuracy value of top k=  1 : 0.03976094546959979\n",
            "\n",
            "std of Test Accuracy value of top k=  1 : 0.022767435138189972\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  1 : 0.44074074074074077 +- 0.03976094546959979\n",
            "0.48050168621034056\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  1 : 0.4445887445887446 +- 0.022767435138189972\n",
            "0.46735617972693455\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "[0.53240741 0.55092593 0.52314815 0.46296296 0.5787037 ]\n",
            "[0.53463203 0.55194805 0.57792208 0.56277056 0.52813853]\n",
            "\n",
            "Max Validation Accuracy value of top k=  2 : 0.5787037037037037\n",
            "\n",
            "Max Test Accuracy value of top k=  2 : 0.577922077922078\n",
            "\n",
            "Mean Validation Accuracy value of top k=  2 : 0.5296296296296297\n",
            "\n",
            "Mean Test Accuracy value of top k=  2 : 0.5510822510822512\n",
            "\n",
            "std of Validation Accuracy value of top k=  2 : 0.03835613921705182\n",
            "\n",
            "std of Test Accuracy value of top k=  2 : 0.018192122415108834\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  2 : 0.5296296296296297 +- 0.03835613921705182\n",
            "0.5679857688466815\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  2 : 0.5510822510822512 +- 0.018192122415108834\n",
            "0.56927437349736\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "[0.57407407 0.55092593 0.54166667 0.53240741 0.61111111]\n",
            "[0.55194805 0.56709957 0.5995671  0.6038961  0.55411255]\n",
            "\n",
            "Max Validation Accuracy value of top k=  3 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  3 : 0.6038961038961039\n",
            "\n",
            "Mean Validation Accuracy value of top k=  3 : 0.562037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  3 : 0.5753246753246752\n",
            "\n",
            "std of Validation Accuracy value of top k=  3 : 0.02817615565685731\n",
            "\n",
            "std of Test Accuracy value of top k=  3 : 0.02221753665862569\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  3 : 0.562037037037037 +- 0.02817615565685731\n",
            "0.5902131926938943\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  3 : 0.5753246753246752 +- 0.02221753665862569\n",
            "0.5975422119833009\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "[0.58333333 0.61111111 0.56944444 0.5787037  0.59722222]\n",
            "[0.58441558 0.60606061 0.61904762 0.58441558 0.55844156]\n",
            "\n",
            "Max Validation Accuracy value of top k=  4 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  4 : 0.6190476190476191\n",
            "\n",
            "Mean Validation Accuracy value of top k=  4 : 0.587962962962963\n",
            "\n",
            "Mean Test Accuracy value of top k=  4 : 0.5904761904761905\n",
            "\n",
            "std of Validation Accuracy value of top k=  4 : 0.0146401743526314\n",
            "\n",
            "std of Test Accuracy value of top k=  4 : 0.020779220779220803\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  4 : 0.587962962962963 +- 0.0146401743526314\n",
            "0.6026031373155945\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  4 : 0.5904761904761905 +- 0.020779220779220803\n",
            "0.6112554112554113\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.5787037  0.59722222]\n",
            "[0.58441558 0.60822511 0.61471861 0.57359307 0.57359307]\n",
            "\n",
            "Max Validation Accuracy value of top k=  5 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  5 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  5 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  5 : 0.5909090909090909\n",
            "\n",
            "std of Validation Accuracy value of top k=  5 : 0.0171733675842513\n",
            "\n",
            "std of Test Accuracy value of top k=  5 : 0.017370045582457912\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  5 : 0.587037037037037 +- 0.0171733675842513\n",
            "0.6042104046212883\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  5 : 0.5909090909090909 +- 0.017370045582457912\n",
            "0.6082791364915489\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.5787037  0.59722222]\n",
            "[0.58441558 0.60822511 0.61471861 0.57359307 0.57359307]\n",
            "\n",
            "Max Validation Accuracy value of top k=  6 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  6 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  6 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  6 : 0.5909090909090909\n",
            "\n",
            "std of Validation Accuracy value of top k=  6 : 0.0171733675842513\n",
            "\n",
            "std of Test Accuracy value of top k=  6 : 0.017370045582457912\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  6 : 0.587037037037037 +- 0.0171733675842513\n",
            "0.6042104046212883\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  6 : 0.5909090909090909 +- 0.017370045582457912\n",
            "0.6082791364915489\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.5787037  0.59722222]\n",
            "[0.58441558 0.60822511 0.61471861 0.57359307 0.57359307]\n",
            "\n",
            "Max Validation Accuracy value of top k=  7 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  7 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  7 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  7 : 0.5909090909090909\n",
            "\n",
            "std of Validation Accuracy value of top k=  7 : 0.0171733675842513\n",
            "\n",
            "std of Test Accuracy value of top k=  7 : 0.017370045582457912\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  7 : 0.587037037037037 +- 0.0171733675842513\n",
            "0.6042104046212883\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  7 : 0.5909090909090909 +- 0.017370045582457912\n",
            "0.6082791364915489\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.57407407 0.59722222]\n",
            "[0.58225108 0.60606061 0.61471861 0.57359307 0.57359307]\n",
            "\n",
            "Max Validation Accuracy value of top k=  8 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  8 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  8 : 0.586111111111111\n",
            "\n",
            "Mean Test Accuracy value of top k=  8 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  8 : 0.017714005990471286\n",
            "\n",
            "std of Test Accuracy value of top k=  8 : 0.017131046699055748\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  8 : 0.586111111111111 +- 0.017714005990471286\n",
            "0.6038251171015823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  8 : 0.59004329004329 +- 0.017131046699055748\n",
            "0.6071743367423458\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.57407407 0.59722222]\n",
            "[0.58225108 0.60606061 0.61471861 0.57359307 0.57359307]\n",
            "\n",
            "Max Validation Accuracy value of top k=  9 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  9 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  9 : 0.586111111111111\n",
            "\n",
            "Mean Test Accuracy value of top k=  9 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  9 : 0.017714005990471286\n",
            "\n",
            "std of Test Accuracy value of top k=  9 : 0.017131046699055748\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  9 : 0.586111111111111 +- 0.017714005990471286\n",
            "0.6038251171015823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  9 : 0.59004329004329 +- 0.017131046699055748\n",
            "0.6071743367423458\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.57407407 0.59722222]\n",
            "[0.58225108 0.60606061 0.61471861 0.57359307 0.57359307]\n",
            "\n",
            "Max Validation Accuracy value of top k=  10 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  10 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  10 : 0.586111111111111\n",
            "\n",
            "Mean Test Accuracy value of top k=  10 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  10 : 0.017714005990471286\n",
            "\n",
            "std of Test Accuracy value of top k=  10 : 0.017131046699055748\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  10 : 0.586111111111111 +- 0.017714005990471286\n",
            "0.6038251171015823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  10 : 0.59004329004329 +- 0.017131046699055748\n",
            "0.6071743367423458\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.57407407 0.59722222]\n",
            "[0.58225108 0.60606061 0.61688312 0.57359307 0.57359307]\n",
            "\n",
            "Max Validation Accuracy value of top k=  11 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  11 : 0.6168831168831169\n",
            "\n",
            "Mean Validation Accuracy value of top k=  11 : 0.586111111111111\n",
            "\n",
            "Mean Test Accuracy value of top k=  11 : 0.5904761904761905\n",
            "\n",
            "std of Validation Accuracy value of top k=  11 : 0.017714005990471286\n",
            "\n",
            "std of Test Accuracy value of top k=  11 : 0.01776474850968245\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  11 : 0.586111111111111 +- 0.017714005990471286\n",
            "0.6038251171015823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  11 : 0.5904761904761905 +- 0.01776474850968245\n",
            "0.608240938985873\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "[0.58796296 0.61574074 0.56018519 0.57407407 0.59722222]\n",
            "[0.58225108 0.60606061 0.61688312 0.57359307 0.57359307]\n",
            "\n",
            "Max Validation Accuracy value of top k=  12 : 0.6157407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  12 : 0.6168831168831169\n",
            "\n",
            "Mean Validation Accuracy value of top k=  12 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  12 : 0.5904761904761905\n",
            "\n",
            "std of Validation Accuracy value of top k=  12 : 0.019065981742568492\n",
            "\n",
            "std of Test Accuracy value of top k=  12 : 0.01776474850968245\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  12 : 0.587037037037037 +- 0.019065981742568492\n",
            "0.6061030187796055\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  12 : 0.5904761904761905 +- 0.01776474850968245\n",
            "0.608240938985873\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "[0.58796296 0.61574074 0.56018519 0.5787037  0.59722222]\n",
            "[0.58225108 0.60606061 0.61471861 0.57359307 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  13 : 0.6157407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  13 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  13 : 0.587962962962963\n",
            "\n",
            "Mean Test Accuracy value of top k=  13 : 0.5896103896103895\n",
            "\n",
            "std of Validation Accuracy value of top k=  13 : 0.01851851851851849\n",
            "\n",
            "std of Test Accuracy value of top k=  13 : 0.017563171233853505\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  13 : 0.587962962962963 +- 0.01851851851851849\n",
            "0.6064814814814815\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  13 : 0.5896103896103895 +- 0.017563171233853505\n",
            "0.607173560844243\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.5787037  0.59722222]\n",
            "[0.58225108 0.60606061 0.61688312 0.57359307 0.56926407]\n",
            "\n",
            "Max Validation Accuracy value of top k=  14 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  14 : 0.6168831168831169\n",
            "\n",
            "Mean Validation Accuracy value of top k=  14 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  14 : 0.5896103896103896\n",
            "\n",
            "std of Validation Accuracy value of top k=  14 : 0.0171733675842513\n",
            "\n",
            "std of Test Accuracy value of top k=  14 : 0.018649921409989653\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  14 : 0.587037037037037 +- 0.0171733675842513\n",
            "0.6042104046212883\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  14 : 0.5896103896103896 +- 0.018649921409989653\n",
            "0.6082603110203793\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "[0.58796296 0.61111111 0.56018519 0.5787037  0.59722222]\n",
            "[0.58225108 0.60606061 0.61688312 0.57142857 0.56926407]\n",
            "\n",
            "Max Validation Accuracy value of top k=  15 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  15 : 0.6168831168831169\n",
            "\n",
            "Mean Validation Accuracy value of top k=  15 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  15 : 0.5891774891774891\n",
            "\n",
            "std of Validation Accuracy value of top k=  15 : 0.0171733675842513\n",
            "\n",
            "std of Test Accuracy value of top k=  15 : 0.01903777785913078\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  15 : 0.587037037037037 +- 0.0171733675842513\n",
            "0.6042104046212883\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  15 : 0.5891774891774891 +- 0.01903777785913078\n",
            "0.6082152670366199\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "[0.58333333 0.61111111 0.56018519 0.5787037  0.59722222]\n",
            "[0.58225108 0.60606061 0.61688312 0.56926407 0.56926407]\n",
            "\n",
            "Max Validation Accuracy value of top k=  16 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  16 : 0.6168831168831169\n",
            "\n",
            "Mean Validation Accuracy value of top k=  16 : 0.5861111111111111\n",
            "\n",
            "Mean Test Accuracy value of top k=  16 : 0.5887445887445887\n",
            "\n",
            "std of Validation Accuracy value of top k=  16 : 0.017223217812720622\n",
            "\n",
            "std of Test Accuracy value of top k=  16 : 0.019456454592419268\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  16 : 0.5861111111111111 +- 0.017223217812720622\n",
            "0.6033343289238318\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  16 : 0.5887445887445887 +- 0.019456454592419268\n",
            "0.608201043337008\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "[0.58333333 0.61111111 0.56018519 0.5787037  0.59722222]\n",
            "[0.58441558 0.60606061 0.61688312 0.57142857 0.56926407]\n",
            "\n",
            "Max Validation Accuracy value of top k=  17 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  17 : 0.6168831168831169\n",
            "\n",
            "Mean Validation Accuracy value of top k=  17 : 0.5861111111111111\n",
            "\n",
            "Mean Test Accuracy value of top k=  17 : 0.5896103896103896\n",
            "\n",
            "std of Validation Accuracy value of top k=  17 : 0.017223217812720622\n",
            "\n",
            "std of Test Accuracy value of top k=  17 : 0.0188994631627213\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  17 : 0.5861111111111111 +- 0.017223217812720622\n",
            "0.6033343289238318\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  17 : 0.5896103896103896 +- 0.0188994631627213\n",
            "0.6085098527731109\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "[0.58333333 0.61111111 0.56018519 0.5787037  0.59722222]\n",
            "[0.58658009 0.60606061 0.61471861 0.57142857 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  18 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  18 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  18 : 0.5861111111111111\n",
            "\n",
            "Mean Test Accuracy value of top k=  18 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  18 : 0.017223217812720622\n",
            "\n",
            "std of Test Accuracy value of top k=  18 : 0.017722501723340885\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  18 : 0.5861111111111111 +- 0.017223217812720622\n",
            "0.6033343289238318\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  18 : 0.59004329004329 +- 0.017722501723340885\n",
            "0.6077657917666309\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "[0.58333333 0.61111111 0.56018519 0.58333333 0.59722222]\n",
            "[0.58658009 0.60606061 0.61471861 0.57142857 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  19 : 0.6111111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  19 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  19 : 0.5870370370370371\n",
            "\n",
            "Mean Test Accuracy value of top k=  19 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  19 : 0.016921913780089868\n",
            "\n",
            "std of Test Accuracy value of top k=  19 : 0.017722501723340885\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  19 : 0.5870370370370371 +- 0.016921913780089868\n",
            "0.603958950817127\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  19 : 0.59004329004329 +- 0.017722501723340885\n",
            "0.6077657917666309\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "[0.58333333 0.61574074 0.56018519 0.58333333 0.59722222]\n",
            "[0.58658009 0.60606061 0.61471861 0.57142857 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  20 : 0.6157407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  20 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  20 : 0.587962962962963\n",
            "\n",
            "Mean Test Accuracy value of top k=  20 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  20 : 0.01828557190567728\n",
            "\n",
            "std of Test Accuracy value of top k=  20 : 0.017722501723340885\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  20 : 0.587962962962963 +- 0.01828557190567728\n",
            "0.6062485348686403\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  20 : 0.59004329004329 +- 0.017722501723340885\n",
            "0.6077657917666309\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "[0.58333333 0.61574074 0.56018519 0.58333333 0.59722222]\n",
            "[0.58658009 0.60606061 0.61471861 0.57142857 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  21 : 0.6157407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  21 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  21 : 0.587962962962963\n",
            "\n",
            "Mean Test Accuracy value of top k=  21 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  21 : 0.01828557190567728\n",
            "\n",
            "std of Test Accuracy value of top k=  21 : 0.017722501723340885\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  21 : 0.587962962962963 +- 0.01828557190567728\n",
            "0.6062485348686403\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  21 : 0.59004329004329 +- 0.017722501723340885\n",
            "0.6077657917666309\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "[0.58333333 0.61574074 0.55555556 0.58333333 0.59722222]\n",
            "[0.58874459 0.6038961  0.61471861 0.57142857 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  22 : 0.6157407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  22 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  22 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  22 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  22 : 0.01972895903024305\n",
            "\n",
            "std of Test Accuracy value of top k=  22 : 0.017294358749297127\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  22 : 0.587037037037037 +- 0.01972895903024305\n",
            "0.6067659960672801\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  22 : 0.59004329004329 +- 0.017294358749297127\n",
            "0.6073376487925871\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "[0.58333333 0.61574074 0.55555556 0.58333333 0.59722222]\n",
            "[0.58874459 0.6038961  0.61471861 0.57142857 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  23 : 0.6157407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  23 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  23 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  23 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  23 : 0.01972895903024305\n",
            "\n",
            "std of Test Accuracy value of top k=  23 : 0.017294358749297127\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  23 : 0.587037037037037 +- 0.01972895903024305\n",
            "0.6067659960672801\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  23 : 0.59004329004329 +- 0.017294358749297127\n",
            "0.6073376487925871\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "[0.58333333 0.61574074 0.55555556 0.58333333 0.59722222]\n",
            "[0.58874459 0.6017316  0.61471861 0.57142857 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  24 : 0.6157407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  24 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  24 : 0.587037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  24 : 0.5896103896103895\n",
            "\n",
            "std of Validation Accuracy value of top k=  24 : 0.01972895903024305\n",
            "\n",
            "std of Test Accuracy value of top k=  24 : 0.01696616272057613\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  24 : 0.587037037037037 +- 0.01972895903024305\n",
            "0.6067659960672801\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  24 : 0.5896103896103895 +- 0.01696616272057613\n",
            "0.6065765523309656\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "[0.58333333 0.61574074 0.55555556 0.58333333 0.60185185]\n",
            "[0.58874459 0.6017316  0.61471861 0.57359307 0.57142857]\n",
            "\n",
            "Max Validation Accuracy value of top k=  25 : 0.6157407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  25 : 0.6147186147186147\n",
            "\n",
            "Mean Validation Accuracy value of top k=  25 : 0.587962962962963\n",
            "\n",
            "Mean Test Accuracy value of top k=  25 : 0.59004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  25 : 0.020286020648339464\n",
            "\n",
            "std of Test Accuracy value of top k=  25 : 0.016518427730163567\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  25 : 0.587962962962963 +- 0.020286020648339464\n",
            "0.6082489836113025\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  25 : 0.59004329004329 +- 0.016518427730163567\n",
            "0.6065617177734536\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "╒════╤════════════════╤═══════════════════════════════╤════════════════════════════════╤══════════════════════════════╤════════════════════════════════════╕\n",
            "│    │   Top K Number │   KNN Max Validation Accuracy │   KNN Mean Validation Accuracy │   KNN SD Validation Accuracy │   KNN Mean+-SD Validation Accuracy │\n",
            "╞════╪════════════════╪═══════════════════════════════╪════════════════════════════════╪══════════════════════════════╪════════════════════════════════════╡\n",
            "│  0 │              1 │                      0.49537  │                       0.440741 │                    0.0397609 │                           0.480502 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  1 │              2 │                      0.578704 │                       0.52963  │                    0.0383561 │                           0.567986 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  2 │              3 │                      0.611111 │                       0.562037 │                    0.0281762 │                           0.590213 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  3 │              4 │                      0.611111 │                       0.587963 │                    0.0146402 │                           0.602603 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  4 │              5 │                      0.611111 │                       0.587037 │                    0.0171734 │                           0.60421  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  5 │              6 │                      0.611111 │                       0.587037 │                    0.0171734 │                           0.60421  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  6 │              7 │                      0.611111 │                       0.587037 │                    0.0171734 │                           0.60421  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  7 │              8 │                      0.611111 │                       0.586111 │                    0.017714  │                           0.603825 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  8 │              9 │                      0.611111 │                       0.586111 │                    0.017714  │                           0.603825 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│  9 │             10 │                      0.611111 │                       0.586111 │                    0.017714  │                           0.603825 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 10 │             11 │                      0.611111 │                       0.586111 │                    0.017714  │                           0.603825 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 11 │             12 │                      0.615741 │                       0.587037 │                    0.019066  │                           0.606103 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 12 │             13 │                      0.615741 │                       0.587963 │                    0.0185185 │                           0.606481 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 13 │             14 │                      0.611111 │                       0.587037 │                    0.0171734 │                           0.60421  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 14 │             15 │                      0.611111 │                       0.587037 │                    0.0171734 │                           0.60421  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 15 │             16 │                      0.611111 │                       0.586111 │                    0.0172232 │                           0.603334 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 16 │             17 │                      0.611111 │                       0.586111 │                    0.0172232 │                           0.603334 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 17 │             18 │                      0.611111 │                       0.586111 │                    0.0172232 │                           0.603334 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 18 │             19 │                      0.611111 │                       0.587037 │                    0.0169219 │                           0.603959 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 19 │             20 │                      0.615741 │                       0.587963 │                    0.0182856 │                           0.606249 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 20 │             21 │                      0.615741 │                       0.587963 │                    0.0182856 │                           0.606249 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 21 │             22 │                      0.615741 │                       0.587037 │                    0.019729  │                           0.606766 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 22 │             23 │                      0.615741 │                       0.587037 │                    0.019729  │                           0.606766 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 23 │             24 │                      0.615741 │                       0.587037 │                    0.019729  │                           0.606766 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────────┤\n",
            "│ 24 │             25 │                      0.615741 │                       0.587963 │                    0.020286  │                           0.608249 │\n",
            "╘════╧════════════════╧═══════════════════════════════╧════════════════════════════════╧══════════════════════════════╧════════════════════════════════════╛\n",
            "╒════╤════════════════╤═════════════════════════╤══════════════════════════╤════════════════════════╤══════════════════════════════╕\n",
            "│    │   Top K Number │   KNN Max Test Accuracy │   Knn Mean Test Accuracy │   KNN SD Test Accuracy │   KNN Mean+-SD Test Accuracy │\n",
            "╞════╪════════════════╪═════════════════════════╪══════════════════════════╪════════════════════════╪══════════════════════════════╡\n",
            "│  0 │              1 │                0.487013 │                 0.444589 │              0.0227674 │                     0.467356 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  1 │              2 │                0.577922 │                 0.551082 │              0.0181921 │                     0.569274 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  2 │              3 │                0.603896 │                 0.575325 │              0.0222175 │                     0.597542 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  3 │              4 │                0.619048 │                 0.590476 │              0.0207792 │                     0.611255 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  4 │              5 │                0.614719 │                 0.590909 │              0.01737   │                     0.608279 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  5 │              6 │                0.614719 │                 0.590909 │              0.01737   │                     0.608279 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  6 │              7 │                0.614719 │                 0.590909 │              0.01737   │                     0.608279 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  7 │              8 │                0.614719 │                 0.590043 │              0.017131  │                     0.607174 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  8 │              9 │                0.614719 │                 0.590043 │              0.017131  │                     0.607174 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│  9 │             10 │                0.614719 │                 0.590043 │              0.017131  │                     0.607174 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 10 │             11 │                0.616883 │                 0.590476 │              0.0177647 │                     0.608241 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 11 │             12 │                0.616883 │                 0.590476 │              0.0177647 │                     0.608241 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 12 │             13 │                0.614719 │                 0.58961  │              0.0175632 │                     0.607174 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 13 │             14 │                0.616883 │                 0.58961  │              0.0186499 │                     0.60826  │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 14 │             15 │                0.616883 │                 0.589177 │              0.0190378 │                     0.608215 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 15 │             16 │                0.616883 │                 0.588745 │              0.0194565 │                     0.608201 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 16 │             17 │                0.616883 │                 0.58961  │              0.0188995 │                     0.60851  │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 17 │             18 │                0.614719 │                 0.590043 │              0.0177225 │                     0.607766 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 18 │             19 │                0.614719 │                 0.590043 │              0.0177225 │                     0.607766 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 19 │             20 │                0.614719 │                 0.590043 │              0.0177225 │                     0.607766 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 20 │             21 │                0.614719 │                 0.590043 │              0.0177225 │                     0.607766 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 21 │             22 │                0.614719 │                 0.590043 │              0.0172944 │                     0.607338 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 22 │             23 │                0.614719 │                 0.590043 │              0.0172944 │                     0.607338 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 23 │             24 │                0.614719 │                 0.58961  │              0.0169662 │                     0.606577 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────────┼────────────────────────┼──────────────────────────────┤\n",
            "│ 24 │             25 │                0.614719 │                 0.590043 │              0.0165184 │                     0.606562 │\n",
            "╘════╧════════════════╧═════════════════════════╧══════════════════════════╧════════════════════════╧══════════════════════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vaidation and Test Accuracey Results For Decision Tree Classifier:\")\n",
        "\n",
        "Vdt0=ValidationDTAcc[0:25]\n",
        "Vdt1=ValidationDTAcc[25:50]\n",
        "Vdt2=ValidationDTAcc[50:75]\n",
        "Vdt3=ValidationDTAcc[75:100]\n",
        "Vdt4=ValidationDTAcc[100:125]\n",
        "Tdt0=TestDTAcc[0:25]\n",
        "Tdt1=TestDTAcc[25:50]\n",
        "Tdt2=TestDTAcc[50:75]\n",
        "Tdt3=TestDTAcc[75:100]\n",
        "Tdt4=TestDTAcc[100:125]\n",
        "\n",
        "MET=[]\n",
        "MAT=[]\n",
        "SDT=[]\n",
        "VT=[]\n",
        "ET=[]\n",
        "MEV=[]\n",
        "MAV=[]\n",
        "SDV=[]\n",
        "VV=[]\n",
        "EV=[]\n",
        "A=[]\n",
        "B=[]\n",
        "# Original array\n",
        "for i in range(0,25):\n",
        "  a= np.array([Vdt0[i],Vdt1[i],Vdt2[i],Vdt3[i],Vdt4[i]])\n",
        "  b= np.array([Tdt0[i],Tdt1[i],Tdt2[i],Tdt3[i],Tdt4[i]])\n",
        "  A.append(a)\n",
        "  B.append(b)\n",
        "  print(A[i])\n",
        "  print(B[i])\n",
        "  mav= np.max(A[i])\n",
        "  MAV.append(mav)\n",
        "  mat= np.max(B[i])\n",
        "  MAT.append(mat)\n",
        "  print(\"\\nMax Validation Accuracy value of top k= \", i+1,\":\", mav)\n",
        "  print(\"\\nMax Test Accuracy value of top k= \", i+1,\":\", mat)\n",
        "\n",
        "  mev= np.mean(A[i])\n",
        "  MEV.append(mev)\n",
        "  print(\"\\nMean Validation Accuracy value of top k= \", i+1,\":\", mev)\n",
        "  met= np.mean(B[i])\n",
        "  MET.append(met)\n",
        "  print(\"\\nMean Test Accuracy value of top k= \", i+1,\":\", met)\n",
        "   \n",
        "  sdv= np.std(A[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Validation Accuracy value of top k= \", i+1,\":\", sdv)\n",
        "  SDV.append(sdv)\n",
        "\n",
        "\n",
        "  sdt= np.std(B[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Test Accuracy value of top k= \", i+1,\":\", sdt)\n",
        "  SDT.append(sdt)\n",
        "\n",
        "  ev=mev+sdv\n",
        "  print(\"\\n Mean + SD: of Validation Accuracy value of top k= \", i+1,\":\", mev, \"+-\", sdv)\n",
        "  print(ev)\n",
        "  EV.append(ev)\n",
        "\n",
        "  et=met+sdt\n",
        "  print(\"\\n Mean + SD: of Test Accuracy value of top k= \", i+1,\":\", met, \"+-\", sdt)\n",
        "  print(et)\n",
        "  ET.append(et)\n",
        "  \n",
        "  vv= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vv)\n",
        "  VV.append(vv)\n",
        "\n",
        "  vt= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vt)\n",
        "  VT.append(vt)\n",
        "\n",
        "X= np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
        "df=pd.DataFrame({'Top K Number': X+1, 'Decision Tree Max Validation Accuracy': MAV, 'Decision Tree Mean Validation Accuracy': MEV, 'Decision Tree SD Validation Accuracy': SDV, 'Decision Tree Mean+-SD Validation Accuracy': EV})\n",
        "print(tabulate(df,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df.to_excel('DecisionTreeValidationResults.xlsx')\n",
        "df1=pd.DataFrame({'Top K Number': X+1, 'Decision Tree Max Test Accuracy': MAT, 'Decision Tree Mean Test Accuracy': MET, 'Decision Tree SD Test Accuracy': SDT, ' Decision Tree Mean+-SD Test Accuracy': ET})\n",
        "print(tabulate(df1,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df1.to_excel('DecisionTreeTestResults.xlsx')"
      ],
      "metadata": {
        "id": "cR1tlhvDSpHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8681f2ea-828e-4dec-ff65-586310492126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vaidation and Test Accuracey Results For Decision Tree Classifier:\n",
            "[0.37037037 0.41203704 0.44907407 0.43055556 0.45833333]\n",
            "[0.43939394 0.39177489 0.38744589 0.4025974  0.39393939]\n",
            "\n",
            "Max Validation Accuracy value of top k=  1 : 0.4583333333333333\n",
            "\n",
            "Max Test Accuracy value of top k=  1 : 0.4393939393939394\n",
            "\n",
            "Mean Validation Accuracy value of top k=  1 : 0.42407407407407405\n",
            "\n",
            "Mean Test Accuracy value of top k=  1 : 0.403030303030303\n",
            "\n",
            "std of Validation Accuracy value of top k=  1 : 0.031207962122875407\n",
            "\n",
            "std of Test Accuracy value of top k=  1 : 0.018839874586137617\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  1 : 0.42407407407407405 +- 0.031207962122875407\n",
            "0.45528203619694946\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  1 : 0.403030303030303 +- 0.018839874586137617\n",
            "0.42187017761644063\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "[0.51851852 0.56018519 0.5        0.52314815 0.53240741]\n",
            "[0.53896104 0.53679654 0.55194805 0.51298701 0.51948052]\n",
            "\n",
            "Max Validation Accuracy value of top k=  2 : 0.5601851851851852\n",
            "\n",
            "Max Test Accuracy value of top k=  2 : 0.551948051948052\n",
            "\n",
            "Mean Validation Accuracy value of top k=  2 : 0.5268518518518519\n",
            "\n",
            "Mean Test Accuracy value of top k=  2 : 0.5320346320346321\n",
            "\n",
            "std of Validation Accuracy value of top k=  2 : 0.01972895903024309\n",
            "\n",
            "std of Test Accuracy value of top k=  2 : 0.014054271065582637\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  2 : 0.5268518518518519 +- 0.01972895903024309\n",
            "0.546580810882095\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  2 : 0.5320346320346321 +- 0.014054271065582637\n",
            "0.5460889031002147\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "[0.61574074 0.625      0.57407407 0.57407407 0.63888889]\n",
            "[0.62121212 0.5952381  0.63419913 0.62121212 0.60606061]\n",
            "\n",
            "Max Validation Accuracy value of top k=  3 : 0.6388888888888888\n",
            "\n",
            "Max Test Accuracy value of top k=  3 : 0.6341991341991342\n",
            "\n",
            "Mean Validation Accuracy value of top k=  3 : 0.6055555555555555\n",
            "\n",
            "Mean Test Accuracy value of top k=  3 : 0.6155844155844156\n",
            "\n",
            "std of Validation Accuracy value of top k=  3 : 0.026739868663321564\n",
            "\n",
            "std of Test Accuracy value of top k=  3 : 0.013524241863041827\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  3 : 0.6055555555555555 +- 0.026739868663321564\n",
            "0.6322954242188771\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  3 : 0.6155844155844156 +- 0.013524241863041827\n",
            "0.6291086574474574\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "[0.58796296 0.62037037 0.58796296 0.63425926 0.67592593]\n",
            "[0.65800866 0.63203463 0.63636364 0.62121212 0.61688312]\n",
            "\n",
            "Max Validation Accuracy value of top k=  4 : 0.6759259259259259\n",
            "\n",
            "Max Test Accuracy value of top k=  4 : 0.658008658008658\n",
            "\n",
            "Mean Validation Accuracy value of top k=  4 : 0.6212962962962962\n",
            "\n",
            "Mean Test Accuracy value of top k=  4 : 0.6329004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  4 : 0.03278876149930701\n",
            "\n",
            "std of Test Accuracy value of top k=  4 : 0.014396787141399041\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  4 : 0.6212962962962962 +- 0.03278876149930701\n",
            "0.6540850577956032\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  4 : 0.6329004329004329 +- 0.014396787141399041\n",
            "0.6472972200418319\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "[0.61111111 0.68981481 0.61111111 0.61111111 0.65277778]\n",
            "[0.65584416 0.62554113 0.67965368 0.61904762 0.61688312]\n",
            "\n",
            "Max Validation Accuracy value of top k=  5 : 0.6898148148148148\n",
            "\n",
            "Max Test Accuracy value of top k=  5 : 0.6796536796536796\n",
            "\n",
            "Mean Validation Accuracy value of top k=  5 : 0.6351851851851852\n",
            "\n",
            "Mean Test Accuracy value of top k=  5 : 0.6393939393939394\n",
            "\n",
            "std of Validation Accuracy value of top k=  5 : 0.03172563277594776\n",
            "\n",
            "std of Test Accuracy value of top k=  5 : 0.024511493795484087\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  5 : 0.6351851851851852 +- 0.03172563277594776\n",
            "0.6669108179611329\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  5 : 0.6393939393939394 +- 0.024511493795484087\n",
            "0.6639054331894235\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "[0.60648148 0.64351852 0.625      0.59259259 0.63425926]\n",
            "[0.65367965 0.6017316  0.66450216 0.64069264 0.6017316 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  6 : 0.6435185185185185\n",
            "\n",
            "Max Test Accuracy value of top k=  6 : 0.6645021645021645\n",
            "\n",
            "Mean Validation Accuracy value of top k=  6 : 0.6203703703703705\n",
            "\n",
            "Mean Test Accuracy value of top k=  6 : 0.6324675324675325\n",
            "\n",
            "std of Validation Accuracy value of top k=  6 : 0.018518518518518524\n",
            "\n",
            "std of Test Accuracy value of top k=  6 : 0.02620388909163902\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  6 : 0.6203703703703705 +- 0.018518518518518524\n",
            "0.638888888888889\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  6 : 0.6324675324675325 +- 0.02620388909163902\n",
            "0.6586714215591716\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "[0.60185185 0.63888889 0.61111111 0.61111111 0.63425926]\n",
            "[0.64718615 0.6038961  0.64502165 0.62554113 0.61471861]\n",
            "\n",
            "Max Validation Accuracy value of top k=  7 : 0.6388888888888888\n",
            "\n",
            "Max Test Accuracy value of top k=  7 : 0.6471861471861472\n",
            "\n",
            "Mean Validation Accuracy value of top k=  7 : 0.6194444444444445\n",
            "\n",
            "Mean Test Accuracy value of top k=  7 : 0.6272727272727272\n",
            "\n",
            "std of Validation Accuracy value of top k=  7 : 0.014463425325753046\n",
            "\n",
            "std of Test Accuracy value of top k=  7 : 0.016844222041856042\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  7 : 0.6194444444444445 +- 0.014463425325753046\n",
            "0.6339078697701975\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  7 : 0.6272727272727272 +- 0.016844222041856042\n",
            "0.6441169493145833\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "[0.60648148 0.65740741 0.61574074 0.60185185 0.63425926]\n",
            "[0.63852814 0.61904762 0.65800866 0.62987013 0.59307359]\n",
            "\n",
            "Max Validation Accuracy value of top k=  8 : 0.6574074074074074\n",
            "\n",
            "Max Test Accuracy value of top k=  8 : 0.658008658008658\n",
            "\n",
            "Mean Validation Accuracy value of top k=  8 : 0.6231481481481481\n",
            "\n",
            "Mean Test Accuracy value of top k=  8 : 0.6277056277056279\n",
            "\n",
            "std of Validation Accuracy value of top k=  8 : 0.020412414523193163\n",
            "\n",
            "std of Test Accuracy value of top k=  8 : 0.02151475954941849\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  8 : 0.6231481481481481 +- 0.020412414523193163\n",
            "0.6435605626713413\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  8 : 0.6277056277056279 +- 0.02151475954941849\n",
            "0.6492203872550464\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "[0.61574074 0.66203704 0.61111111 0.60648148 0.63888889]\n",
            "[0.63419913 0.5974026  0.65584416 0.64718615 0.59307359]\n",
            "\n",
            "Max Validation Accuracy value of top k=  9 : 0.6620370370370371\n",
            "\n",
            "Max Test Accuracy value of top k=  9 : 0.6558441558441559\n",
            "\n",
            "Mean Validation Accuracy value of top k=  9 : 0.6268518518518518\n",
            "\n",
            "Mean Test Accuracy value of top k=  9 : 0.6255411255411255\n",
            "\n",
            "std of Validation Accuracy value of top k=  9 : 0.020828188665188877\n",
            "\n",
            "std of Test Accuracy value of top k=  9 : 0.025720261086602525\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  9 : 0.6268518518518518 +- 0.020828188665188877\n",
            "0.6476800405170406\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  9 : 0.6255411255411255 +- 0.025720261086602525\n",
            "0.651261386627728\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "[0.61111111 0.67592593 0.61574074 0.59722222 0.64351852]\n",
            "[0.65367965 0.61688312 0.66450216 0.61904762 0.61471861]\n",
            "\n",
            "Max Validation Accuracy value of top k=  10 : 0.6759259259259259\n",
            "\n",
            "Max Test Accuracy value of top k=  10 : 0.6645021645021645\n",
            "\n",
            "Mean Validation Accuracy value of top k=  10 : 0.6287037037037038\n",
            "\n",
            "Mean Test Accuracy value of top k=  10 : 0.6337662337662338\n",
            "\n",
            "std of Validation Accuracy value of top k=  10 : 0.02799299344042773\n",
            "\n",
            "std of Test Accuracy value of top k=  10 : 0.021003479602957915\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  10 : 0.6287037037037038 +- 0.02799299344042773\n",
            "0.6566966971441315\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  10 : 0.6337662337662338 +- 0.021003479602957915\n",
            "0.6547697133691917\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "[0.59722222 0.63425926 0.62037037 0.60185185 0.63425926]\n",
            "[0.64718615 0.61471861 0.66666667 0.64285714 0.60822511]\n",
            "\n",
            "Max Validation Accuracy value of top k=  11 : 0.6342592592592593\n",
            "\n",
            "Max Test Accuracy value of top k=  11 : 0.6666666666666666\n",
            "\n",
            "Mean Validation Accuracy value of top k=  11 : 0.6175925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  11 : 0.6359307359307358\n",
            "\n",
            "std of Validation Accuracy value of top k=  11 : 0.015658828264155357\n",
            "\n",
            "std of Test Accuracy value of top k=  11 : 0.021619032015902644\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  11 : 0.6175925925925926 +- 0.015658828264155357\n",
            "0.6332514208567479\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  11 : 0.6359307359307358 +- 0.021619032015902644\n",
            "0.6575497679466384\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "[0.62962963 0.64351852 0.61574074 0.58333333 0.63888889]\n",
            "[0.64718615 0.60822511 0.65800866 0.62987013 0.60606061]\n",
            "\n",
            "Max Validation Accuracy value of top k=  12 : 0.6435185185185185\n",
            "\n",
            "Max Test Accuracy value of top k=  12 : 0.658008658008658\n",
            "\n",
            "Mean Validation Accuracy value of top k=  12 : 0.6222222222222222\n",
            "\n",
            "Mean Test Accuracy value of top k=  12 : 0.6298701298701299\n",
            "\n",
            "std of Validation Accuracy value of top k=  12 : 0.02163578045471835\n",
            "\n",
            "std of Test Accuracy value of top k=  12 : 0.020625332033890217\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  12 : 0.6222222222222222 +- 0.02163578045471835\n",
            "0.6438580026769406\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  12 : 0.6298701298701299 +- 0.020625332033890217\n",
            "0.6504954619040201\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "[0.61111111 0.65277778 0.61574074 0.58796296 0.64351852]\n",
            "[0.64935065 0.58874459 0.65151515 0.62554113 0.61255411]\n",
            "\n",
            "Max Validation Accuracy value of top k=  13 : 0.6527777777777778\n",
            "\n",
            "Max Test Accuracy value of top k=  13 : 0.6515151515151515\n",
            "\n",
            "Mean Validation Accuracy value of top k=  13 : 0.6222222222222222\n",
            "\n",
            "Mean Test Accuracy value of top k=  13 : 0.6255411255411255\n",
            "\n",
            "std of Validation Accuracy value of top k=  13 : 0.023350963357256477\n",
            "\n",
            "std of Test Accuracy value of top k=  13 : 0.023512511885714734\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  13 : 0.6222222222222222 +- 0.023350963357256477\n",
            "0.6455731855794787\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  13 : 0.6255411255411255 +- 0.023512511885714734\n",
            "0.6490536374268402\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "[0.59722222 0.66203704 0.60648148 0.60185185 0.63425926]\n",
            "[0.65367965 0.6038961  0.64285714 0.64285714 0.62987013]\n",
            "\n",
            "Max Validation Accuracy value of top k=  14 : 0.6620370370370371\n",
            "\n",
            "Max Test Accuracy value of top k=  14 : 0.6536796536796536\n",
            "\n",
            "Mean Validation Accuracy value of top k=  14 : 0.6203703703703705\n",
            "\n",
            "Mean Test Accuracy value of top k=  14 : 0.6346320346320347\n",
            "\n",
            "std of Validation Accuracy value of top k=  14 : 0.024497697324672156\n",
            "\n",
            "std of Test Accuracy value of top k=  14 : 0.01712010383834214\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  14 : 0.6203703703703705 +- 0.024497697324672156\n",
            "0.6448680676950426\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  14 : 0.6346320346320347 +- 0.01712010383834214\n",
            "0.6517521384703768\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "[0.625      0.64351852 0.61574074 0.58796296 0.63888889]\n",
            "[0.65151515 0.61255411 0.66017316 0.62987013 0.62337662]\n",
            "\n",
            "Max Validation Accuracy value of top k=  15 : 0.6435185185185185\n",
            "\n",
            "Max Test Accuracy value of top k=  15 : 0.6601731601731602\n",
            "\n",
            "Mean Validation Accuracy value of top k=  15 : 0.6222222222222222\n",
            "\n",
            "Mean Test Accuracy value of top k=  15 : 0.6354978354978356\n",
            "\n",
            "std of Validation Accuracy value of top k=  15 : 0.019772367133391325\n",
            "\n",
            "std of Test Accuracy value of top k=  15 : 0.017722501723340892\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  15 : 0.6222222222222222 +- 0.019772367133391325\n",
            "0.6419945893556136\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  15 : 0.6354978354978356 +- 0.017722501723340892\n",
            "0.6532203372211765\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "[0.60648148 0.65740741 0.60648148 0.5787037  0.64351852]\n",
            "[0.64935065 0.61471861 0.65800866 0.63419913 0.60822511]\n",
            "\n",
            "Max Validation Accuracy value of top k=  16 : 0.6574074074074074\n",
            "\n",
            "Max Test Accuracy value of top k=  16 : 0.658008658008658\n",
            "\n",
            "Mean Validation Accuracy value of top k=  16 : 0.6185185185185185\n",
            "\n",
            "Mean Test Accuracy value of top k=  16 : 0.6329004329004329\n",
            "\n",
            "std of Validation Accuracy value of top k=  16 : 0.02832788618662658\n",
            "\n",
            "std of Test Accuracy value of top k=  16 : 0.019194631409000334\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  16 : 0.6185185185185185 +- 0.02832788618662658\n",
            "0.6468464047051451\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  16 : 0.6329004329004329 +- 0.019194631409000334\n",
            "0.6520950643094332\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "[0.625      0.65740741 0.60185185 0.61111111 0.65740741]\n",
            "[0.66017316 0.61688312 0.66017316 0.64502165 0.61688312]\n",
            "\n",
            "Max Validation Accuracy value of top k=  17 : 0.6574074074074074\n",
            "\n",
            "Max Test Accuracy value of top k=  17 : 0.6601731601731602\n",
            "\n",
            "Mean Validation Accuracy value of top k=  17 : 0.6305555555555555\n",
            "\n",
            "Mean Test Accuracy value of top k=  17 : 0.6398268398268399\n",
            "\n",
            "std of Validation Accuracy value of top k=  17 : 0.023129622216290367\n",
            "\n",
            "std of Test Accuracy value of top k=  17 : 0.019533357874767916\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  17 : 0.6305555555555555 +- 0.023129622216290367\n",
            "0.6536851777718459\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  17 : 0.6398268398268399 +- 0.019533357874767916\n",
            "0.6593601977016078\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "[0.625      0.64351852 0.60648148 0.58796296 0.66203704]\n",
            "[0.65367965 0.62337662 0.65367965 0.62987013 0.61255411]\n",
            "\n",
            "Max Validation Accuracy value of top k=  18 : 0.6620370370370371\n",
            "\n",
            "Max Test Accuracy value of top k=  18 : 0.6536796536796536\n",
            "\n",
            "Mean Validation Accuracy value of top k=  18 : 0.625\n",
            "\n",
            "Mean Test Accuracy value of top k=  18 : 0.6346320346320347\n",
            "\n",
            "std of Validation Accuracy value of top k=  18 : 0.026189140043946225\n",
            "\n",
            "std of Test Accuracy value of top k=  18 : 0.016507078757521738\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  18 : 0.625 +- 0.026189140043946225\n",
            "0.6511891400439462\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  18 : 0.6346320346320347 +- 0.016507078757521738\n",
            "0.6511391133895564\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "[0.61574074 0.66203704 0.62962963 0.58333333 0.63425926]\n",
            "[0.65800866 0.61471861 0.66017316 0.63419913 0.6017316 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  19 : 0.6620370370370371\n",
            "\n",
            "Max Test Accuracy value of top k=  19 : 0.6601731601731602\n",
            "\n",
            "Mean Validation Accuracy value of top k=  19 : 0.625\n",
            "\n",
            "Mean Test Accuracy value of top k=  19 : 0.6337662337662338\n",
            "\n",
            "std of Validation Accuracy value of top k=  19 : 0.025693401713910394\n",
            "\n",
            "std of Test Accuracy value of top k=  19 : 0.023126771236697978\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  19 : 0.625 +- 0.025693401713910394\n",
            "0.6506934017139104\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  19 : 0.6337662337662338 +- 0.023126771236697978\n",
            "0.6568930050029317\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "[0.61574074 0.64351852 0.60648148 0.60648148 0.625     ]\n",
            "[0.64285714 0.61038961 0.65800866 0.63636364 0.60822511]\n",
            "\n",
            "Max Validation Accuracy value of top k=  20 : 0.6435185185185185\n",
            "\n",
            "Max Test Accuracy value of top k=  20 : 0.658008658008658\n",
            "\n",
            "Mean Validation Accuracy value of top k=  20 : 0.6194444444444444\n",
            "\n",
            "Mean Test Accuracy value of top k=  20 : 0.631168831168831\n",
            "\n",
            "std of Validation Accuracy value of top k=  20 : 0.013857990321384949\n",
            "\n",
            "std of Test Accuracy value of top k=  20 : 0.019194631409000334\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  20 : 0.6194444444444444 +- 0.013857990321384949\n",
            "0.6333024347658293\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  20 : 0.631168831168831 +- 0.019194631409000334\n",
            "0.6503634625778314\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "[0.61574074 0.65277778 0.62037037 0.61111111 0.63425926]\n",
            "[0.65367965 0.61038961 0.66883117 0.63852814 0.61255411]\n",
            "\n",
            "Max Validation Accuracy value of top k=  21 : 0.6527777777777778\n",
            "\n",
            "Max Test Accuracy value of top k=  21 : 0.6688311688311688\n",
            "\n",
            "Mean Validation Accuracy value of top k=  21 : 0.6268518518518519\n",
            "\n",
            "Mean Test Accuracy value of top k=  21 : 0.6367965367965367\n",
            "\n",
            "std of Validation Accuracy value of top k=  21 : 0.01510139484287046\n",
            "\n",
            "std of Test Accuracy value of top k=  21 : 0.022800336073183873\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  21 : 0.6268518518518519 +- 0.01510139484287046\n",
            "0.6419532466947223\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  21 : 0.6367965367965367 +- 0.022800336073183873\n",
            "0.6595968728697206\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "[0.62037037 0.65277778 0.61574074 0.60648148 0.62962963]\n",
            "[0.64718615 0.61038961 0.64718615 0.62337662 0.6038961 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  22 : 0.6527777777777778\n",
            "\n",
            "Max Test Accuracy value of top k=  22 : 0.6471861471861472\n",
            "\n",
            "Mean Validation Accuracy value of top k=  22 : 0.625\n",
            "\n",
            "Mean Test Accuracy value of top k=  22 : 0.6264069264069263\n",
            "\n",
            "std of Validation Accuracy value of top k=  22 : 0.01576795033882075\n",
            "\n",
            "std of Test Accuracy value of top k=  22 : 0.018088815944394963\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  22 : 0.625 +- 0.01576795033882075\n",
            "0.6407679503388207\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  22 : 0.6264069264069263 +- 0.018088815944394963\n",
            "0.6444957423513212\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "[0.60648148 0.62962963 0.60648148 0.60648148 0.625     ]\n",
            "[0.62770563 0.6017316  0.65151515 0.61904762 0.5995671 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  23 : 0.6296296296296297\n",
            "\n",
            "Max Test Accuracy value of top k=  23 : 0.6515151515151515\n",
            "\n",
            "Mean Validation Accuracy value of top k=  23 : 0.6148148148148148\n",
            "\n",
            "Mean Test Accuracy value of top k=  23 : 0.6199134199134199\n",
            "\n",
            "std of Validation Accuracy value of top k=  23 : 0.010310674745981515\n",
            "\n",
            "std of Test Accuracy value of top k=  23 : 0.01899836212769842\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  23 : 0.6148148148148148 +- 0.010310674745981515\n",
            "0.6251254895607963\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  23 : 0.6199134199134199 +- 0.01899836212769842\n",
            "0.6389117820411183\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "[0.60648148 0.65277778 0.62037037 0.60648148 0.65277778]\n",
            "[0.63636364 0.60606061 0.65800866 0.63419913 0.62337662]\n",
            "\n",
            "Max Validation Accuracy value of top k=  24 : 0.6527777777777778\n",
            "\n",
            "Max Test Accuracy value of top k=  24 : 0.658008658008658\n",
            "\n",
            "Mean Validation Accuracy value of top k=  24 : 0.6277777777777778\n",
            "\n",
            "Mean Test Accuracy value of top k=  24 : 0.6316017316017316\n",
            "\n",
            "std of Validation Accuracy value of top k=  24 : 0.021032993873334342\n",
            "\n",
            "std of Test Accuracy value of top k=  24 : 0.017010288055747627\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  24 : 0.6277777777777778 +- 0.021032993873334342\n",
            "0.6488107716511121\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  24 : 0.6316017316017316 +- 0.017010288055747627\n",
            "0.6486120196574793\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "[0.61574074 0.63888889 0.61111111 0.59259259 0.63425926]\n",
            "[0.65367965 0.61038961 0.65800866 0.64718615 0.61904762]\n",
            "\n",
            "Max Validation Accuracy value of top k=  25 : 0.6388888888888888\n",
            "\n",
            "Max Test Accuracy value of top k=  25 : 0.658008658008658\n",
            "\n",
            "Mean Validation Accuracy value of top k=  25 : 0.6185185185185185\n",
            "\n",
            "Mean Test Accuracy value of top k=  25 : 0.6376623376623376\n",
            "\n",
            "std of Validation Accuracy value of top k=  25 : 0.016718027856729434\n",
            "\n",
            "std of Test Accuracy value of top k=  25 : 0.019243385948824996\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  25 : 0.6185185185185185 +- 0.016718027856729434\n",
            "0.6352365463752478\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  25 : 0.6376623376623376 +- 0.019243385948824996\n",
            "0.6569057236111626\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "╒════╤════════════════╤═════════════════════════════════════════╤══════════════════════════════════════════╤════════════════════════════════════════╤══════════════════════════════════════════════╕\n",
            "│    │   Top K Number │   Decision Tree Max Validation Accuracy │   Decision Tree Mean Validation Accuracy │   Decision Tree SD Validation Accuracy │   Decision Tree Mean+-SD Validation Accuracy │\n",
            "╞════╪════════════════╪═════════════════════════════════════════╪══════════════════════════════════════════╪════════════════════════════════════════╪══════════════════════════════════════════════╡\n",
            "│  0 │              1 │                                0.458333 │                                 0.424074 │                              0.031208  │                                     0.455282 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  1 │              2 │                                0.560185 │                                 0.526852 │                              0.019729  │                                     0.546581 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  2 │              3 │                                0.638889 │                                 0.605556 │                              0.0267399 │                                     0.632295 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  3 │              4 │                                0.675926 │                                 0.621296 │                              0.0327888 │                                     0.654085 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  4 │              5 │                                0.689815 │                                 0.635185 │                              0.0317256 │                                     0.666911 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  5 │              6 │                                0.643519 │                                 0.62037  │                              0.0185185 │                                     0.638889 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  6 │              7 │                                0.638889 │                                 0.619444 │                              0.0144634 │                                     0.633908 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  7 │              8 │                                0.657407 │                                 0.623148 │                              0.0204124 │                                     0.643561 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  8 │              9 │                                0.662037 │                                 0.626852 │                              0.0208282 │                                     0.64768  │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  9 │             10 │                                0.675926 │                                 0.628704 │                              0.027993  │                                     0.656697 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 10 │             11 │                                0.634259 │                                 0.617593 │                              0.0156588 │                                     0.633251 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 11 │             12 │                                0.643519 │                                 0.622222 │                              0.0216358 │                                     0.643858 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 12 │             13 │                                0.652778 │                                 0.622222 │                              0.023351  │                                     0.645573 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 13 │             14 │                                0.662037 │                                 0.62037  │                              0.0244977 │                                     0.644868 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 14 │             15 │                                0.643519 │                                 0.622222 │                              0.0197724 │                                     0.641995 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 15 │             16 │                                0.657407 │                                 0.618519 │                              0.0283279 │                                     0.646846 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 16 │             17 │                                0.657407 │                                 0.630556 │                              0.0231296 │                                     0.653685 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 17 │             18 │                                0.662037 │                                 0.625    │                              0.0261891 │                                     0.651189 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 18 │             19 │                                0.662037 │                                 0.625    │                              0.0256934 │                                     0.650693 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 19 │             20 │                                0.643519 │                                 0.619444 │                              0.013858  │                                     0.633302 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 20 │             21 │                                0.652778 │                                 0.626852 │                              0.0151014 │                                     0.641953 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 21 │             22 │                                0.652778 │                                 0.625    │                              0.015768  │                                     0.640768 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 22 │             23 │                                0.62963  │                                 0.614815 │                              0.0103107 │                                     0.625125 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 23 │             24 │                                0.652778 │                                 0.627778 │                              0.021033  │                                     0.648811 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 24 │             25 │                                0.638889 │                                 0.618519 │                              0.016718  │                                     0.635237 │\n",
            "╘════╧════════════════╧═════════════════════════════════════════╧══════════════════════════════════════════╧════════════════════════════════════════╧══════════════════════════════════════════════╛\n",
            "╒════╤════════════════╤═══════════════════════════════════╤════════════════════════════════════╤══════════════════════════════════╤═════════════════════════════════════════╕\n",
            "│    │   Top K Number │   Decision Tree Max Test Accuracy │   Decision Tree Mean Test Accuracy │   Decision Tree SD Test Accuracy │    Decision Tree Mean+-SD Test Accuracy │\n",
            "╞════╪════════════════╪═══════════════════════════════════╪════════════════════════════════════╪══════════════════════════════════╪═════════════════════════════════════════╡\n",
            "│  0 │              1 │                          0.439394 │                           0.40303  │                        0.0188399 │                                0.42187  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  1 │              2 │                          0.551948 │                           0.532035 │                        0.0140543 │                                0.546089 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  2 │              3 │                          0.634199 │                           0.615584 │                        0.0135242 │                                0.629109 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  3 │              4 │                          0.658009 │                           0.6329   │                        0.0143968 │                                0.647297 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  4 │              5 │                          0.679654 │                           0.639394 │                        0.0245115 │                                0.663905 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  5 │              6 │                          0.664502 │                           0.632468 │                        0.0262039 │                                0.658671 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  6 │              7 │                          0.647186 │                           0.627273 │                        0.0168442 │                                0.644117 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  7 │              8 │                          0.658009 │                           0.627706 │                        0.0215148 │                                0.64922  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  8 │              9 │                          0.655844 │                           0.625541 │                        0.0257203 │                                0.651261 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│  9 │             10 │                          0.664502 │                           0.633766 │                        0.0210035 │                                0.65477  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 10 │             11 │                          0.666667 │                           0.635931 │                        0.021619  │                                0.65755  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 11 │             12 │                          0.658009 │                           0.62987  │                        0.0206253 │                                0.650495 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 12 │             13 │                          0.651515 │                           0.625541 │                        0.0235125 │                                0.649054 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 13 │             14 │                          0.65368  │                           0.634632 │                        0.0171201 │                                0.651752 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 14 │             15 │                          0.660173 │                           0.635498 │                        0.0177225 │                                0.65322  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 15 │             16 │                          0.658009 │                           0.6329   │                        0.0191946 │                                0.652095 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 16 │             17 │                          0.660173 │                           0.639827 │                        0.0195334 │                                0.65936  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 17 │             18 │                          0.65368  │                           0.634632 │                        0.0165071 │                                0.651139 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 18 │             19 │                          0.660173 │                           0.633766 │                        0.0231268 │                                0.656893 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 19 │             20 │                          0.658009 │                           0.631169 │                        0.0191946 │                                0.650363 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 20 │             21 │                          0.668831 │                           0.636797 │                        0.0228003 │                                0.659597 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 21 │             22 │                          0.647186 │                           0.626407 │                        0.0180888 │                                0.644496 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 22 │             23 │                          0.651515 │                           0.619913 │                        0.0189984 │                                0.638912 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 23 │             24 │                          0.658009 │                           0.631602 │                        0.0170103 │                                0.648612 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼─────────────────────────────────────────┤\n",
            "│ 24 │             25 │                          0.658009 │                           0.637662 │                        0.0192434 │                                0.656906 │\n",
            "╘════╧════════════════╧═══════════════════════════════════╧════════════════════════════════════╧══════════════════════════════════╧═════════════════════════════════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vaidation and Test Accuracey Results For Random Forest Classifier:\")\n",
        "\n",
        "Vrf0=ValidationRFAcc[0:25]\n",
        "Vrf1=ValidationRFAcc[25:50]\n",
        "Vrf2=ValidationRFAcc[50:75]\n",
        "Vrf3=ValidationRFAcc[75:100]\n",
        "Vrf4=ValidationRFAcc[100:125]\n",
        "Trf0=TestRFAcc[0:25]\n",
        "Trf1=TestRFAcc[25:50]\n",
        "Trf2=TestRFAcc[50:75]\n",
        "Trf3=TestRFAcc[75:100]\n",
        "Trf4=TestRFAcc[100:125]\n",
        "\n",
        "MET=[]\n",
        "MAT=[]\n",
        "SDT=[]\n",
        "VT=[]\n",
        "ET=[]\n",
        "MEV=[]\n",
        "MAV=[]\n",
        "SDV=[]\n",
        "VV=[]\n",
        "EV=[]\n",
        "A=[]\n",
        "B=[]\n",
        "# Original array\n",
        "for i in range(0,25):\n",
        "  a= np.array([Vrf0[i],Vrf1[i],Vrf2[i],Vrf3[i],Vrf4[i]])\n",
        "  b= np.array([Trf0[i],Trf1[i],Trf2[i],Trf3[i],Trf4[i]])\n",
        "  A.append(a)\n",
        "  B.append(b)\n",
        "  print(A[i])\n",
        "  print(B[i])\n",
        "  mav= np.max(A[i])\n",
        "  MAV.append(mav)\n",
        "  mat= np.max(B[i])\n",
        "  MAT.append(mat)\n",
        "  print(\"\\nMax Validation Accuracy value of top k= \", i+1,\":\", mav)\n",
        "  print(\"\\nMax Test Accuracy value of top k= \", i+1,\":\", mat)\n",
        "\n",
        "  mev= np.mean(A[i])\n",
        "  MEV.append(mev)\n",
        "  print(\"\\nMean Validation Accuracy value of top k= \", i+1,\":\", mev)\n",
        "  met= np.mean(B[i])\n",
        "  MET.append(met)\n",
        "  print(\"\\nMean Test Accuracy value of top k= \", i+1,\":\", met)\n",
        "   \n",
        "  sdv= np.std(A[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Validation Accuracy value of top k= \", i+1,\":\", sdv)\n",
        "  SDV.append(sdv)\n",
        "\n",
        "\n",
        "  sdt= np.std(B[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Test Accuracy value of top k= \", i+1,\":\", sdt)\n",
        "  SDT.append(sdt)\n",
        "\n",
        "  ev=mev+sdv\n",
        "  print(\"\\n Mean + SD: of Validation Accuracy value of top k= \", i+1,\":\", mev, \"+-\", sdv)\n",
        "  print(ev)\n",
        "  EV.append(ev)\n",
        "\n",
        "  et=met+sdt\n",
        "  print(\"\\n Mean + SD: of Test Accuracy value of top k= \", i+1,\":\", met, \"+-\", sdt)\n",
        "  print(et)\n",
        "  ET.append(et)\n",
        "  \n",
        "  vv= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vv)\n",
        "  VV.append(vv)\n",
        "\n",
        "  vt= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vt)\n",
        "  VT.append(vt)\n",
        "\n",
        "X= np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
        "df=pd.DataFrame({'Top K Number': X+1, 'Random Forest Max Validation Accuracy': MAV, 'Random Forest Mean Validation Accuracy': MEV, 'Random Forest SD Validation Accuracy': SDV, 'Random Forest Mean+-SD Validation Accuracy': EV})\n",
        "print(tabulate(df,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df.to_excel('RandomForestValidationResults.xlsx')\n",
        "df1=pd.DataFrame({'Top K Number': X+1, 'Random Forest Max Test Accuracy': MAT, 'Random Forest Mean Test Accuracy': MET, 'Random Forest SD Test Accuracy': SDT, 'Random Forest Mean+-SD Test Accuracy': ET})\n",
        "print(tabulate(df1,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df1.to_excel('RandomForestTestResults.xlsx')"
      ],
      "metadata": {
        "id": "w2PpGPKgUAi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893549b4-02d0-4cc3-ec6f-a84ba10ab021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vaidation and Test Accuracey Results For Random Forest Classifier:\n",
            "[0.37037037 0.41203704 0.44907407 0.43055556 0.45833333]\n",
            "[0.43939394 0.39177489 0.38744589 0.4025974  0.39393939]\n",
            "\n",
            "Max Validation Accuracy value of top k=  1 : 0.4583333333333333\n",
            "\n",
            "Max Test Accuracy value of top k=  1 : 0.4393939393939394\n",
            "\n",
            "Mean Validation Accuracy value of top k=  1 : 0.42407407407407405\n",
            "\n",
            "Mean Test Accuracy value of top k=  1 : 0.403030303030303\n",
            "\n",
            "std of Validation Accuracy value of top k=  1 : 0.031207962122875407\n",
            "\n",
            "std of Test Accuracy value of top k=  1 : 0.018839874586137617\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  1 : 0.42407407407407405 +- 0.031207962122875407\n",
            "0.45528203619694946\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  1 : 0.403030303030303 +- 0.018839874586137617\n",
            "0.42187017761644063\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "[0.5462963  0.625      0.5787037  0.5462963  0.58796296]\n",
            "[0.60606061 0.56709957 0.59307359 0.57575758 0.56060606]\n",
            "\n",
            "Max Validation Accuracy value of top k=  2 : 0.625\n",
            "\n",
            "Max Test Accuracy value of top k=  2 : 0.6060606060606061\n",
            "\n",
            "Mean Validation Accuracy value of top k=  2 : 0.5768518518518518\n",
            "\n",
            "Mean Test Accuracy value of top k=  2 : 0.5805194805194805\n",
            "\n",
            "std of Validation Accuracy value of top k=  2 : 0.02936805838361798\n",
            "\n",
            "std of Test Accuracy value of top k=  2 : 0.01678850167070592\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  2 : 0.5768518518518518 +- 0.02936805838361798\n",
            "0.6062199102354698\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  2 : 0.5805194805194805 +- 0.01678850167070592\n",
            "0.5973079821901864\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "[0.65740741 0.71296296 0.62962963 0.6712963  0.74537037]\n",
            "[0.68181818 0.66450216 0.70562771 0.66666667 0.64935065]\n",
            "\n",
            "Max Validation Accuracy value of top k=  3 : 0.7453703703703703\n",
            "\n",
            "Max Test Accuracy value of top k=  3 : 0.7056277056277056\n",
            "\n",
            "Mean Validation Accuracy value of top k=  3 : 0.6833333333333333\n",
            "\n",
            "Mean Test Accuracy value of top k=  3 : 0.6735930735930735\n",
            "\n",
            "std of Validation Accuracy value of top k=  3 : 0.04105518384702843\n",
            "\n",
            "std of Test Accuracy value of top k=  3 : 0.019037777859130748\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  3 : 0.6833333333333333 +- 0.04105518384702843\n",
            "0.7243885171803618\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  3 : 0.6735930735930735 +- 0.019037777859130748\n",
            "0.6926308514522043\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "[0.66203704 0.74074074 0.66203704 0.7037037  0.73148148]\n",
            "[0.70562771 0.68831169 0.74242424 0.6991342  0.65800866]\n",
            "\n",
            "Max Validation Accuracy value of top k=  4 : 0.7407407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  4 : 0.7424242424242424\n",
            "\n",
            "Mean Validation Accuracy value of top k=  4 : 0.7\n",
            "\n",
            "Mean Test Accuracy value of top k=  4 : 0.6987012987012988\n",
            "\n",
            "std of Validation Accuracy value of top k=  4 : 0.033307603238157284\n",
            "\n",
            "std of Test Accuracy value of top k=  4 : 0.027289900454501262\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  4 : 0.7 +- 0.033307603238157284\n",
            "0.7333076032381572\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  4 : 0.6987012987012988 +- 0.027289900454501262\n",
            "0.7259911991558001\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "[0.66666667 0.74074074 0.64351852 0.68055556 0.73148148]\n",
            "[0.69047619 0.68398268 0.71428571 0.67532468 0.68181818]\n",
            "\n",
            "Max Validation Accuracy value of top k=  5 : 0.7407407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  5 : 0.7142857142857143\n",
            "\n",
            "Mean Validation Accuracy value of top k=  5 : 0.6925925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  5 : 0.6891774891774892\n",
            "\n",
            "std of Validation Accuracy value of top k=  5 : 0.03756567180574222\n",
            "\n",
            "std of Test Accuracy value of top k=  5 : 0.013454779452744642\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  5 : 0.6925925925925926 +- 0.03756567180574222\n",
            "0.7301582643983349\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  5 : 0.6891774891774892 +- 0.013454779452744642\n",
            "0.7026322686302339\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "[0.66666667 0.73148148 0.64814815 0.6712963  0.74537037]\n",
            "[0.69480519 0.67532468 0.71861472 0.67099567 0.67099567]\n",
            "\n",
            "Max Validation Accuracy value of top k=  6 : 0.7453703703703703\n",
            "\n",
            "Max Test Accuracy value of top k=  6 : 0.7186147186147186\n",
            "\n",
            "Mean Validation Accuracy value of top k=  6 : 0.6925925925925925\n",
            "\n",
            "Mean Test Accuracy value of top k=  6 : 0.6861471861471862\n",
            "\n",
            "std of Validation Accuracy value of top k=  6 : 0.038467737181187026\n",
            "\n",
            "std of Test Accuracy value of top k=  6 : 0.018468163643031613\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  6 : 0.6925925925925925 +- 0.038467737181187026\n",
            "0.7310603297737795\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  6 : 0.6861471861471862 +- 0.018468163643031613\n",
            "0.7046153497902178\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "[0.63425926 0.72222222 0.68518519 0.68981481 0.75      ]\n",
            "[0.7034632  0.67748918 0.71212121 0.67316017 0.64502165]\n",
            "\n",
            "Max Validation Accuracy value of top k=  7 : 0.75\n",
            "\n",
            "Max Test Accuracy value of top k=  7 : 0.7121212121212122\n",
            "\n",
            "Mean Validation Accuracy value of top k=  7 : 0.6962962962962963\n",
            "\n",
            "Mean Test Accuracy value of top k=  7 : 0.6822510822510823\n",
            "\n",
            "std of Validation Accuracy value of top k=  7 : 0.038910928498982755\n",
            "\n",
            "std of Test Accuracy value of top k=  7 : 0.02380558802574462\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  7 : 0.6962962962962963 +- 0.038910928498982755\n",
            "0.735207224795279\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  7 : 0.6822510822510823 +- 0.02380558802574462\n",
            "0.706056670276827\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "[0.63888889 0.70833333 0.69444444 0.68518519 0.71759259]\n",
            "[0.69047619 0.67965368 0.70562771 0.67099567 0.64935065]\n",
            "\n",
            "Max Validation Accuracy value of top k=  8 : 0.7175925925925926\n",
            "\n",
            "Max Test Accuracy value of top k=  8 : 0.7056277056277056\n",
            "\n",
            "Mean Validation Accuracy value of top k=  8 : 0.6888888888888889\n",
            "\n",
            "Mean Test Accuracy value of top k=  8 : 0.6792207792207792\n",
            "\n",
            "std of Validation Accuracy value of top k=  8 : 0.027373602776100906\n",
            "\n",
            "std of Test Accuracy value of top k=  8 : 0.01888954478724798\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  8 : 0.6888888888888889 +- 0.027373602776100906\n",
            "0.7162624916649898\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  8 : 0.6792207792207792 +- 0.01888954478724798\n",
            "0.6981103240080272\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "[0.62962963 0.71759259 0.6712963  0.68518519 0.74074074]\n",
            "[0.6969697  0.67099567 0.72294372 0.69264069 0.66017316]\n",
            "\n",
            "Max Validation Accuracy value of top k=  9 : 0.7407407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  9 : 0.7229437229437229\n",
            "\n",
            "Mean Validation Accuracy value of top k=  9 : 0.6888888888888889\n",
            "\n",
            "Mean Test Accuracy value of top k=  9 : 0.6887445887445887\n",
            "\n",
            "std of Validation Accuracy value of top k=  9 : 0.03833378063793414\n",
            "\n",
            "std of Test Accuracy value of top k=  9 : 0.02183466703535667\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  9 : 0.6888888888888889 +- 0.03833378063793414\n",
            "0.727222669526823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  9 : 0.6887445887445887 +- 0.02183466703535667\n",
            "0.7105792557799453\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "[0.65277778 0.69444444 0.66203704 0.66666667 0.73611111]\n",
            "[0.6969697  0.66666667 0.70995671 0.67748918 0.64718615]\n",
            "\n",
            "Max Validation Accuracy value of top k=  10 : 0.7361111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  10 : 0.70995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  10 : 0.6824074074074075\n",
            "\n",
            "Mean Test Accuracy value of top k=  10 : 0.6796536796536797\n",
            "\n",
            "std of Validation Accuracy value of top k=  10 : 0.03023116243947132\n",
            "\n",
            "std of Test Accuracy value of top k=  10 : 0.02211608618951483\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  10 : 0.6824074074074075 +- 0.03023116243947132\n",
            "0.7126385698468788\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  10 : 0.6796536796536797 +- 0.02211608618951483\n",
            "0.7017697658431946\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "[0.625      0.68518519 0.65740741 0.6712963  0.72685185]\n",
            "[0.68831169 0.65800866 0.70779221 0.68614719 0.64502165]\n",
            "\n",
            "Max Validation Accuracy value of top k=  11 : 0.7268518518518519\n",
            "\n",
            "Max Test Accuracy value of top k=  11 : 0.7077922077922078\n",
            "\n",
            "Mean Validation Accuracy value of top k=  11 : 0.673148148148148\n",
            "\n",
            "Mean Test Accuracy value of top k=  11 : 0.6770562770562771\n",
            "\n",
            "std of Validation Accuracy value of top k=  11 : 0.03346168703628269\n",
            "\n",
            "std of Test Accuracy value of top k=  11 : 0.02255240913875514\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  11 : 0.673148148148148 +- 0.03346168703628269\n",
            "0.7066098351844308\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  11 : 0.6770562770562771 +- 0.02255240913875514\n",
            "0.6996086861950322\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "[0.62962963 0.69444444 0.66203704 0.64814815 0.74537037]\n",
            "[0.6991342  0.67099567 0.69264069 0.65584416 0.66017316]\n",
            "\n",
            "Max Validation Accuracy value of top k=  12 : 0.7453703703703703\n",
            "\n",
            "Max Test Accuracy value of top k=  12 : 0.6991341991341992\n",
            "\n",
            "Mean Validation Accuracy value of top k=  12 : 0.6759259259259259\n",
            "\n",
            "Mean Test Accuracy value of top k=  12 : 0.6757575757575758\n",
            "\n",
            "std of Validation Accuracy value of top k=  12 : 0.0406775604379422\n",
            "\n",
            "std of Test Accuracy value of top k=  12 : 0.01728351928803213\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  12 : 0.6759259259259259 +- 0.0406775604379422\n",
            "0.7166034863638682\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  12 : 0.6757575757575758 +- 0.01728351928803213\n",
            "0.693041095045608\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "[0.61111111 0.66666667 0.66203704 0.66203704 0.74074074]\n",
            "[0.68614719 0.66017316 0.69047619 0.68398268 0.64285714]\n",
            "\n",
            "Max Validation Accuracy value of top k=  13 : 0.7407407407407407\n",
            "\n",
            "Max Test Accuracy value of top k=  13 : 0.6904761904761905\n",
            "\n",
            "Mean Validation Accuracy value of top k=  13 : 0.6685185185185185\n",
            "\n",
            "Mean Test Accuracy value of top k=  13 : 0.6727272727272727\n",
            "\n",
            "std of Validation Accuracy value of top k=  13 : 0.04147073273436792\n",
            "\n",
            "std of Test Accuracy value of top k=  13 : 0.018284599205145337\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  13 : 0.6685185185185185 +- 0.04147073273436792\n",
            "0.7099892512528865\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  13 : 0.6727272727272727 +- 0.018284599205145337\n",
            "0.6910118719324181\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "[0.65277778 0.6712963  0.64814815 0.62037037 0.71759259]\n",
            "[0.66666667 0.65151515 0.6969697  0.65367965 0.61688312]\n",
            "\n",
            "Max Validation Accuracy value of top k=  14 : 0.7175925925925926\n",
            "\n",
            "Max Test Accuracy value of top k=  14 : 0.696969696969697\n",
            "\n",
            "Mean Validation Accuracy value of top k=  14 : 0.662037037037037\n",
            "\n",
            "Mean Test Accuracy value of top k=  14 : 0.6571428571428571\n",
            "\n",
            "std of Validation Accuracy value of top k=  14 : 0.03220838357578904\n",
            "\n",
            "std of Test Accuracy value of top k=  14 : 0.025851079873304966\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  14 : 0.662037037037037 +- 0.03220838357578904\n",
            "0.694245420612826\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  14 : 0.6571428571428571 +- 0.025851079873304966\n",
            "0.6829939370161621\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "[0.61111111 0.66666667 0.64814815 0.64351852 0.72222222]\n",
            "[0.67748918 0.66233766 0.6969697  0.67748918 0.61471861]\n",
            "\n",
            "Max Validation Accuracy value of top k=  15 : 0.7222222222222222\n",
            "\n",
            "Max Test Accuracy value of top k=  15 : 0.696969696969697\n",
            "\n",
            "Mean Validation Accuracy value of top k=  15 : 0.6583333333333333\n",
            "\n",
            "Mean Test Accuracy value of top k=  15 : 0.6658008658008658\n",
            "\n",
            "std of Validation Accuracy value of top k=  15 : 0.03661799987645403\n",
            "\n",
            "std of Test Accuracy value of top k=  15 : 0.027806903641128557\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  15 : 0.6583333333333333 +- 0.03661799987645403\n",
            "0.6949513332097874\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  15 : 0.6658008658008658 +- 0.027806903641128557\n",
            "0.6936077694419944\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "[0.62962963 0.67592593 0.64351852 0.63888889 0.70833333]\n",
            "[0.68181818 0.65584416 0.6969697  0.69047619 0.64069264]\n",
            "\n",
            "Max Validation Accuracy value of top k=  16 : 0.7083333333333334\n",
            "\n",
            "Max Test Accuracy value of top k=  16 : 0.696969696969697\n",
            "\n",
            "Mean Validation Accuracy value of top k=  16 : 0.6592592592592592\n",
            "\n",
            "Mean Test Accuracy value of top k=  16 : 0.6731601731601732\n",
            "\n",
            "std of Validation Accuracy value of top k=  16 : 0.029074663829828873\n",
            "\n",
            "std of Test Accuracy value of top k=  16 : 0.021427478217774177\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  16 : 0.6592592592592592 +- 0.029074663829828873\n",
            "0.6883339230890881\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  16 : 0.6731601731601732 +- 0.021427478217774177\n",
            "0.6945876513779473\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "[0.625      0.68518519 0.64814815 0.63888889 0.70833333]\n",
            "[0.68181818 0.65800866 0.68398268 0.66450216 0.63852814]\n",
            "\n",
            "Max Validation Accuracy value of top k=  17 : 0.7083333333333334\n",
            "\n",
            "Max Test Accuracy value of top k=  17 : 0.683982683982684\n",
            "\n",
            "Mean Validation Accuracy value of top k=  17 : 0.6611111111111112\n",
            "\n",
            "Mean Test Accuracy value of top k=  17 : 0.6653679653679653\n",
            "\n",
            "std of Validation Accuracy value of top k=  17 : 0.03090429494125676\n",
            "\n",
            "std of Test Accuracy value of top k=  17 : 0.016687736162247956\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  17 : 0.6611111111111112 +- 0.03090429494125676\n",
            "0.692015406052368\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  17 : 0.6653679653679653 +- 0.016687736162247956\n",
            "0.6820557015302132\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "[0.62037037 0.65740741 0.67592593 0.64351852 0.73611111]\n",
            "[0.69480519 0.65800866 0.68181818 0.68831169 0.62987013]\n",
            "\n",
            "Max Validation Accuracy value of top k=  18 : 0.7361111111111112\n",
            "\n",
            "Max Test Accuracy value of top k=  18 : 0.6948051948051948\n",
            "\n",
            "Mean Validation Accuracy value of top k=  18 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  18 : 0.6705627705627706\n",
            "\n",
            "std of Validation Accuracy value of top k=  18 : 0.039174436669085544\n",
            "\n",
            "std of Test Accuracy value of top k=  18 : 0.023844916630165185\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  18 : 0.6666666666666667 +- 0.039174436669085544\n",
            "0.7058411033357523\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  18 : 0.6705627705627706 +- 0.023844916630165185\n",
            "0.6944076871929358\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "[0.60648148 0.68518519 0.65277778 0.62962963 0.72685185]\n",
            "[0.68181818 0.66233766 0.69047619 0.66666667 0.62554113]\n",
            "\n",
            "Max Validation Accuracy value of top k=  19 : 0.7268518518518519\n",
            "\n",
            "Max Test Accuracy value of top k=  19 : 0.6904761904761905\n",
            "\n",
            "Mean Validation Accuracy value of top k=  19 : 0.6601851851851853\n",
            "\n",
            "Mean Test Accuracy value of top k=  19 : 0.6653679653679653\n",
            "\n",
            "std of Validation Accuracy value of top k=  19 : 0.042289582395071246\n",
            "\n",
            "std of Test Accuracy value of top k=  19 : 0.02235208769424344\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  19 : 0.6601851851851853 +- 0.042289582395071246\n",
            "0.7024747675802565\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  19 : 0.6653679653679653 +- 0.02235208769424344\n",
            "0.6877200530622087\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "[0.60648148 0.67592593 0.65277778 0.64814815 0.71296296]\n",
            "[0.67532468 0.66233766 0.67965368 0.64718615 0.61255411]\n",
            "\n",
            "Max Validation Accuracy value of top k=  20 : 0.7129629629629629\n",
            "\n",
            "Max Test Accuracy value of top k=  20 : 0.6796536796536796\n",
            "\n",
            "Mean Validation Accuracy value of top k=  20 : 0.6592592592592592\n",
            "\n",
            "Mean Test Accuracy value of top k=  20 : 0.6554112554112554\n",
            "\n",
            "std of Validation Accuracy value of top k=  20 : 0.03496519923796147\n",
            "\n",
            "std of Test Accuracy value of top k=  20 : 0.024234692644637935\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  20 : 0.6592592592592592 +- 0.03496519923796147\n",
            "0.6942244584972207\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  20 : 0.6554112554112554 +- 0.024234692644637935\n",
            "0.6796459480558934\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "[0.60648148 0.69907407 0.65277778 0.63425926 0.68518519]\n",
            "[0.66233766 0.64069264 0.68831169 0.67316017 0.61688312]\n",
            "\n",
            "Max Validation Accuracy value of top k=  21 : 0.6990740740740741\n",
            "\n",
            "Max Test Accuracy value of top k=  21 : 0.6883116883116883\n",
            "\n",
            "Mean Validation Accuracy value of top k=  21 : 0.6555555555555556\n",
            "\n",
            "Mean Test Accuracy value of top k=  21 : 0.6562770562770563\n",
            "\n",
            "std of Validation Accuracy value of top k=  21 : 0.03358955027262417\n",
            "\n",
            "std of Test Accuracy value of top k=  21 : 0.02507087832999179\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  21 : 0.6555555555555556 +- 0.03358955027262417\n",
            "0.6891451058281797\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  21 : 0.6562770562770563 +- 0.02507087832999179\n",
            "0.6813479346070481\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "[0.58333333 0.64814815 0.66203704 0.64351852 0.70833333]\n",
            "[0.66233766 0.64718615 0.66883117 0.67099567 0.5995671 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  22 : 0.7083333333333334\n",
            "\n",
            "Max Test Accuracy value of top k=  22 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  22 : 0.6490740740740741\n",
            "\n",
            "Mean Test Accuracy value of top k=  22 : 0.6497835497835498\n",
            "\n",
            "std of Validation Accuracy value of top k=  22 : 0.04008307559613586\n",
            "\n",
            "std of Test Accuracy value of top k=  22 : 0.026453014921995147\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  22 : 0.6490740740740741 +- 0.04008307559613586\n",
            "0.68915714967021\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  22 : 0.6497835497835498 +- 0.026453014921995147\n",
            "0.6762365647055449\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "[0.59722222 0.65277778 0.65277778 0.625      0.67592593]\n",
            "[0.64935065 0.64069264 0.68398268 0.65151515 0.61255411]\n",
            "\n",
            "Max Validation Accuracy value of top k=  23 : 0.6759259259259259\n",
            "\n",
            "Max Test Accuracy value of top k=  23 : 0.683982683982684\n",
            "\n",
            "Mean Validation Accuracy value of top k=  23 : 0.6407407407407407\n",
            "\n",
            "Mean Test Accuracy value of top k=  23 : 0.6476190476190476\n",
            "\n",
            "std of Validation Accuracy value of top k=  23 : 0.02709025710801444\n",
            "\n",
            "std of Test Accuracy value of top k=  23 : 0.02288238144831606\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  23 : 0.6407407407407407 +- 0.02709025710801444\n",
            "0.6678309978487552\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  23 : 0.6476190476190476 +- 0.02288238144831606\n",
            "0.6705014290673638\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "[0.59259259 0.63888889 0.63888889 0.60648148 0.7037037 ]\n",
            "[0.67099567 0.63852814 0.66450216 0.64502165 0.5952381 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  24 : 0.7037037037037037\n",
            "\n",
            "Max Test Accuracy value of top k=  24 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  24 : 0.6361111111111111\n",
            "\n",
            "Mean Test Accuracy value of top k=  24 : 0.6428571428571429\n",
            "\n",
            "std of Validation Accuracy value of top k=  24 : 0.03835613921705182\n",
            "\n",
            "std of Test Accuracy value of top k=  24 : 0.02665063891189851\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  24 : 0.6361111111111111 +- 0.03835613921705182\n",
            "0.6744672503281629\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  24 : 0.6428571428571429 +- 0.02665063891189851\n",
            "0.6695077817690415\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "[0.58333333 0.66203704 0.64814815 0.62037037 0.73148148]\n",
            "[0.68831169 0.65584416 0.68398268 0.66233766 0.62987013]\n",
            "\n",
            "Max Validation Accuracy value of top k=  25 : 0.7314814814814815\n",
            "\n",
            "Max Test Accuracy value of top k=  25 : 0.6883116883116883\n",
            "\n",
            "Mean Validation Accuracy value of top k=  25 : 0.649074074074074\n",
            "\n",
            "Mean Test Accuracy value of top k=  25 : 0.6640692640692641\n",
            "\n",
            "std of Validation Accuracy value of top k=  25 : 0.04920492687254232\n",
            "\n",
            "std of Test Accuracy value of top k=  25 : 0.021092515523947756\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  25 : 0.649074074074074 +- 0.04920492687254232\n",
            "0.6982790009466163\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  25 : 0.6640692640692641 +- 0.021092515523947756\n",
            "0.6851617795932119\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "╒════╤════════════════╤═════════════════════════════════════════╤══════════════════════════════════════════╤════════════════════════════════════════╤══════════════════════════════════════════════╕\n",
            "│    │   Top K Number │   Random Forest Max Validation Accuracy │   Random Forest Mean Validation Accuracy │   Random Forest SD Validation Accuracy │   Random Forest Mean+-SD Validation Accuracy │\n",
            "╞════╪════════════════╪═════════════════════════════════════════╪══════════════════════════════════════════╪════════════════════════════════════════╪══════════════════════════════════════════════╡\n",
            "│  0 │              1 │                                0.458333 │                                 0.424074 │                              0.031208  │                                     0.455282 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  1 │              2 │                                0.625    │                                 0.576852 │                              0.0293681 │                                     0.60622  │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  2 │              3 │                                0.74537  │                                 0.683333 │                              0.0410552 │                                     0.724389 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  3 │              4 │                                0.740741 │                                 0.7      │                              0.0333076 │                                     0.733308 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  4 │              5 │                                0.740741 │                                 0.692593 │                              0.0375657 │                                     0.730158 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  5 │              6 │                                0.74537  │                                 0.692593 │                              0.0384677 │                                     0.73106  │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  6 │              7 │                                0.75     │                                 0.696296 │                              0.0389109 │                                     0.735207 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  7 │              8 │                                0.717593 │                                 0.688889 │                              0.0273736 │                                     0.716262 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  8 │              9 │                                0.740741 │                                 0.688889 │                              0.0383338 │                                     0.727223 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│  9 │             10 │                                0.736111 │                                 0.682407 │                              0.0302312 │                                     0.712639 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 10 │             11 │                                0.726852 │                                 0.673148 │                              0.0334617 │                                     0.70661  │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 11 │             12 │                                0.74537  │                                 0.675926 │                              0.0406776 │                                     0.716603 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 12 │             13 │                                0.740741 │                                 0.668519 │                              0.0414707 │                                     0.709989 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 13 │             14 │                                0.717593 │                                 0.662037 │                              0.0322084 │                                     0.694245 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 14 │             15 │                                0.722222 │                                 0.658333 │                              0.036618  │                                     0.694951 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 15 │             16 │                                0.708333 │                                 0.659259 │                              0.0290747 │                                     0.688334 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 16 │             17 │                                0.708333 │                                 0.661111 │                              0.0309043 │                                     0.692015 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 17 │             18 │                                0.736111 │                                 0.666667 │                              0.0391744 │                                     0.705841 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 18 │             19 │                                0.726852 │                                 0.660185 │                              0.0422896 │                                     0.702475 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 19 │             20 │                                0.712963 │                                 0.659259 │                              0.0349652 │                                     0.694224 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 20 │             21 │                                0.699074 │                                 0.655556 │                              0.0335896 │                                     0.689145 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 21 │             22 │                                0.708333 │                                 0.649074 │                              0.0400831 │                                     0.689157 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 22 │             23 │                                0.675926 │                                 0.640741 │                              0.0270903 │                                     0.667831 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 23 │             24 │                                0.703704 │                                 0.636111 │                              0.0383561 │                                     0.674467 │\n",
            "├────┼────────────────┼─────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼──────────────────────────────────────────────┤\n",
            "│ 24 │             25 │                                0.731481 │                                 0.649074 │                              0.0492049 │                                     0.698279 │\n",
            "╘════╧════════════════╧═════════════════════════════════════════╧══════════════════════════════════════════╧════════════════════════════════════════╧══════════════════════════════════════════════╛\n",
            "╒════╤════════════════╤═══════════════════════════════════╤════════════════════════════════════╤══════════════════════════════════╤════════════════════════════════════════╕\n",
            "│    │   Top K Number │   Random Forest Max Test Accuracy │   Random Forest Mean Test Accuracy │   Random Forest SD Test Accuracy │   Random Forest Mean+-SD Test Accuracy │\n",
            "╞════╪════════════════╪═══════════════════════════════════╪════════════════════════════════════╪══════════════════════════════════╪════════════════════════════════════════╡\n",
            "│  0 │              1 │                          0.439394 │                           0.40303  │                        0.0188399 │                               0.42187  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  1 │              2 │                          0.606061 │                           0.580519 │                        0.0167885 │                               0.597308 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  2 │              3 │                          0.705628 │                           0.673593 │                        0.0190378 │                               0.692631 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  3 │              4 │                          0.742424 │                           0.698701 │                        0.0272899 │                               0.725991 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  4 │              5 │                          0.714286 │                           0.689177 │                        0.0134548 │                               0.702632 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  5 │              6 │                          0.718615 │                           0.686147 │                        0.0184682 │                               0.704615 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  6 │              7 │                          0.712121 │                           0.682251 │                        0.0238056 │                               0.706057 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  7 │              8 │                          0.705628 │                           0.679221 │                        0.0188895 │                               0.69811  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  8 │              9 │                          0.722944 │                           0.688745 │                        0.0218347 │                               0.710579 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  9 │             10 │                          0.709957 │                           0.679654 │                        0.0221161 │                               0.70177  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 10 │             11 │                          0.707792 │                           0.677056 │                        0.0225524 │                               0.699609 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 11 │             12 │                          0.699134 │                           0.675758 │                        0.0172835 │                               0.693041 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 12 │             13 │                          0.690476 │                           0.672727 │                        0.0182846 │                               0.691012 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 13 │             14 │                          0.69697  │                           0.657143 │                        0.0258511 │                               0.682994 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 14 │             15 │                          0.69697  │                           0.665801 │                        0.0278069 │                               0.693608 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 15 │             16 │                          0.69697  │                           0.67316  │                        0.0214275 │                               0.694588 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 16 │             17 │                          0.683983 │                           0.665368 │                        0.0166877 │                               0.682056 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 17 │             18 │                          0.694805 │                           0.670563 │                        0.0238449 │                               0.694408 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 18 │             19 │                          0.690476 │                           0.665368 │                        0.0223521 │                               0.68772  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 19 │             20 │                          0.679654 │                           0.655411 │                        0.0242347 │                               0.679646 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 20 │             21 │                          0.688312 │                           0.656277 │                        0.0250709 │                               0.681348 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 21 │             22 │                          0.670996 │                           0.649784 │                        0.026453  │                               0.676237 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 22 │             23 │                          0.683983 │                           0.647619 │                        0.0228824 │                               0.670501 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 23 │             24 │                          0.670996 │                           0.642857 │                        0.0266506 │                               0.669508 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 24 │             25 │                          0.688312 │                           0.664069 │                        0.0210925 │                               0.685162 │\n",
            "╘════╧════════════════╧═══════════════════════════════════╧════════════════════════════════════╧══════════════════════════════════╧════════════════════════════════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vaidation and Test Accuracey Results For XGBoost Classifier:\")\n",
        "\n",
        "Vxgboost0=ValidationXGBoostAcc[0:25]\n",
        "Vxgboost1=ValidationXGBoostAcc[25:50]\n",
        "Vxgboost2=ValidationXGBoostAcc[50:75]\n",
        "Vxgboost3=ValidationXGBoostAcc[75:100]\n",
        "Vxgboost4=ValidationXGBoostAcc[100:125]\n",
        "Txgboost0=TestXGBoostAcc[0:25]\n",
        "Txgboost1=TestXGBoostAcc[25:50]\n",
        "Txgboost2=TestXGBoostAcc[50:75]\n",
        "Txgboost3=TestXGBoostAcc[75:100]\n",
        "Txgboost4=TestXGBoostAccTrf4=TestRFAcc[100:125]\n",
        "\n",
        "MET=[]\n",
        "MAT=[]\n",
        "SDT=[]\n",
        "VT=[]\n",
        "ET=[]\n",
        "MEV=[]\n",
        "MAV=[]\n",
        "SDV=[]\n",
        "VV=[]\n",
        "EV=[]\n",
        "A=[]\n",
        "B=[]\n",
        "# Original array\n",
        "for i in range(0,25):\n",
        "  a= np.array([Vxgboost0[i],Vxgboost1[i],Vxgboost2[i],Vxgboost3[i],Vxgboost4[i]])\n",
        "  b= np.array([Txgboost0[i],Txgboost1[i],Txgboost2[i],Txgboost3[i],Txgboost4[i]])\n",
        "  A.append(a)\n",
        "  B.append(b)\n",
        "  print(A[i])\n",
        "  print(B[i])\n",
        "  mav= np.max(A[i])\n",
        "  MAV.append(mav)\n",
        "  mat= np.max(B[i])\n",
        "  MAT.append(mat)\n",
        "  print(\"\\nMax Validation Accuracy value of top k= \", i+1,\":\", mav)\n",
        "  print(\"\\nMax Test Accuracy value of top k= \", i+1,\":\", mat)\n",
        "\n",
        "  mev= np.mean(A[i])\n",
        "  MEV.append(mev)\n",
        "  print(\"\\nMean Validation Accuracy value of top k= \", i+1,\":\", mev)\n",
        "  met= np.mean(B[i])\n",
        "  MET.append(met)\n",
        "  print(\"\\nMean Test Accuracy value of top k= \", i+1,\":\", met)\n",
        "   \n",
        "  sdv= np.std(A[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Validation Accuracy value of top k= \", i+1,\":\", sdv)\n",
        "  SDV.append(sdv)\n",
        "\n",
        "\n",
        "  sdt= np.std(B[i])\n",
        "  #sd[i] = np.sqrt(np.mean((array - np.mean(array)) ** 2))\n",
        "  print(\"\\nstd of Test Accuracy value of top k= \", i+1,\":\", sdt)\n",
        "  SDT.append(sdt)\n",
        "\n",
        "  ev=mev+sdv\n",
        "  print(\"\\n Mean + SD: of Validation Accuracy value of top k= \", i+1,\":\", mev, \"+-\", sdv)\n",
        "  print(ev)\n",
        "  EV.append(ev)\n",
        "\n",
        "  et=met+sdt\n",
        "  print(\"\\n Mean + SD: of Test Accuracy value of top k= \", i+1,\":\", met, \"+-\", sdt)\n",
        "  print(et)\n",
        "  ET.append(et)\n",
        "  \n",
        "  vv= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vv)\n",
        "  VV.append(vv)\n",
        "\n",
        "  vt= np.var(i, dtype = np.float64)\n",
        "  print(\"\\nvariance of Validation Accuracy value of top k= \", i+1,\":\",vt)\n",
        "  VT.append(vt)\n",
        "\n",
        "X= np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
        "df=pd.DataFrame({'Top K Number': X+1, 'XGBoost Max Validation Accuracy': MAV, 'XGBoost Mean Validation Accuracy': MEV, 'XGBoost SD Validation Accuracy': SDV, 'XGBoost Mean+-SD Validation Accuracy': EV})\n",
        "print(tabulate(df,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df.to_excel('XGBoostValidationResults.xlsx')\n",
        "df1=pd.DataFrame({'Top K Number': X+1, 'XGBoost Max Test Accuracy': MAT, 'XGBoost Mean Test Accuracy': MET, 'XGBoost SD Test Accuracy': SDT, 'XGBoost Mean+-SD Test Accuracy': ET})\n",
        "print(tabulate(df1,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df1.to_excel('XGBoostTestResults.xlsx')"
      ],
      "metadata": {
        "id": "BLwG268UU4GA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aacdde9-d04e-418a-bfd2-c9da87417402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vaidation and Test Accuracey Results For XGBoost Classifier:\n",
            "[0.44444444 0.5        0.48148148 0.49074074 0.48148148]\n",
            "[0.49350649 0.45670996 0.49350649 0.48051948 0.39393939]\n",
            "\n",
            "Max Validation Accuracy value of top k=  1 : 0.5\n",
            "\n",
            "Max Test Accuracy value of top k=  1 : 0.4935064935064935\n",
            "\n",
            "Mean Validation Accuracy value of top k=  1 : 0.4796296296296296\n",
            "\n",
            "Mean Test Accuracy value of top k=  1 : 0.4636363636363637\n",
            "\n",
            "std of Validation Accuracy value of top k=  1 : 0.018885257457751065\n",
            "\n",
            "std of Test Accuracy value of top k=  1 : 0.03735005127294024\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  1 : 0.4796296296296296 +- 0.018885257457751065\n",
            "0.4985148870873806\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  1 : 0.4636363636363637 +- 0.03735005127294024\n",
            "0.500986414909304\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  1 : 0.0\n",
            "[0.55555556 0.55555556 0.53703704 0.52314815 0.55092593]\n",
            "[0.55627706 0.57359307 0.56709957 0.55411255 0.56060606]\n",
            "\n",
            "Max Validation Accuracy value of top k=  2 : 0.5555555555555556\n",
            "\n",
            "Max Test Accuracy value of top k=  2 : 0.5735930735930735\n",
            "\n",
            "Mean Validation Accuracy value of top k=  2 : 0.5444444444444445\n",
            "\n",
            "Mean Test Accuracy value of top k=  2 : 0.5623376623376622\n",
            "\n",
            "std of Validation Accuracy value of top k=  2 : 0.012627946015727651\n",
            "\n",
            "std of Test Accuracy value of top k=  2 : 0.007165777210929362\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  2 : 0.5444444444444445 +- 0.012627946015727651\n",
            "0.5570723904601722\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  2 : 0.5623376623376622 +- 0.007165777210929362\n",
            "0.5695034395485916\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  2 : 0.0\n",
            "[0.58796296 0.65740741 0.62962963 0.60648148 0.64351852]\n",
            "[0.63636364 0.64935065 0.64935065 0.61904762 0.64935065]\n",
            "\n",
            "Max Validation Accuracy value of top k=  3 : 0.6574074074074074\n",
            "\n",
            "Max Test Accuracy value of top k=  3 : 0.6493506493506493\n",
            "\n",
            "Mean Validation Accuracy value of top k=  3 : 0.625\n",
            "\n",
            "Mean Test Accuracy value of top k=  3 : 0.6406926406926406\n",
            "\n",
            "std of Validation Accuracy value of top k=  3 : 0.025017140900196858\n",
            "\n",
            "std of Test Accuracy value of top k=  3 : 0.011934241343801048\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  3 : 0.625 +- 0.025017140900196858\n",
            "0.6500171409001969\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  3 : 0.6406926406926406 +- 0.011934241343801048\n",
            "0.6526268820364417\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  3 : 0.0\n",
            "[0.61111111 0.6712963  0.66203704 0.65277778 0.65277778]\n",
            "[0.65584416 0.62770563 0.66233766 0.65800866 0.65800866]\n",
            "\n",
            "Max Validation Accuracy value of top k=  4 : 0.6712962962962963\n",
            "\n",
            "Max Test Accuracy value of top k=  4 : 0.6623376623376623\n",
            "\n",
            "Mean Validation Accuracy value of top k=  4 : 0.65\n",
            "\n",
            "Mean Test Accuracy value of top k=  4 : 0.6523809523809524\n",
            "\n",
            "std of Validation Accuracy value of top k=  4 : 0.020621349491963026\n",
            "\n",
            "std of Test Accuracy value of top k=  4 : 0.012516737917576617\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  4 : 0.65 +- 0.020621349491963026\n",
            "0.6706213494919631\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  4 : 0.6523809523809524 +- 0.012516737917576617\n",
            "0.664897690298529\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  4 : 0.0\n",
            "[0.625      0.67592593 0.65740741 0.67592593 0.66666667]\n",
            "[0.65584416 0.64285714 0.67099567 0.61038961 0.68181818]\n",
            "\n",
            "Max Validation Accuracy value of top k=  5 : 0.6759259259259259\n",
            "\n",
            "Max Test Accuracy value of top k=  5 : 0.6818181818181818\n",
            "\n",
            "Mean Validation Accuracy value of top k=  5 : 0.6601851851851852\n",
            "\n",
            "Mean Test Accuracy value of top k=  5 : 0.6523809523809524\n",
            "\n",
            "std of Validation Accuracy value of top k=  5 : 0.01888525745775105\n",
            "\n",
            "std of Test Accuracy value of top k=  5 : 0.024815429693448507\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  5 : 0.6601851851851852 +- 0.01888525745775105\n",
            "0.6790704426429363\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  5 : 0.6523809523809524 +- 0.024815429693448507\n",
            "0.6771963820744009\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  5 : 0.0\n",
            "[0.63888889 0.69907407 0.66203704 0.66203704 0.66666667]\n",
            "[0.67099567 0.64935065 0.67316017 0.63852814 0.67099567]\n",
            "\n",
            "Max Validation Accuracy value of top k=  6 : 0.6990740740740741\n",
            "\n",
            "Max Test Accuracy value of top k=  6 : 0.6731601731601732\n",
            "\n",
            "Mean Validation Accuracy value of top k=  6 : 0.6657407407407407\n",
            "\n",
            "Mean Test Accuracy value of top k=  6 : 0.6606060606060605\n",
            "\n",
            "std of Validation Accuracy value of top k=  6 : 0.019289506162962655\n",
            "\n",
            "std of Test Accuracy value of top k=  6 : 0.014054271065582644\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  6 : 0.6657407407407407 +- 0.019289506162962655\n",
            "0.6850302469037034\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  6 : 0.6606060606060605 +- 0.014054271065582644\n",
            "0.6746603316716432\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  6 : 0.0\n",
            "[0.625      0.70833333 0.65740741 0.66203704 0.67592593]\n",
            "[0.66666667 0.64502165 0.67748918 0.63852814 0.64502165]\n",
            "\n",
            "Max Validation Accuracy value of top k=  7 : 0.7083333333333334\n",
            "\n",
            "Max Test Accuracy value of top k=  7 : 0.6774891774891775\n",
            "\n",
            "Mean Validation Accuracy value of top k=  7 : 0.6657407407407409\n",
            "\n",
            "Mean Test Accuracy value of top k=  7 : 0.6545454545454545\n",
            "\n",
            "std of Validation Accuracy value of top k=  7 : 0.027058591104078634\n",
            "\n",
            "std of Test Accuracy value of top k=  7 : 0.014908376933093116\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  7 : 0.6657407407407409 +- 0.027058591104078634\n",
            "0.6927993318448195\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  7 : 0.6545454545454545 +- 0.014908376933093116\n",
            "0.6694538314785476\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  7 : 0.0\n",
            "[0.625      0.70833333 0.65740741 0.66203704 0.67592593]\n",
            "[0.66666667 0.64502165 0.67748918 0.63852814 0.64935065]\n",
            "\n",
            "Max Validation Accuracy value of top k=  8 : 0.7083333333333334\n",
            "\n",
            "Max Test Accuracy value of top k=  8 : 0.6774891774891775\n",
            "\n",
            "Mean Validation Accuracy value of top k=  8 : 0.6657407407407409\n",
            "\n",
            "Mean Test Accuracy value of top k=  8 : 0.6554112554112554\n",
            "\n",
            "std of Validation Accuracy value of top k=  8 : 0.027058591104078634\n",
            "\n",
            "std of Test Accuracy value of top k=  8 : 0.014448761271236905\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  8 : 0.6657407407407409 +- 0.027058591104078634\n",
            "0.6927993318448195\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  8 : 0.6554112554112554 +- 0.014448761271236905\n",
            "0.6698600166824924\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  8 : 0.0\n",
            "[0.63425926 0.68518519 0.64814815 0.66203704 0.66666667]\n",
            "[0.66450216 0.63636364 0.67316017 0.63852814 0.66017316]\n",
            "\n",
            "Max Validation Accuracy value of top k=  9 : 0.6851851851851852\n",
            "\n",
            "Max Test Accuracy value of top k=  9 : 0.6731601731601732\n",
            "\n",
            "Mean Validation Accuracy value of top k=  9 : 0.6592592592592592\n",
            "\n",
            "Mean Test Accuracy value of top k=  9 : 0.6545454545454545\n",
            "\n",
            "std of Validation Accuracy value of top k=  9 : 0.017223217812720622\n",
            "\n",
            "std of Test Accuracy value of top k=  9 : 0.014590735537967733\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  9 : 0.6592592592592592 +- 0.017223217812720622\n",
            "0.6764824770719798\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  9 : 0.6545454545454545 +- 0.014590735537967733\n",
            "0.6691361900834223\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  9 : 0.0\n",
            "[0.625      0.68981481 0.66666667 0.67592593 0.67592593]\n",
            "[0.67099567 0.64502165 0.68181818 0.64069264 0.64718615]\n",
            "\n",
            "Max Validation Accuracy value of top k=  10 : 0.6898148148148148\n",
            "\n",
            "Max Test Accuracy value of top k=  10 : 0.6818181818181818\n",
            "\n",
            "Mean Validation Accuracy value of top k=  10 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  10 : 0.6571428571428573\n",
            "\n",
            "std of Validation Accuracy value of top k=  10 : 0.02210617849317281\n",
            "\n",
            "std of Test Accuracy value of top k=  10 : 0.01623232316818413\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  10 : 0.6666666666666667 +- 0.02210617849317281\n",
            "0.6887728451598395\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  10 : 0.6571428571428573 +- 0.01623232316818413\n",
            "0.6733751803110414\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  10 : 0.0\n",
            "[0.625      0.68981481 0.66666667 0.67592593 0.67592593]\n",
            "[0.67099567 0.64502165 0.68181818 0.64069264 0.64502165]\n",
            "\n",
            "Max Validation Accuracy value of top k=  11 : 0.6898148148148148\n",
            "\n",
            "Max Test Accuracy value of top k=  11 : 0.6818181818181818\n",
            "\n",
            "Mean Validation Accuracy value of top k=  11 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  11 : 0.6567099567099568\n",
            "\n",
            "std of Validation Accuracy value of top k=  11 : 0.02210617849317281\n",
            "\n",
            "std of Test Accuracy value of top k=  11 : 0.016518427730163553\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  11 : 0.6666666666666667 +- 0.02210617849317281\n",
            "0.6887728451598395\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  11 : 0.6567099567099568 +- 0.016518427730163553\n",
            "0.6732283844401203\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  11 : 0.0\n",
            "[0.625      0.68981481 0.66666667 0.67592593 0.67592593]\n",
            "[0.67099567 0.64502165 0.68181818 0.64069264 0.66017316]\n",
            "\n",
            "Max Validation Accuracy value of top k=  12 : 0.6898148148148148\n",
            "\n",
            "Max Test Accuracy value of top k=  12 : 0.6818181818181818\n",
            "\n",
            "Mean Validation Accuracy value of top k=  12 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  12 : 0.6597402597402597\n",
            "\n",
            "std of Validation Accuracy value of top k=  12 : 0.02210617849317281\n",
            "\n",
            "std of Test Accuracy value of top k=  12 : 0.015451574283614496\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  12 : 0.6666666666666667 +- 0.02210617849317281\n",
            "0.6887728451598395\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  12 : 0.6597402597402597 +- 0.015451574283614496\n",
            "0.6751918340238743\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  12 : 0.0\n",
            "[0.625      0.69444444 0.66203704 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.64285714]\n",
            "\n",
            "Max Validation Accuracy value of top k=  13 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  13 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  13 : 0.6675925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  13 : 0.6489177489177489\n",
            "\n",
            "std of Validation Accuracy value of top k=  13 : 0.023679096023230407\n",
            "\n",
            "std of Test Accuracy value of top k=  13 : 0.014318472215187221\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  13 : 0.6675925925925926 +- 0.023679096023230407\n",
            "0.691271688615823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  13 : 0.6489177489177489 +- 0.014318472215187221\n",
            "0.6632362211329361\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  13 : 0.0\n",
            "[0.625      0.69444444 0.66203704 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.61688312]\n",
            "\n",
            "Max Validation Accuracy value of top k=  14 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  14 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  14 : 0.6675925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  14 : 0.6437229437229438\n",
            "\n",
            "std of Validation Accuracy value of top k=  14 : 0.023679096023230407\n",
            "\n",
            "std of Test Accuracy value of top k=  14 : 0.019388914005678524\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  14 : 0.6675925925925926 +- 0.023679096023230407\n",
            "0.691271688615823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  14 : 0.6437229437229438 +- 0.019388914005678524\n",
            "0.6631118577286224\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  14 : 0.0\n",
            "[0.625      0.69444444 0.66203704 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.61471861]\n",
            "\n",
            "Max Validation Accuracy value of top k=  15 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  15 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  15 : 0.6675925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  15 : 0.6432900432900432\n",
            "\n",
            "std of Validation Accuracy value of top k=  15 : 0.023679096023230407\n",
            "\n",
            "std of Test Accuracy value of top k=  15 : 0.019997938463118814\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  15 : 0.6675925925925926 +- 0.023679096023230407\n",
            "0.691271688615823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  15 : 0.6432900432900432 +- 0.019997938463118814\n",
            "0.6632879817531621\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  15 : 0.0\n",
            "[0.625      0.69444444 0.66203704 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.64069264]\n",
            "\n",
            "Max Validation Accuracy value of top k=  16 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  16 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  16 : 0.6675925925925926\n",
            "\n",
            "Mean Test Accuracy value of top k=  16 : 0.6484848484848486\n",
            "\n",
            "std of Validation Accuracy value of top k=  16 : 0.023679096023230407\n",
            "\n",
            "std of Test Accuracy value of top k=  16 : 0.014526373797023843\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  16 : 0.6675925925925926 +- 0.023679096023230407\n",
            "0.691271688615823\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  16 : 0.6484848484848486 +- 0.014526373797023843\n",
            "0.6630112222818724\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  16 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.63852814]\n",
            "\n",
            "Max Validation Accuracy value of top k=  17 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  17 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  17 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  17 : 0.6480519480519481\n",
            "\n",
            "std of Validation Accuracy value of top k=  17 : 0.023966998343601446\n",
            "\n",
            "std of Test Accuracy value of top k=  17 : 0.0147821394623064\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  17 : 0.6666666666666667 +- 0.023966998343601446\n",
            "0.6906336650102682\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  17 : 0.6480519480519481 +- 0.0147821394623064\n",
            "0.6628340875142545\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  17 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.62987013]\n",
            "\n",
            "Max Validation Accuracy value of top k=  18 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  18 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  18 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  18 : 0.6463203463203463\n",
            "\n",
            "std of Validation Accuracy value of top k=  18 : 0.023966998343601446\n",
            "\n",
            "std of Test Accuracy value of top k=  18 : 0.01623232316818414\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  18 : 0.6666666666666667 +- 0.023966998343601446\n",
            "0.6906336650102682\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  18 : 0.6463203463203463 +- 0.01623232316818414\n",
            "0.6625526694885304\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  18 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.62554113]\n",
            "\n",
            "Max Validation Accuracy value of top k=  19 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  19 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  19 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  19 : 0.6454545454545455\n",
            "\n",
            "std of Validation Accuracy value of top k=  19 : 0.023966998343601446\n",
            "\n",
            "std of Test Accuracy value of top k=  19 : 0.01717474841980727\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  19 : 0.6666666666666667 +- 0.023966998343601446\n",
            "0.6906336650102682\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  19 : 0.6454545454545455 +- 0.01717474841980727\n",
            "0.6626292938743528\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  19 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.61255411]\n",
            "\n",
            "Max Validation Accuracy value of top k=  20 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  20 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  20 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  20 : 0.6428571428571429\n",
            "\n",
            "std of Validation Accuracy value of top k=  20 : 0.023966998343601446\n",
            "\n",
            "std of Test Accuracy value of top k=  20 : 0.02062533203389023\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  20 : 0.6666666666666667 +- 0.023966998343601446\n",
            "0.6906336650102682\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  20 : 0.6428571428571429 +- 0.02062533203389023\n",
            "0.6634824748910332\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  20 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.61688312]\n",
            "\n",
            "Max Validation Accuracy value of top k=  21 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  21 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  21 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  21 : 0.6437229437229438\n",
            "\n",
            "std of Validation Accuracy value of top k=  21 : 0.023966998343601446\n",
            "\n",
            "std of Test Accuracy value of top k=  21 : 0.019388914005678524\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  21 : 0.6666666666666667 +- 0.023966998343601446\n",
            "0.6906336650102682\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  21 : 0.6437229437229438 +- 0.019388914005678524\n",
            "0.6631118577286224\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  21 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.5995671 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  22 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  22 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  22 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  22 : 0.6402597402597403\n",
            "\n",
            "std of Validation Accuracy value of top k=  22 : 0.023966998343601446\n",
            "\n",
            "std of Test Accuracy value of top k=  22 : 0.024694304237145758\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  22 : 0.6666666666666667 +- 0.023966998343601446\n",
            "0.6906336650102682\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  22 : 0.6402597402597403 +- 0.024694304237145758\n",
            "0.6649540444968861\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  22 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.67592593]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.61255411]\n",
            "\n",
            "Max Validation Accuracy value of top k=  23 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  23 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  23 : 0.6666666666666667\n",
            "\n",
            "Mean Test Accuracy value of top k=  23 : 0.6428571428571429\n",
            "\n",
            "std of Validation Accuracy value of top k=  23 : 0.023966998343601446\n",
            "\n",
            "std of Test Accuracy value of top k=  23 : 0.02062533203389023\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  23 : 0.6666666666666667 +- 0.023966998343601446\n",
            "0.6906336650102682\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  23 : 0.6428571428571429 +- 0.02062533203389023\n",
            "0.6634824748910332\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  23 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.66203704]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.5952381 ]\n",
            "\n",
            "Max Validation Accuracy value of top k=  24 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  24 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  24 : 0.663888888888889\n",
            "\n",
            "Mean Test Accuracy value of top k=  24 : 0.6393939393939394\n",
            "\n",
            "std of Validation Accuracy value of top k=  24 : 0.02353382412433117\n",
            "\n",
            "std of Test Accuracy value of top k=  24 : 0.026139444397316944\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  24 : 0.663888888888889 +- 0.02353382412433117\n",
            "0.6874227130132201\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  24 : 0.6393939393939394 +- 0.026139444397316944\n",
            "0.6655333837912564\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  24 : 0.0\n",
            "[0.625      0.69444444 0.65740741 0.68055556 0.66203704]\n",
            "[0.67099567 0.63419913 0.66017316 0.63636364 0.62987013]\n",
            "\n",
            "Max Validation Accuracy value of top k=  25 : 0.6944444444444444\n",
            "\n",
            "Max Test Accuracy value of top k=  25 : 0.670995670995671\n",
            "\n",
            "Mean Validation Accuracy value of top k=  25 : 0.663888888888889\n",
            "\n",
            "Mean Test Accuracy value of top k=  25 : 0.6463203463203463\n",
            "\n",
            "std of Validation Accuracy value of top k=  25 : 0.02353382412433117\n",
            "\n",
            "std of Test Accuracy value of top k=  25 : 0.01623232316818414\n",
            "\n",
            " Mean + SD: of Validation Accuracy value of top k=  25 : 0.663888888888889 +- 0.02353382412433117\n",
            "0.6874227130132201\n",
            "\n",
            " Mean + SD: of Test Accuracy value of top k=  25 : 0.6463203463203463 +- 0.01623232316818414\n",
            "0.6625526694885304\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "\n",
            "variance of Validation Accuracy value of top k=  25 : 0.0\n",
            "╒════╤════════════════╤═══════════════════════════════════╤════════════════════════════════════╤══════════════════════════════════╤════════════════════════════════════════╕\n",
            "│    │   Top K Number │   XGBoost Max Validation Accuracy │   XGBoost Mean Validation Accuracy │   XGBoost SD Validation Accuracy │   XGBoost Mean+-SD Validation Accuracy │\n",
            "╞════╪════════════════╪═══════════════════════════════════╪════════════════════════════════════╪══════════════════════════════════╪════════════════════════════════════════╡\n",
            "│  0 │              1 │                          0.5      │                           0.47963  │                        0.0188853 │                               0.498515 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  1 │              2 │                          0.555556 │                           0.544444 │                        0.0126279 │                               0.557072 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  2 │              3 │                          0.657407 │                           0.625    │                        0.0250171 │                               0.650017 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  3 │              4 │                          0.671296 │                           0.65     │                        0.0206213 │                               0.670621 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  4 │              5 │                          0.675926 │                           0.660185 │                        0.0188853 │                               0.67907  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  5 │              6 │                          0.699074 │                           0.665741 │                        0.0192895 │                               0.68503  │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  6 │              7 │                          0.708333 │                           0.665741 │                        0.0270586 │                               0.692799 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  7 │              8 │                          0.708333 │                           0.665741 │                        0.0270586 │                               0.692799 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  8 │              9 │                          0.685185 │                           0.659259 │                        0.0172232 │                               0.676482 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│  9 │             10 │                          0.689815 │                           0.666667 │                        0.0221062 │                               0.688773 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 10 │             11 │                          0.689815 │                           0.666667 │                        0.0221062 │                               0.688773 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 11 │             12 │                          0.689815 │                           0.666667 │                        0.0221062 │                               0.688773 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 12 │             13 │                          0.694444 │                           0.667593 │                        0.0236791 │                               0.691272 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 13 │             14 │                          0.694444 │                           0.667593 │                        0.0236791 │                               0.691272 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 14 │             15 │                          0.694444 │                           0.667593 │                        0.0236791 │                               0.691272 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 15 │             16 │                          0.694444 │                           0.667593 │                        0.0236791 │                               0.691272 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 16 │             17 │                          0.694444 │                           0.666667 │                        0.023967  │                               0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 17 │             18 │                          0.694444 │                           0.666667 │                        0.023967  │                               0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 18 │             19 │                          0.694444 │                           0.666667 │                        0.023967  │                               0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 19 │             20 │                          0.694444 │                           0.666667 │                        0.023967  │                               0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 20 │             21 │                          0.694444 │                           0.666667 │                        0.023967  │                               0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 21 │             22 │                          0.694444 │                           0.666667 │                        0.023967  │                               0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 22 │             23 │                          0.694444 │                           0.666667 │                        0.023967  │                               0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 23 │             24 │                          0.694444 │                           0.663889 │                        0.0235338 │                               0.687423 │\n",
            "├────┼────────────────┼───────────────────────────────────┼────────────────────────────────────┼──────────────────────────────────┼────────────────────────────────────────┤\n",
            "│ 24 │             25 │                          0.694444 │                           0.663889 │                        0.0235338 │                               0.687423 │\n",
            "╘════╧════════════════╧═══════════════════════════════════╧════════════════════════════════════╧══════════════════════════════════╧════════════════════════════════════════╛\n",
            "╒════╤════════════════╤═════════════════════════════╤══════════════════════════════╤════════════════════════════╤══════════════════════════════════╕\n",
            "│    │   Top K Number │   XGBoost Max Test Accuracy │   XGBoost Mean Test Accuracy │   XGBoost SD Test Accuracy │   XGBoost Mean+-SD Test Accuracy │\n",
            "╞════╪════════════════╪═════════════════════════════╪══════════════════════════════╪════════════════════════════╪══════════════════════════════════╡\n",
            "│  0 │              1 │                    0.493506 │                     0.463636 │                 0.0373501  │                         0.500986 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  1 │              2 │                    0.573593 │                     0.562338 │                 0.00716578 │                         0.569503 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  2 │              3 │                    0.649351 │                     0.640693 │                 0.0119342  │                         0.652627 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  3 │              4 │                    0.662338 │                     0.652381 │                 0.0125167  │                         0.664898 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  4 │              5 │                    0.681818 │                     0.652381 │                 0.0248154  │                         0.677196 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  5 │              6 │                    0.67316  │                     0.660606 │                 0.0140543  │                         0.67466  │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  6 │              7 │                    0.677489 │                     0.654545 │                 0.0149084  │                         0.669454 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  7 │              8 │                    0.677489 │                     0.655411 │                 0.0144488  │                         0.66986  │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  8 │              9 │                    0.67316  │                     0.654545 │                 0.0145907  │                         0.669136 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│  9 │             10 │                    0.681818 │                     0.657143 │                 0.0162323  │                         0.673375 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 10 │             11 │                    0.681818 │                     0.65671  │                 0.0165184  │                         0.673228 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 11 │             12 │                    0.681818 │                     0.65974  │                 0.0154516  │                         0.675192 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 12 │             13 │                    0.670996 │                     0.648918 │                 0.0143185  │                         0.663236 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 13 │             14 │                    0.670996 │                     0.643723 │                 0.0193889  │                         0.663112 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 14 │             15 │                    0.670996 │                     0.64329  │                 0.0199979  │                         0.663288 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 15 │             16 │                    0.670996 │                     0.648485 │                 0.0145264  │                         0.663011 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 16 │             17 │                    0.670996 │                     0.648052 │                 0.0147821  │                         0.662834 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 17 │             18 │                    0.670996 │                     0.64632  │                 0.0162323  │                         0.662553 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 18 │             19 │                    0.670996 │                     0.645455 │                 0.0171747  │                         0.662629 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 19 │             20 │                    0.670996 │                     0.642857 │                 0.0206253  │                         0.663482 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 20 │             21 │                    0.670996 │                     0.643723 │                 0.0193889  │                         0.663112 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 21 │             22 │                    0.670996 │                     0.64026  │                 0.0246943  │                         0.664954 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 22 │             23 │                    0.670996 │                     0.642857 │                 0.0206253  │                         0.663482 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 23 │             24 │                    0.670996 │                     0.639394 │                 0.0261394  │                         0.665533 │\n",
            "├────┼────────────────┼─────────────────────────────┼──────────────────────────────┼────────────────────────────┼──────────────────────────────────┤\n",
            "│ 24 │             25 │                    0.670996 │                     0.64632  │                 0.0162323  │                         0.662553 │\n",
            "╘════╧════════════════╧═════════════════════════════╧══════════════════════════════╧════════════════════════════╧══════════════════════════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
        "df=pd.DataFrame({'Top K Number': X+1, 'SVM Max Validation Accuracy': MAV, 'SVM Mean Validation Accuracy': MEV, 'SVM SD Validation Accuracy': SDV, 'Mean+-SD Validation Accuracy': EV})\n",
        "print(tabulate(df,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df.to_excel('SVMValidationResults.xlsx')\n",
        "df1=pd.DataFrame({'Top K Number': X+1, 'SVM Max Test Accuracy': MAT, 'Mean Test Accuracy': MET, 'SVM SD Test Accuracy': SDT, 'Mean+-SD Test Accuracy': ET})\n",
        "print(tabulate(df1,headers=\"keys\", tablefmt='fancy_grid'))\n",
        "df1.to_excel('SVMTestResults.xlsx')"
      ],
      "metadata": {
        "id": "HZ3HeUnVQeE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a889c5-b905-484e-ae70-5b19c32f2f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒════╤════════════════╤═══════════════════════════════╤════════════════════════════════╤══════════════════════════════╤════════════════════════════════╕\n",
            "│    │   Top K Number │   SVM Max Validation Accuracy │   SVM Mean Validation Accuracy │   SVM SD Validation Accuracy │   Mean+-SD Validation Accuracy │\n",
            "╞════╪════════════════╪═══════════════════════════════╪════════════════════════════════╪══════════════════════════════╪════════════════════════════════╡\n",
            "│  0 │              1 │                      0.5      │                       0.47963  │                    0.0188853 │                       0.498515 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  1 │              2 │                      0.555556 │                       0.544444 │                    0.0126279 │                       0.557072 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  2 │              3 │                      0.657407 │                       0.625    │                    0.0250171 │                       0.650017 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  3 │              4 │                      0.671296 │                       0.65     │                    0.0206213 │                       0.670621 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  4 │              5 │                      0.675926 │                       0.660185 │                    0.0188853 │                       0.67907  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  5 │              6 │                      0.699074 │                       0.665741 │                    0.0192895 │                       0.68503  │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  6 │              7 │                      0.708333 │                       0.665741 │                    0.0270586 │                       0.692799 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  7 │              8 │                      0.708333 │                       0.665741 │                    0.0270586 │                       0.692799 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  8 │              9 │                      0.685185 │                       0.659259 │                    0.0172232 │                       0.676482 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│  9 │             10 │                      0.689815 │                       0.666667 │                    0.0221062 │                       0.688773 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 10 │             11 │                      0.689815 │                       0.666667 │                    0.0221062 │                       0.688773 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 11 │             12 │                      0.689815 │                       0.666667 │                    0.0221062 │                       0.688773 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 12 │             13 │                      0.694444 │                       0.667593 │                    0.0236791 │                       0.691272 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 13 │             14 │                      0.694444 │                       0.667593 │                    0.0236791 │                       0.691272 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 14 │             15 │                      0.694444 │                       0.667593 │                    0.0236791 │                       0.691272 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 15 │             16 │                      0.694444 │                       0.667593 │                    0.0236791 │                       0.691272 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 16 │             17 │                      0.694444 │                       0.666667 │                    0.023967  │                       0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 17 │             18 │                      0.694444 │                       0.666667 │                    0.023967  │                       0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 18 │             19 │                      0.694444 │                       0.666667 │                    0.023967  │                       0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 19 │             20 │                      0.694444 │                       0.666667 │                    0.023967  │                       0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 20 │             21 │                      0.694444 │                       0.666667 │                    0.023967  │                       0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 21 │             22 │                      0.694444 │                       0.666667 │                    0.023967  │                       0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 22 │             23 │                      0.694444 │                       0.666667 │                    0.023967  │                       0.690634 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 23 │             24 │                      0.694444 │                       0.663889 │                    0.0235338 │                       0.687423 │\n",
            "├────┼────────────────┼───────────────────────────────┼────────────────────────────────┼──────────────────────────────┼────────────────────────────────┤\n",
            "│ 24 │             25 │                      0.694444 │                       0.663889 │                    0.0235338 │                       0.687423 │\n",
            "╘════╧════════════════╧═══════════════════════════════╧════════════════════════════════╧══════════════════════════════╧════════════════════════════════╛\n",
            "╒════╤════════════════╤═════════════════════════╤══════════════════════╤════════════════════════╤══════════════════════════╕\n",
            "│    │   Top K Number │   SVM Max Test Accuracy │   Mean Test Accuracy │   SVM SD Test Accuracy │   Mean+-SD Test Accuracy │\n",
            "╞════╪════════════════╪═════════════════════════╪══════════════════════╪════════════════════════╪══════════════════════════╡\n",
            "│  0 │              1 │                0.493506 │             0.463636 │             0.0373501  │                 0.500986 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  1 │              2 │                0.573593 │             0.562338 │             0.00716578 │                 0.569503 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  2 │              3 │                0.649351 │             0.640693 │             0.0119342  │                 0.652627 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  3 │              4 │                0.662338 │             0.652381 │             0.0125167  │                 0.664898 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  4 │              5 │                0.681818 │             0.652381 │             0.0248154  │                 0.677196 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  5 │              6 │                0.67316  │             0.660606 │             0.0140543  │                 0.67466  │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  6 │              7 │                0.677489 │             0.654545 │             0.0149084  │                 0.669454 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  7 │              8 │                0.677489 │             0.655411 │             0.0144488  │                 0.66986  │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  8 │              9 │                0.67316  │             0.654545 │             0.0145907  │                 0.669136 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│  9 │             10 │                0.681818 │             0.657143 │             0.0162323  │                 0.673375 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 10 │             11 │                0.681818 │             0.65671  │             0.0165184  │                 0.673228 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 11 │             12 │                0.681818 │             0.65974  │             0.0154516  │                 0.675192 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 12 │             13 │                0.670996 │             0.648918 │             0.0143185  │                 0.663236 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 13 │             14 │                0.670996 │             0.643723 │             0.0193889  │                 0.663112 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 14 │             15 │                0.670996 │             0.64329  │             0.0199979  │                 0.663288 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 15 │             16 │                0.670996 │             0.648485 │             0.0145264  │                 0.663011 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 16 │             17 │                0.670996 │             0.648052 │             0.0147821  │                 0.662834 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 17 │             18 │                0.670996 │             0.64632  │             0.0162323  │                 0.662553 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 18 │             19 │                0.670996 │             0.645455 │             0.0171747  │                 0.662629 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 19 │             20 │                0.670996 │             0.642857 │             0.0206253  │                 0.663482 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 20 │             21 │                0.670996 │             0.643723 │             0.0193889  │                 0.663112 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 21 │             22 │                0.670996 │             0.64026  │             0.0246943  │                 0.664954 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 22 │             23 │                0.670996 │             0.642857 │             0.0206253  │                 0.663482 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 23 │             24 │                0.670996 │             0.639394 │             0.0261394  │                 0.665533 │\n",
            "├────┼────────────────┼─────────────────────────┼──────────────────────┼────────────────────────┼──────────────────────────┤\n",
            "│ 24 │             25 │                0.670996 │             0.64632  │             0.0162323  │                 0.662553 │\n",
            "╘════╧════════════════╧═════════════════════════╧══════════════════════╧════════════════════════╧══════════════════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
        "plt.subplots(figsize=(12,5)) # set the size that you'd like (width, height)\n",
        "clrs = ['blue' ]\n",
        "sns.barplot(X, MAV,label=X,x=\"Parameters\", y=\"MAX Validation Accuracy of SVM\",palette=clrs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hGqMnD3MPcpv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "d47a2c45-8baf-40a5-d897-1d92ba11994c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fecc8b01850>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(figsize=(12,5))\n",
        "sns.barplot(X, MEV,label=X,x=\"Parameters\", y=\"Mean of Validation Accuracy of SVM\",palette=clrs)\n"
      ],
      "metadata": {
        "id": "Jy_J3v7eQGUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "d9c71436-98f3-4293-861b-1a0491aaf665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fecc0627850>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(figsize=(12,5))\n",
        "sns.barplot(X, EV,label=X,x=\"Parameters\", y=\"Mean + SD of Validation Accuracy of SVM\",palette=clrs)"
      ],
      "metadata": {
        "id": "t--3w23jQJVO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "4f45de2f-93dd-41d5-99ec-41d7d448f802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fecc008ac10>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train=Dimensionality_Reduction_Train(X_train)\n",
        "#x_test=Dimensionality_Reduction_Test(X_test)\n",
        "\n",
        "\n",
        "\n",
        "#Xtr=x_train[:,K19]\n",
        "#Xte=x_test[:,K19]\n",
        "#s=19\n",
        "#print(\"SVM classifier\")\n",
        "#SVM(Xtr,Xte,y_test,y_train)\n",
        "#print(\"KNN classifier\")\n",
        "#knn(Xtr,Xte,y_test,y_train)\n",
        "#print(\"Desition Tree classifier\")\n",
        "#DTree(Xtr,Xte,y_test,y_train)\n",
        "#print(\"XGBoost classifier\")\n",
        "#xgbclassif(Xtr,Xte,y_test,y_train)"
      ],
      "metadata": {
        "id": "KJ7cF2OwO4Q8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}